{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4857903-f15a-4d84-a936-fb7567ff530f",
   "metadata": {},
   "source": [
    "# Saving the trained agent\n",
    "\n",
    "<img src=\"images/flow/flow.png\" width=\"500\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd8b7d6f-73e9-44f5-bb63-50764c21eb3d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.90',\n",
       " 'raylet_ip_address': '192.168.0.90',\n",
       " 'redis_address': '192.168.0.90:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2022-01-03_16-20-37_811819_21206/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-01-03_16-20-37_811819_21206/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-01-03_16-20-37_811819_21206',\n",
       " 'metrics_export_port': 60671,\n",
       " 'node_id': '149088bc27a879f046e381de9156c733fe92c73726514e7300bc02a0'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cdbfe1-23a7-42a8-ae54-d41b2e6f531c",
   "metadata": {},
   "source": [
    "## Step 1: Create checkpoints during the experiment run\n",
    "\n",
    "- Checkpoint save all information to **restore the current policy**.\n",
    "- Checkpoints can be created at regular intervals via `tune.run()`'s `checkpoint_freq` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e8f94a-5cfd-4f8d-9333-ec38f3986096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m 2022-01-03 16:26:10,700\tINFO trainer.py:722 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also want to then set `eager_tracing=True` in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m 2022-01-03 16:26:10,701\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m 2022-01-03 16:26:10,701\tINFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=23772)\u001b[0m 2022-01-03 16:26:12,614\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m 2022-01-03 16:26:13,323\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m 2022-01-03 16:26:13,836\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:26:14 (running for 00:00:05.75)<br>Memory usage on this node: 8.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m 2022-01-03 16:26:14,494\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:26:15 (running for 00:00:06.76)<br>Memory usage on this node: 8.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m 2022-01-03 16:26:17,492\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:26:20 (running for 00:00:11.76)<br>Memory usage on this node: 8.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-26-21\n",
      "  done: false\n",
      "  episode_len_mean: 21.917127071823206\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 116.0\n",
      "  episode_reward_mean: 21.917127071823206\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 181\n",
      "  episodes_total: 181\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6664252281188965\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027582943439483643\n",
      "          model: {}\n",
      "          policy_loss: -0.03725969418883324\n",
      "          total_loss: 228.5469207763672\n",
      "          vf_explained_var: 0.02650553733110428\n",
      "          vf_loss: 228.57864379882812\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.220000000000002\n",
      "    ram_util_percent: 12.8\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07841399527632605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08139925122254135\n",
      "    mean_inference_ms: 1.1123281736440072\n",
      "    mean_raw_obs_processing_ms: 0.12997227549846843\n",
      "  time_since_restore: 6.655147552490234\n",
      "  time_this_iter_s: 6.655147552490234\n",
      "  time_total_s: 6.655147552490234\n",
      "  timers:\n",
      "    learn_throughput: 1099.147\n",
      "    learn_time_ms: 3639.186\n",
      "    load_throughput: 6558724.003\n",
      "    load_time_ms: 0.61\n",
      "    sample_throughput: 1094.393\n",
      "    sample_time_ms: 3654.993\n",
      "    update_time_ms: 3.275\n",
      "  timestamp: 1641223581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:26:26 (running for 00:00:17.44)<br>Memory usage on this node: 8.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.65515</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 21.9171</td><td style=\"text-align: right;\">                 116</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">           21.9171</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:26:31 (running for 00:00:22.45)<br>Memory usage on this node: 8.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.65515</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 21.9171</td><td style=\"text-align: right;\">                 116</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">           21.9171</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-26-31\n",
      "  done: false\n",
      "  episode_len_mean: 41.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 151.0\n",
      "  episode_reward_mean: 41.96\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 92\n",
      "  episodes_total: 273\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 95.55\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 273.0\n",
      "    episode_reward_mean: 95.55\n",
      "    episode_reward_min: 32.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 121\n",
      "      - 157\n",
      "      - 73\n",
      "      - 91\n",
      "      - 99\n",
      "      - 34\n",
      "      - 80\n",
      "      - 42\n",
      "      - 89\n",
      "      - 34\n",
      "      - 32\n",
      "      - 121\n",
      "      - 273\n",
      "      - 89\n",
      "      - 80\n",
      "      - 117\n",
      "      - 80\n",
      "      - 58\n",
      "      - 173\n",
      "      - 68\n",
      "      episode_reward:\n",
      "      - 121.0\n",
      "      - 157.0\n",
      "      - 73.0\n",
      "      - 91.0\n",
      "      - 99.0\n",
      "      - 34.0\n",
      "      - 80.0\n",
      "      - 42.0\n",
      "      - 89.0\n",
      "      - 34.0\n",
      "      - 32.0\n",
      "      - 121.0\n",
      "      - 273.0\n",
      "      - 89.0\n",
      "      - 80.0\n",
      "      - 117.0\n",
      "      - 80.0\n",
      "      - 58.0\n",
      "      - 173.0\n",
      "      - 68.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10046524981574531\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10273820685542276\n",
      "      mean_inference_ms: 1.39953214753123\n",
      "      mean_raw_obs_processing_ms: 0.12801294546247025\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6172640323638916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01662641391158104\n",
      "          model: {}\n",
      "          policy_loss: -0.031194239854812622\n",
      "          total_loss: 403.20538330078125\n",
      "          vf_explained_var: 0.03669131174683571\n",
      "          vf_loss: 403.2315979003906\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.606666666666667\n",
      "    ram_util_percent: 12.873333333333337\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08074499751492123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08406925218473987\n",
      "    mean_inference_ms: 1.1408297360420843\n",
      "    mean_raw_obs_processing_ms: 0.12918153300671822\n",
      "  time_since_restore: 16.828144788742065\n",
      "  time_this_iter_s: 10.172997236251831\n",
      "  time_total_s: 16.828144788742065\n",
      "  timers:\n",
      "    learn_throughput: 1122.665\n",
      "    learn_time_ms: 3562.95\n",
      "    load_throughput: 6856238.66\n",
      "    load_time_ms: 0.583\n",
      "    sample_throughput: 754.431\n",
      "    sample_time_ms: 5302.009\n",
      "    update_time_ms: 2.802\n",
      "  timestamp: 1641223591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:26:36 (running for 00:00:27.68)<br>Memory usage on this node: 8.1/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         16.8281</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   41.96</td><td style=\"text-align: right;\">                 151</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             41.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-26-38\n",
      "  done: false\n",
      "  episode_len_mean: 64.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 294.0\n",
      "  episode_reward_mean: 64.68\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 48\n",
      "  episodes_total: 321\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5778853893280029\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0102156363427639\n",
      "          model: {}\n",
      "          policy_loss: -0.026395536959171295\n",
      "          total_loss: 610.8380126953125\n",
      "          vf_explained_var: 0.08600593358278275\n",
      "          vf_loss: 610.861328125\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.16666666666667\n",
      "    ram_util_percent: 12.9\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08294687410590146\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08626269303603491\n",
      "    mean_inference_ms: 1.1707447289944957\n",
      "    mean_raw_obs_processing_ms: 0.12940688890844987\n",
      "  time_since_restore: 23.690666913986206\n",
      "  time_this_iter_s: 6.862522125244141\n",
      "  time_total_s: 23.690666913986206\n",
      "  timers:\n",
      "    learn_throughput: 1130.973\n",
      "    learn_time_ms: 3536.778\n",
      "    load_throughput: 6888141.234\n",
      "    load_time_ms: 0.581\n",
      "    sample_throughput: 572.57\n",
      "    sample_time_ms: 6986.047\n",
      "    update_time_ms: 2.655\n",
      "  timestamp: 1641223598\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:26:42 (running for 00:00:33.61)<br>Memory usage on this node: 8.1/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         23.6907</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   64.68</td><td style=\"text-align: right;\">                 294</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             64.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:26:47 (running for 00:00:38.62)<br>Memory usage on this node: 8.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         23.6907</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   64.68</td><td style=\"text-align: right;\">                 294</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             64.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:26:52 (running for 00:00:43.63)<br>Memory usage on this node: 8.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         23.6907</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   64.68</td><td style=\"text-align: right;\">                 294</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             64.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:26:57 (running for 00:00:48.64)<br>Memory usage on this node: 8.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         23.6907</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   64.68</td><td style=\"text-align: right;\">                 294</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             64.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-26-57\n",
      "  done: false\n",
      "  episode_len_mean: 93.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 363.0\n",
      "  episode_reward_mean: 93.09\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 343\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 379.65\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 379.65\n",
      "    episode_reward_min: 134.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 351\n",
      "      - 330\n",
      "      - 254\n",
      "      - 185\n",
      "      - 500\n",
      "      - 475\n",
      "      - 491\n",
      "      - 349\n",
      "      - 500\n",
      "      - 406\n",
      "      - 417\n",
      "      - 423\n",
      "      - 420\n",
      "      - 134\n",
      "      - 500\n",
      "      - 304\n",
      "      - 347\n",
      "      - 336\n",
      "      - 460\n",
      "      - 411\n",
      "      episode_reward:\n",
      "      - 351.0\n",
      "      - 330.0\n",
      "      - 254.0\n",
      "      - 185.0\n",
      "      - 500.0\n",
      "      - 475.0\n",
      "      - 491.0\n",
      "      - 349.0\n",
      "      - 500.0\n",
      "      - 406.0\n",
      "      - 417.0\n",
      "      - 423.0\n",
      "      - 420.0\n",
      "      - 134.0\n",
      "      - 500.0\n",
      "      - 304.0\n",
      "      - 347.0\n",
      "      - 336.0\n",
      "      - 460.0\n",
      "      - 411.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09892259002797923\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10168059508089389\n",
      "      mean_inference_ms: 1.3819190591714807\n",
      "      mean_raw_obs_processing_ms: 0.1207713639341111\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5606166124343872\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006913498975336552\n",
      "          model: {}\n",
      "          policy_loss: -0.01752745360136032\n",
      "          total_loss: 844.9335327148438\n",
      "          vf_explained_var: 0.0798516795039177\n",
      "          vf_loss: 844.9489135742188\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.98214285714286\n",
      "    ram_util_percent: 13.142857142857148\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08340957065333188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08675562845906849\n",
      "    mean_inference_ms: 1.1763500319084028\n",
      "    mean_raw_obs_processing_ms: 0.12852212527880877\n",
      "  time_since_restore: 43.13603973388672\n",
      "  time_this_iter_s: 19.445372819900513\n",
      "  time_total_s: 43.13603973388672\n",
      "  timers:\n",
      "    learn_throughput: 1136.579\n",
      "    learn_time_ms: 3519.332\n",
      "    load_throughput: 6926294.148\n",
      "    load_time_ms: 0.578\n",
      "    sample_throughput: 581.505\n",
      "    sample_time_ms: 6878.698\n",
      "    update_time_ms: 2.654\n",
      "  timestamp: 1641223617\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:27:02 (running for 00:00:54.11)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">          43.136</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   93.09</td><td style=\"text-align: right;\">                 363</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             93.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-27-04\n",
      "  done: false\n",
      "  episode_len_mean: 124.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 124.81\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 354\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5423797965049744\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005181576125323772\n",
      "          model: {}\n",
      "          policy_loss: -0.01488120574504137\n",
      "          total_loss: 887.0194702148438\n",
      "          vf_explained_var: 0.06684495508670807\n",
      "          vf_loss: 887.0327758789062\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.739999999999998\n",
      "    ram_util_percent: 13.52\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0836764850981119\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08707177571345362\n",
      "    mean_inference_ms: 1.1789606643781958\n",
      "    mean_raw_obs_processing_ms: 0.12802314676502513\n",
      "  time_since_restore: 50.01827669143677\n",
      "  time_this_iter_s: 6.882236957550049\n",
      "  time_total_s: 50.01827669143677\n",
      "  timers:\n",
      "    learn_throughput: 1139.508\n",
      "    learn_time_ms: 3510.288\n",
      "    load_throughput: 6901931.874\n",
      "    load_time_ms: 0.58\n",
      "    sample_throughput: 421.886\n",
      "    sample_time_ms: 9481.239\n",
      "    update_time_ms: 2.483\n",
      "  timestamp: 1641223624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:27:08 (running for 00:01:00.02)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         50.0183</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  124.81</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            124.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:27:13 (running for 00:01:05.03)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         50.0183</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  124.81</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            124.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:27:18 (running for 00:01:10.03)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         50.0183</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  124.81</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            124.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:27:23 (running for 00:01:15.05)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         50.0183</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  124.81</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            124.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-27-26\n",
      "  done: false\n",
      "  episode_len_mean: 161.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 161.7\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 363\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 435.1\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 435.1\n",
      "    episode_reward_min: 250.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 406\n",
      "      - 494\n",
      "      - 447\n",
      "      - 250\n",
      "      - 468\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 346\n",
      "      - 446\n",
      "      - 500\n",
      "      - 363\n",
      "      - 500\n",
      "      - 344\n",
      "      - 500\n",
      "      - 500\n",
      "      - 335\n",
      "      - 394\n",
      "      - 409\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 406.0\n",
      "      - 494.0\n",
      "      - 447.0\n",
      "      - 250.0\n",
      "      - 468.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 346.0\n",
      "      - 446.0\n",
      "      - 500.0\n",
      "      - 363.0\n",
      "      - 500.0\n",
      "      - 344.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 335.0\n",
      "      - 394.0\n",
      "      - 409.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09927499783374975\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10222888971046826\n",
      "      mean_inference_ms: 1.392918271563254\n",
      "      mean_raw_obs_processing_ms: 0.12093862631100046\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5619332194328308\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037699141539633274\n",
      "          model: {}\n",
      "          policy_loss: -0.009215299971401691\n",
      "          total_loss: 655.686767578125\n",
      "          vf_explained_var: 0.04506475478410721\n",
      "          vf_loss: 655.6948852539062\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.561290322580646\n",
      "    ram_util_percent: 13.493548387096775\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0837986721917596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08723814168690999\n",
      "    mean_inference_ms: 1.1798668177810896\n",
      "    mean_raw_obs_processing_ms: 0.12735432859813653\n",
      "  time_since_restore: 71.62701082229614\n",
      "  time_this_iter_s: 21.608734130859375\n",
      "  time_total_s: 71.62701082229614\n",
      "  timers:\n",
      "    learn_throughput: 1140.813\n",
      "    learn_time_ms: 3506.272\n",
      "    load_throughput: 6927962.56\n",
      "    load_time_ms: 0.577\n",
      "    sample_throughput: 445.035\n",
      "    sample_time_ms: 8988.066\n",
      "    update_time_ms: 2.407\n",
      "  timestamp: 1641223646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:27:29 (running for 00:01:20.70)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">          71.627</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   161.7</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             161.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-27-33\n",
      "  done: false\n",
      "  episode_len_mean: 197.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 197.34\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 371\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5885235667228699\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003530152840539813\n",
      "          model: {}\n",
      "          policy_loss: -0.0062471735291182995\n",
      "          total_loss: 463.6754150390625\n",
      "          vf_explained_var: 0.20378383994102478\n",
      "          vf_loss: 463.6811218261719\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.579999999999998\n",
      "    ram_util_percent: 13.5\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08392030659795005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08739526657482405\n",
      "    mean_inference_ms: 1.1809781461117226\n",
      "    mean_raw_obs_processing_ms: 0.12669778318233024\n",
      "  time_since_restore: 78.35220837593079\n",
      "  time_this_iter_s: 6.7251975536346436\n",
      "  time_total_s: 78.35220837593079\n",
      "  timers:\n",
      "    learn_throughput: 1142.217\n",
      "    learn_time_ms: 3501.96\n",
      "    load_throughput: 6899742.201\n",
      "    load_time_ms: 0.58\n",
      "    sample_throughput: 369.161\n",
      "    sample_time_ms: 10835.379\n",
      "    update_time_ms: 2.34\n",
      "  timestamp: 1641223653\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:27:35 (running for 00:01:26.44)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         78.3522</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  197.34</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            197.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:27:40 (running for 00:01:31.45)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         78.3522</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  197.34</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            197.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:27:45 (running for 00:01:36.46)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         78.3522</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  197.34</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            197.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:27:50 (running for 00:01:41.47)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         78.3522</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  197.34</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            197.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-27-52\n",
      "  done: false\n",
      "  episode_len_mean: 231.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 231.02\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 382\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 384.85\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 384.85\n",
      "    episode_reward_min: 170.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 414\n",
      "      - 500\n",
      "      - 440\n",
      "      - 500\n",
      "      - 308\n",
      "      - 170\n",
      "      - 500\n",
      "      - 274\n",
      "      - 500\n",
      "      - 381\n",
      "      - 299\n",
      "      - 191\n",
      "      - 327\n",
      "      - 500\n",
      "      - 450\n",
      "      - 500\n",
      "      - 253\n",
      "      - 321\n",
      "      - 369\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 414.0\n",
      "      - 500.0\n",
      "      - 440.0\n",
      "      - 500.0\n",
      "      - 308.0\n",
      "      - 170.0\n",
      "      - 500.0\n",
      "      - 274.0\n",
      "      - 500.0\n",
      "      - 381.0\n",
      "      - 299.0\n",
      "      - 191.0\n",
      "      - 327.0\n",
      "      - 500.0\n",
      "      - 450.0\n",
      "      - 500.0\n",
      "      - 253.0\n",
      "      - 321.0\n",
      "      - 369.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09911348600310997\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10223403214082963\n",
      "      mean_inference_ms: 1.39154861045517\n",
      "      mean_raw_obs_processing_ms: 0.12062799238437043\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.554244875907898\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006881164386868477\n",
      "          model: {}\n",
      "          policy_loss: -0.009692110121250153\n",
      "          total_loss: 340.6525573730469\n",
      "          vf_explained_var: 0.2609829008579254\n",
      "          vf_loss: 340.66168212890625\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.910714285714286\n",
      "    ram_util_percent: 13.5\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0837253306534588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08715253808272065\n",
      "    mean_inference_ms: 1.179444065414167\n",
      "    mean_raw_obs_processing_ms: 0.1254589751605462\n",
      "  time_since_restore: 98.05285000801086\n",
      "  time_this_iter_s: 19.700641632080078\n",
      "  time_total_s: 98.05285000801086\n",
      "  timers:\n",
      "    learn_throughput: 1143.961\n",
      "    learn_time_ms: 3496.623\n",
      "    load_throughput: 7358428.07\n",
      "    load_time_ms: 0.544\n",
      "    sample_throughput: 388.469\n",
      "    sample_time_ms: 10296.839\n",
      "    update_time_ms: 2.298\n",
      "  timestamp: 1641223672\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:27:55 (running for 00:01:47.19)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         98.0529</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  231.02</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            231.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-27-59\n",
      "  done: false\n",
      "  episode_len_mean: 258.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 258.77\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 392\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5269732475280762\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0047079939395189285\n",
      "          model: {}\n",
      "          policy_loss: -0.008760922588407993\n",
      "          total_loss: 333.5567321777344\n",
      "          vf_explained_var: 0.2962828278541565\n",
      "          vf_loss: 333.56512451171875\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.266666666666667\n",
      "    ram_util_percent: 13.5\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08346741626581791\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08682203945648247\n",
      "    mean_inference_ms: 1.1772968705711528\n",
      "    mean_raw_obs_processing_ms: 0.12421185374177282\n",
      "  time_since_restore: 104.47196555137634\n",
      "  time_this_iter_s: 6.4191155433654785\n",
      "  time_total_s: 104.47196555137634\n",
      "  timers:\n",
      "    learn_throughput: 1145.153\n",
      "    learn_time_ms: 3492.983\n",
      "    load_throughput: 7787660.22\n",
      "    load_time_ms: 0.514\n",
      "    sample_throughput: 352.801\n",
      "    sample_time_ms: 11337.838\n",
      "    update_time_ms: 2.278\n",
      "  timestamp: 1641223679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:28:01 (running for 00:01:52.64)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         104.472</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  258.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            258.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:28:06 (running for 00:01:57.65)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         104.472</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  258.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            258.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:28:11 (running for 00:02:02.66)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         104.472</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  258.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            258.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:28:16 (running for 00:02:07.67)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         104.472</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  258.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            258.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:28:21 (running for 00:02:12.67)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         104.472</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  258.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            258.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-28-23\n",
      "  done: false\n",
      "  episode_len_mean: 293.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 293.75\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 402\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09953860797771583\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10262152807597809\n",
      "      mean_inference_ms: 1.3969615367432118\n",
      "      mean_raw_obs_processing_ms: 0.12117045235931341\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5425429940223694\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010737420991063118\n",
      "          model: {}\n",
      "          policy_loss: -0.009084474295377731\n",
      "          total_loss: 395.06121826171875\n",
      "          vf_explained_var: 0.26123329997062683\n",
      "          vf_loss: 395.06988525390625\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.305714285714288\n",
      "    ram_util_percent: 13.5\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08325403260625515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08658162756610036\n",
      "    mean_inference_ms: 1.175092351808078\n",
      "    mean_raw_obs_processing_ms: 0.12301358649292546\n",
      "  time_since_restore: 128.73230123519897\n",
      "  time_this_iter_s: 24.260335683822632\n",
      "  time_total_s: 128.73230123519897\n",
      "  timers:\n",
      "    learn_throughput: 1146.361\n",
      "    learn_time_ms: 3489.303\n",
      "    load_throughput: 7881807.761\n",
      "    load_time_ms: 0.507\n",
      "    sample_throughput: 367.348\n",
      "    sample_time_ms: 10888.862\n",
      "    update_time_ms: 2.266\n",
      "  timestamp: 1641223703\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:28:26 (running for 00:02:18.01)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         128.732</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  293.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            293.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-28-30\n",
      "  done: false\n",
      "  episode_len_mean: 324.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 324.27\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 410\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5431795120239258\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00387349771335721\n",
      "          model: {}\n",
      "          policy_loss: -0.002108412329107523\n",
      "          total_loss: 387.6857604980469\n",
      "          vf_explained_var: 0.15076985955238342\n",
      "          vf_loss: 387.6877136230469\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.711111111111112\n",
      "    ram_util_percent: 13.5\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08304564838656443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08639800860953895\n",
      "    mean_inference_ms: 1.1720430834238627\n",
      "    mean_raw_obs_processing_ms: 0.1220244178075502\n",
      "  time_since_restore: 135.21616744995117\n",
      "  time_this_iter_s: 6.483866214752197\n",
      "  time_total_s: 135.21616744995117\n",
      "  timers:\n",
      "    learn_throughput: 1152.794\n",
      "    learn_time_ms: 3469.83\n",
      "    load_throughput: 7939246.64\n",
      "    load_time_ms: 0.504\n",
      "    sample_throughput: 309.443\n",
      "    sample_time_ms: 12926.458\n",
      "    update_time_ms: 2.175\n",
      "  timestamp: 1641223710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:28:32 (running for 00:02:23.49)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         135.216</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  324.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            324.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:28:37 (running for 00:02:28.50)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         135.216</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  324.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            324.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:28:42 (running for 00:02:33.51)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         135.216</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  324.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            324.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:28:47 (running for 00:02:38.52)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         135.216</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  324.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            324.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:28:52 (running for 00:02:43.53)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         135.216</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  324.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            324.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-28-54\n",
      "  done: false\n",
      "  episode_len_mean: 357.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 357.89\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 419\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09947970545886077\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10259064098150732\n",
      "      mean_inference_ms: 1.397280638622631\n",
      "      mean_raw_obs_processing_ms: 0.12082490990825875\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5185824036598206\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006411642301827669\n",
      "          model: {}\n",
      "          policy_loss: -0.009671492502093315\n",
      "          total_loss: 278.4486083984375\n",
      "          vf_explained_var: 0.4278179407119751\n",
      "          vf_loss: 278.4581604003906\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.920588235294117\n",
      "    ram_util_percent: 13.5\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08286131158349054\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08624183105479467\n",
      "    mean_inference_ms: 1.169338331247146\n",
      "    mean_raw_obs_processing_ms: 0.1209465655114275\n",
      "  time_since_restore: 159.24180841445923\n",
      "  time_this_iter_s: 24.025640964508057\n",
      "  time_total_s: 159.24180841445923\n",
      "  timers:\n",
      "    learn_throughput: 1152.758\n",
      "    learn_time_ms: 3469.94\n",
      "    load_throughput: 7914154.441\n",
      "    load_time_ms: 0.505\n",
      "    sample_throughput: 309.946\n",
      "    sample_time_ms: 12905.462\n",
      "    update_time_ms: 2.181\n",
      "  timestamp: 1641223734\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:28:57 (running for 00:02:48.63)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         159.242</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  357.89</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            357.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-29-00\n",
      "  done: false\n",
      "  episode_len_mean: 381.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 381.12\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 428\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5315417647361755\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005102929193526506\n",
      "          model: {}\n",
      "          policy_loss: -0.007035453338176012\n",
      "          total_loss: 534.12744140625\n",
      "          vf_explained_var: 0.21898993849754333\n",
      "          vf_loss: 534.1343994140625\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.969999999999999\n",
      "    ram_util_percent: 13.5\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08280742814789453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08614625355820259\n",
      "    mean_inference_ms: 1.169359046922206\n",
      "    mean_raw_obs_processing_ms: 0.12012232870499442\n",
      "  time_since_restore: 165.86456823349\n",
      "  time_this_iter_s: 6.622759819030762\n",
      "  time_total_s: 165.86456823349\n",
      "  timers:\n",
      "    learn_throughput: 1152.394\n",
      "    learn_time_ms: 3471.035\n",
      "    load_throughput: 7795017.423\n",
      "    load_time_ms: 0.513\n",
      "    sample_throughput: 280.386\n",
      "    sample_time_ms: 14266.066\n",
      "    update_time_ms: 2.16\n",
      "  timestamp: 1641223740\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:29:02 (running for 00:02:54.25)<br>Memory usage on this node: 8.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         165.865</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  381.12</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            381.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:29:08 (running for 00:02:59.26)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         165.865</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  381.12</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            381.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:29:13 (running for 00:03:04.27)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         165.865</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  381.12</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            381.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:29:18 (running for 00:03:09.28)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         165.865</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  381.12</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            381.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:29:23 (running for 00:03:14.29)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         165.865</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  381.12</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            381.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-29-25\n",
      "  done: false\n",
      "  episode_len_mean: 407.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 407.19\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 436\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09982978352561431\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10304156377374205\n",
      "      mean_inference_ms: 1.4033001161467775\n",
      "      mean_raw_obs_processing_ms: 0.12095114743975684\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5248523950576782\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005778787657618523\n",
      "          model: {}\n",
      "          policy_loss: 0.0009851830545812845\n",
      "          total_loss: 259.5467834472656\n",
      "          vf_explained_var: 0.34937044978141785\n",
      "          vf_loss: 259.5456848144531\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.048571428571428\n",
      "    ram_util_percent: 13.474285714285712\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08278502847904211\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0861244462761047\n",
      "    mean_inference_ms: 1.1694603850099903\n",
      "    mean_raw_obs_processing_ms: 0.11947055739581668\n",
      "  time_since_restore: 190.4103925228119\n",
      "  time_this_iter_s: 24.5458242893219\n",
      "  time_total_s: 190.4103925228119\n",
      "  timers:\n",
      "    learn_throughput: 1152.142\n",
      "    learn_time_ms: 3471.794\n",
      "    load_throughput: 8102978.025\n",
      "    load_time_ms: 0.494\n",
      "    sample_throughput: 279.644\n",
      "    sample_time_ms: 14303.912\n",
      "    update_time_ms: 2.12\n",
      "  timestamp: 1641223765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:29:28 (running for 00:03:19.90)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">          190.41</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  407.19</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            407.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-29-32\n",
      "  done: false\n",
      "  episode_len_mean: 429.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 429.86\n",
      "  episode_reward_min: 14.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 444\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5310676693916321\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002669211011379957\n",
      "          model: {}\n",
      "          policy_loss: -0.0006353632197715342\n",
      "          total_loss: 463.389892578125\n",
      "          vf_explained_var: 0.06647515296936035\n",
      "          vf_loss: 463.39044189453125\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.41111111111111\n",
      "    ram_util_percent: 13.5\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08270939906633591\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08608194431751746\n",
      "    mean_inference_ms: 1.1684522572547298\n",
      "    mean_raw_obs_processing_ms: 0.11878699547740285\n",
      "  time_since_restore: 196.8453106880188\n",
      "  time_this_iter_s: 6.434918165206909\n",
      "  time_total_s: 196.8453106880188\n",
      "  timers:\n",
      "    learn_throughput: 1152.085\n",
      "    learn_time_ms: 3471.966\n",
      "    load_throughput: 8324922.344\n",
      "    load_time_ms: 0.48\n",
      "    sample_throughput: 271.509\n",
      "    sample_time_ms: 14732.492\n",
      "    update_time_ms: 2.214\n",
      "  timestamp: 1641223772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:29:34 (running for 00:03:25.35)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         196.845</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  429.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            429.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:29:39 (running for 00:03:30.36)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         196.845</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  429.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            429.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:29:44 (running for 00:03:35.36)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         196.845</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  429.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            429.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:29:49 (running for 00:03:40.37)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         196.845</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  429.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            429.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:29:54 (running for 00:03:45.38)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         196.845</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  429.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            429.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-29-56\n",
      "  done: false\n",
      "  episode_len_mean: 442.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 442.67\n",
      "  episode_reward_min: 100.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 452\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10061120055028032\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10391276824471438\n",
      "      mean_inference_ms: 1.413652028062732\n",
      "      mean_raw_obs_processing_ms: 0.1216532463991518\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4945480525493622\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005677539389580488\n",
      "          model: {}\n",
      "          policy_loss: -0.0031680716201663017\n",
      "          total_loss: 474.1834716796875\n",
      "          vf_explained_var: -0.039894502609968185\n",
      "          vf_loss: 474.1865539550781\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.542857142857141\n",
      "    ram_util_percent: 13.5\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08259650387488782\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08596375029736919\n",
      "    mean_inference_ms: 1.167257922585499\n",
      "    mean_raw_obs_processing_ms: 0.11813799099415782\n",
      "  time_since_restore: 221.60105895996094\n",
      "  time_this_iter_s: 24.75574827194214\n",
      "  time_total_s: 221.60105895996094\n",
      "  timers:\n",
      "    learn_throughput: 1152.544\n",
      "    learn_time_ms: 3470.583\n",
      "    load_throughput: 8927375.086\n",
      "    load_time_ms: 0.448\n",
      "    sample_throughput: 271.335\n",
      "    sample_time_ms: 14741.936\n",
      "    update_time_ms: 2.227\n",
      "  timestamp: 1641223796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:29:59 (running for 00:03:51.22)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         221.601</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  442.67</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 100</td><td style=\"text-align: right;\">            442.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_788b8_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_16-30-03\n",
      "  done: false\n",
      "  episode_len_mean: 452.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 452.1\n",
      "  episode_reward_min: 165.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 460\n",
      "  experiment_id: 940a38d0c14e4d68b8625ef5adbc1cde\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.48626309633255005\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006992555223405361\n",
      "          model: {}\n",
      "          policy_loss: 0.00016935987514443696\n",
      "          total_loss: 544.2183227539062\n",
      "          vf_explained_var: -0.20592768490314484\n",
      "          vf_loss: 544.2181396484375\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.129999999999999\n",
      "    ram_util_percent: 13.5\n",
      "  pid: 23768\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08249803117763195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08584633691745282\n",
      "    mean_inference_ms: 1.1662665118721514\n",
      "    mean_raw_obs_processing_ms: 0.117593650822598\n",
      "  time_since_restore: 227.99164080619812\n",
      "  time_this_iter_s: 6.390581846237183\n",
      "  time_total_s: 227.99164080619812\n",
      "  timers:\n",
      "    learn_throughput: 1152.085\n",
      "    learn_time_ms: 3471.967\n",
      "    load_throughput: 8930226.22\n",
      "    load_time_ms: 0.448\n",
      "    sample_throughput: 266.443\n",
      "    sample_time_ms: 15012.577\n",
      "    update_time_ms: 2.277\n",
      "  timestamp: 1641223803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 788b8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:30:05 (running for 00:03:56.60)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         227.992</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">   452.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 165</td><td style=\"text-align: right;\">             452.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:30:10 (running for 00:04:01.61)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         227.992</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">   452.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 165</td><td style=\"text-align: right;\">             452.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:30:15 (running for 00:04:06.62)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         227.992</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">   452.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 165</td><td style=\"text-align: right;\">             452.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:30:20 (running for 00:04:11.63)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         227.992</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">   452.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 165</td><td style=\"text-align: right;\">             452.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 16:30:22,387\tWARNING tune.py:582 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 16:30:22 (running for 00:04:13.64)<br>Memory usage on this node: 8.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.65 GiB heap, 0.0/17.33 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/18_saving_the_trained_agent/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_788b8_00000</td><td>RUNNING </td><td>192.168.0.90:23768</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         227.992</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">   452.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 165</td><td style=\"text-align: right;\">             452.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m 2022-01-03 16:30:22,405\tERROR worker.py:431 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 759, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 580, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 618, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 625, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 629, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"python/ray/_raylet.pyx\", line 578, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 609, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/tune/trainable.py\", line 255, in train_buffered\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/tune/trainable.py\", line 314, in train\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 867, in step\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     result = self.step_attempt()\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 953, in step_attempt\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     evaluation_metrics = self.evaluate()\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 1060, in evaluate\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     self.evaluation_workers.local_worker().sample()\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 757, in sample\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 103, in next\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 265, in get_data\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 656, in _env_runner\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     eval_results = _do_policy_eval(\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 1070, in _do_policy_eval\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     policy.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/policy/tf_policy.py\", line 297, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     fetched = builder.get(to_fetch)\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/utils/tf_run_builder.py\", line 42, in get\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     self._executed = run_timeline(\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/utils/tf_run_builder.py\", line 92, in run_timeline\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 970, in run\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1178, in _run\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     fetch_handler = _FetchHandler(\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 487, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     with graph.as_default():\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/contextlib.py\", line 119, in __enter__\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     return next(self.gen)\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 5760, in get_controller\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     context.context().context_switches.push(default.building_function,\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/eager/context.py\", line 1885, in context_switches\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     @property\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/worker.py\", line 428, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(PPO pid=23768)\u001b[0m SystemExit: 1\n",
      "2022-01-03 16:30:22,603\tERROR tune.py:622 -- Trials did not complete: [PPO_CartPole-v1_788b8_00000]\n",
      "2022-01-03 16:30:22,604\tINFO tune.py:626 -- Total run time: 255.67 seconds (253.64 seconds for the tuning loop).\n",
      "2022-01-03 16:30:22,605\tWARNING tune.py:630 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fc948ff2790>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"CartPole-v1\",\n",
    "                 \"evaluation_interval\": 2,\n",
    "                 \"evaluation_num_episodes\": 20\n",
    "                 },\n",
    "         local_dir=\"cartpole_v1\",\n",
    "         checkpoint_freq=2,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4642d-b733-4d3a-8fec-cc62208507c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Manually stop training after a certain performance is reached"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
