{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3595528b-6ea8-474d-aae6-02a79c6c99d3",
   "metadata": {},
   "source": [
    "# Visualizing training and evaluation\n",
    "\n",
    "<img src=\"images/graph.png\" width=\"400\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ce636-9502-41f9-b6b3-a98460332a22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af232e4e-d9dc-4fb3-9654-9e652c80dddd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.90',\n",
       " 'raylet_ip_address': '192.168.0.90',\n",
       " 'redis_address': '192.168.0.90:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2022-01-03_15-00-37_942058_18658/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-01-03_15-00-37_942058_18658/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-01-03_15-00-37_942058_18658',\n",
       " 'metrics_export_port': 59997,\n",
       " 'node_id': '4b2f09b2c1ee6eec67bdf96840daec42e17e11c78f793dc14c0d6184'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0033ea09-1697-4f6e-b8de-578e6aa10c64",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m 2022-01-03 15:00:47,236\tINFO trainer.py:722 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also want to then set `eager_tracing=True` in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m 2022-01-03 15:00:47,236\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m 2022-01-03 15:00:47,237\tINFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18794)\u001b[0m 2022-01-03 15:00:49,091\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m 2022-01-03 15:00:49,783\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m 2022-01-03 15:00:50,324\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:00:50 (running for 00:00:05.80)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_8aa14_00000</td><td>RUNNING </td><td>192.168.0.90:18786</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m 2022-01-03 15:00:50,979\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:00:51 (running for 00:00:06.81)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_8aa14_00000</td><td>RUNNING </td><td>192.168.0.90:18786</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m 2022-01-03 15:00:54,423\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:00:57 (running for 00:00:11.82)<br>Memory usage on this node: 9.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_8aa14_00000</td><td>RUNNING </td><td>192.168.0.90:18786</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_8aa14_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-00-58\n",
      "  done: false\n",
      "  episode_len_mean: 22.45762711864407\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 88.0\n",
      "  episode_reward_mean: 22.45762711864407\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 177\n",
      "  episodes_total: 177\n",
      "  experiment_id: 6f76661480e643268afbb7f3ac0474ff\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6669806241989136\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027769306674599648\n",
      "          model: {}\n",
      "          policy_loss: -0.04107631742954254\n",
      "          total_loss: 210.96861267089844\n",
      "          vf_explained_var: 0.025448909029364586\n",
      "          vf_loss: 211.00416564941406\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.09090909090909\n",
      "    ram_util_percent: 15.009090909090911\n",
      "  pid: 18786\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08767884393501267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09139009905341684\n",
      "    mean_inference_ms: 1.2375054723630263\n",
      "    mean_raw_obs_processing_ms: 0.14297866202518336\n",
      "  time_since_restore: 7.1466100215911865\n",
      "  time_this_iter_s: 7.1466100215911865\n",
      "  time_total_s: 7.1466100215911865\n",
      "  timers:\n",
      "    learn_throughput: 1086.746\n",
      "    learn_time_ms: 3680.712\n",
      "    load_throughput: 6428052.107\n",
      "    load_time_ms: 0.622\n",
      "    sample_throughput: 975.863\n",
      "    sample_time_ms: 4098.935\n",
      "    update_time_ms: 3.391\n",
      "  timestamp: 1641218458\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 8aa14_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:01:02 (running for 00:00:16.99)<br>Memory usage on this node: 9.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_8aa14_00000</td><td>RUNNING </td><td>192.168.0.90:18786</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.14661</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.4576</td><td style=\"text-align: right;\">                  88</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           22.4576</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:01:07 (running for 00:00:22.00)<br>Memory usage on this node: 9.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_8aa14_00000</td><td>RUNNING </td><td>192.168.0.90:18786</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.14661</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.4576</td><td style=\"text-align: right;\">                  88</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           22.4576</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 15:01:08,190\tWARNING tune.py:582 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:01:08 (running for 00:00:23.01)<br>Memory usage on this node: 9.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_8aa14_00000</td><td>RUNNING </td><td>192.168.0.90:18786</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.14661</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.4576</td><td style=\"text-align: right;\">                  88</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           22.4576</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m 2022-01-03 15:01:08,205\tERROR worker.py:431 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"python/ray/_raylet.pyx\", line 759, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"python/ray/_raylet.pyx\", line 580, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"python/ray/_raylet.pyx\", line 618, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"python/ray/_raylet.pyx\", line 625, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"python/ray/_raylet.pyx\", line 629, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"python/ray/_raylet.pyx\", line 578, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 609, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/tune/trainable.py\", line 255, in train_buffered\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/tune/trainable.py\", line 314, in train\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 867, in step\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     result = self.step_attempt()\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 953, in step_attempt\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     evaluation_metrics = self.evaluate()\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 1060, in evaluate\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     self.evaluation_workers.local_worker().sample()\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 757, in sample\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 103, in next\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 265, in get_data\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 667, in _env_runner\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     _process_policy_eval_results(\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 1142, in _process_policy_eval_results\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     policy: Policy = _get_or_raise(policies, policy_id)\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 1217, in _get_or_raise\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     if policy_id not in mapping:\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/utils/threading.py\", line 20, in wrapper\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     with self._lock:\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/worker.py\", line 428, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(PPO pid=18786)\u001b[0m SystemExit: 1\n",
      "2022-01-03 15:01:08,402\tERROR tune.py:622 -- Trials did not complete: [PPO_CartPole-v1_8aa14_00000]\n",
      "2022-01-03 15:01:08,404\tINFO tune.py:626 -- Total run time: 25.09 seconds (23.01 seconds for the tuning loop).\n",
      "2022-01-03 15:01:08,405\tWARNING tune.py:630 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f51fbac9a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"CartPole-v1\",\n",
    "                 \"evaluation_interval\": 2, \n",
    "                 \"evaluation_num_episodes\": 20,\n",
    "                 },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aac37a-e712-47a9-88e2-d13b029f2edd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Choosing a different results directory\n",
    "\n",
    "- Can be set via the `tune.run()` argument `local_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db158dd2-5f3e-4132-b641-2e11ff95383b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m 2022-01-03 15:02:49,145\tINFO trainer.py:722 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also want to then set `eager_tracing=True` in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m 2022-01-03 15:02:49,145\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m 2022-01-03 15:02:49,145\tINFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=18800)\u001b[0m 2022-01-03 15:02:51,152\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m 2022-01-03 15:02:51,859\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m 2022-01-03 15:02:52,442\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:02:53 (running for 00:00:05.99)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m 2022-01-03 15:02:53,083\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:02:54 (running for 00:00:06.99)<br>Memory usage on this node: 9.1/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m 2022-01-03 15:02:56,659\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:02:59 (running for 00:00:12.00)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-03-00\n",
      "  done: false\n",
      "  episode_len_mean: 22.908045977011493\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 77.0\n",
      "  episode_reward_mean: 22.908045977011493\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 174\n",
      "  episodes_total: 174\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.666509211063385\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028093166649341583\n",
      "          model: {}\n",
      "          policy_loss: -0.042971573770046234\n",
      "          total_loss: 189.67881774902344\n",
      "          vf_explained_var: 0.01694451831281185\n",
      "          vf_loss: 189.71615600585938\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.77272727272727\n",
      "    ram_util_percent: 14.627272727272725\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09202963658620829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09591383237392329\n",
      "    mean_inference_ms: 1.310459739055159\n",
      "    mean_raw_obs_processing_ms: 0.1512266071889329\n",
      "  time_since_restore: 7.319287061691284\n",
      "  time_this_iter_s: 7.319287061691284\n",
      "  time_total_s: 7.319287061691284\n",
      "  timers:\n",
      "    learn_throughput: 1073.025\n",
      "    learn_time_ms: 3727.778\n",
      "    load_throughput: 14550924.545\n",
      "    load_time_ms: 0.275\n",
      "    sample_throughput: 948.545\n",
      "    sample_time_ms: 4216.984\n",
      "    update_time_ms: 3.169\n",
      "  timestamp: 1641218580\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:03:04 (running for 00:00:17.33)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.31929</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  22.908</td><td style=\"text-align: right;\">                  77</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            22.908</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:03:09 (running for 00:00:22.34)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.31929</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  22.908</td><td style=\"text-align: right;\">                  77</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            22.908</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-03-11\n",
      "  done: false\n",
      "  episode_len_mean: 44.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.0\n",
      "  episode_reward_mean: 44.07\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 253\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 105.65\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 362.0\n",
      "    episode_reward_mean: 105.65\n",
      "    episode_reward_min: 11.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 79\n",
      "      - 161\n",
      "      - 185\n",
      "      - 246\n",
      "      - 42\n",
      "      - 324\n",
      "      - 11\n",
      "      - 119\n",
      "      - 48\n",
      "      - 36\n",
      "      - 14\n",
      "      - 55\n",
      "      - 33\n",
      "      - 11\n",
      "      - 63\n",
      "      - 362\n",
      "      - 147\n",
      "      - 132\n",
      "      - 23\n",
      "      - 22\n",
      "      episode_reward:\n",
      "      - 79.0\n",
      "      - 161.0\n",
      "      - 185.0\n",
      "      - 246.0\n",
      "      - 42.0\n",
      "      - 324.0\n",
      "      - 11.0\n",
      "      - 119.0\n",
      "      - 48.0\n",
      "      - 36.0\n",
      "      - 14.0\n",
      "      - 55.0\n",
      "      - 33.0\n",
      "      - 11.0\n",
      "      - 63.0\n",
      "      - 362.0\n",
      "      - 147.0\n",
      "      - 132.0\n",
      "      - 23.0\n",
      "      - 22.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10355994063046256\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1077551602640576\n",
      "      mean_inference_ms: 1.4494920295708116\n",
      "      mean_raw_obs_processing_ms: 0.13548694022331131\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6152109503746033\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016989998519420624\n",
      "          model: {}\n",
      "          policy_loss: -0.031975165009498596\n",
      "          total_loss: 487.11810302734375\n",
      "          vf_explained_var: 0.11033960431814194\n",
      "          vf_loss: 487.1449890136719\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.006666666666666\n",
      "    ram_util_percent: 14.699999999999994\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08977212130348691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0931780306840584\n",
      "    mean_inference_ms: 1.2776730765064892\n",
      "    mean_raw_obs_processing_ms: 0.14313832140506\n",
      "  time_since_restore: 18.054560661315918\n",
      "  time_this_iter_s: 10.735273599624634\n",
      "  time_total_s: 18.054560661315918\n",
      "  timers:\n",
      "    learn_throughput: 1097.463\n",
      "    learn_time_ms: 3644.769\n",
      "    load_throughput: 9063866.018\n",
      "    load_time_ms: 0.441\n",
      "    sample_throughput: 709.569\n",
      "    sample_time_ms: 5637.222\n",
      "    update_time_ms: 2.705\n",
      "  timestamp: 1641218591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:03:15 (running for 00:00:28.13)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         18.0546</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   44.07</td><td style=\"text-align: right;\">                 155</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             44.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-03-17\n",
      "  done: false\n",
      "  episode_len_mean: 68.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 302.0\n",
      "  episode_reward_mean: 68.05\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 42\n",
      "  episodes_total: 295\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5704205632209778\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010718091391026974\n",
      "          model: {}\n",
      "          policy_loss: -0.024493379518389702\n",
      "          total_loss: 751.3574829101562\n",
      "          vf_explained_var: 0.02921275421977043\n",
      "          vf_loss: 751.3787841796875\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.72\n",
      "    ram_util_percent: 14.699999999999998\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08778473035640941\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09102275350780896\n",
      "    mean_inference_ms: 1.2504687267878412\n",
      "    mean_raw_obs_processing_ms: 0.1373428125292347\n",
      "  time_since_restore: 24.762033939361572\n",
      "  time_this_iter_s: 6.707473278045654\n",
      "  time_total_s: 24.762033939361572\n",
      "  timers:\n",
      "    learn_throughput: 1105.389\n",
      "    learn_time_ms: 3618.636\n",
      "    load_throughput: 10412008.275\n",
      "    load_time_ms: 0.384\n",
      "    sample_throughput: 547.505\n",
      "    sample_time_ms: 7305.871\n",
      "    update_time_ms: 2.513\n",
      "  timestamp: 1641218597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:03:20 (running for 00:00:33.88)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">          24.762</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   68.05</td><td style=\"text-align: right;\">                 302</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             68.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:03:25 (running for 00:00:38.88)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">          24.762</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   68.05</td><td style=\"text-align: right;\">                 302</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             68.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:03:30 (running for 00:00:43.89)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">          24.762</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   68.05</td><td style=\"text-align: right;\">                 302</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             68.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-03-35\n",
      "  done: false\n",
      "  episode_len_mean: 95.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 381.0\n",
      "  episode_reward_mean: 95.33\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 314\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 287.2\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 287.2\n",
      "    episode_reward_min: 43.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 487\n",
      "      - 43\n",
      "      - 352\n",
      "      - 232\n",
      "      - 500\n",
      "      - 174\n",
      "      - 133\n",
      "      - 200\n",
      "      - 356\n",
      "      - 317\n",
      "      - 209\n",
      "      - 500\n",
      "      - 265\n",
      "      - 343\n",
      "      - 303\n",
      "      - 203\n",
      "      - 295\n",
      "      - 171\n",
      "      - 161\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 487.0\n",
      "      - 43.0\n",
      "      - 352.0\n",
      "      - 232.0\n",
      "      - 500.0\n",
      "      - 174.0\n",
      "      - 133.0\n",
      "      - 200.0\n",
      "      - 356.0\n",
      "      - 317.0\n",
      "      - 209.0\n",
      "      - 500.0\n",
      "      - 265.0\n",
      "      - 343.0\n",
      "      - 303.0\n",
      "      - 203.0\n",
      "      - 295.0\n",
      "      - 171.0\n",
      "      - 161.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.103601550109273\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10785361105748245\n",
      "      mean_inference_ms: 1.466613472433423\n",
      "      mean_raw_obs_processing_ms: 0.13105193905382556\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5465356707572937\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007568166125565767\n",
      "          model: {}\n",
      "          policy_loss: -0.015615947544574738\n",
      "          total_loss: 708.155517578125\n",
      "          vf_explained_var: 0.20805206894874573\n",
      "          vf_loss: 708.1689453125\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.816\n",
      "    ram_util_percent: 14.699999999999998\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08717517810222543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09016774635886525\n",
      "    mean_inference_ms: 1.2416017627983382\n",
      "    mean_raw_obs_processing_ms: 0.1356395031267203\n",
      "  time_since_restore: 41.976891040802\n",
      "  time_this_iter_s: 17.21485710144043\n",
      "  time_total_s: 41.976891040802\n",
      "  timers:\n",
      "    learn_throughput: 1110.73\n",
      "    learn_time_ms: 3601.236\n",
      "    load_throughput: 11450070.636\n",
      "    load_time_ms: 0.349\n",
      "    sample_throughput: 557.252\n",
      "    sample_time_ms: 7178.084\n",
      "    update_time_ms: 2.439\n",
      "  timestamp: 1641218615\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:03:36 (running for 00:00:49.14)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         41.9769</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   95.33</td><td style=\"text-align: right;\">                 381</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             95.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:03:41 (running for 00:00:54.19)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         41.9769</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   95.33</td><td style=\"text-align: right;\">                 381</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             95.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-03-42\n",
      "  done: false\n",
      "  episode_len_mean: 130.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 468.0\n",
      "  episode_reward_mean: 130.09\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 328\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5532336235046387\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0021796037908643484\n",
      "          model: {}\n",
      "          policy_loss: -0.008270847611129284\n",
      "          total_loss: 653.2655639648438\n",
      "          vf_explained_var: 0.3292525112628937\n",
      "          vf_loss: 653.2731323242188\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.88888888888889\n",
      "    ram_util_percent: 14.699999999999998\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08652999240273596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08971893637453668\n",
      "    mean_inference_ms: 1.2336347038328832\n",
      "    mean_raw_obs_processing_ms: 0.13325369271601153\n",
      "  time_since_restore: 48.83925485610962\n",
      "  time_this_iter_s: 6.862363815307617\n",
      "  time_total_s: 48.83925485610962\n",
      "  timers:\n",
      "    learn_throughput: 1111.914\n",
      "    learn_time_ms: 3597.401\n",
      "    load_throughput: 10016248.358\n",
      "    load_time_ms: 0.399\n",
      "    sample_throughput: 434.091\n",
      "    sample_time_ms: 9214.649\n",
      "    update_time_ms: 2.334\n",
      "  timestamp: 1641218622\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:03:47 (running for 00:01:00.05)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         48.8393</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  130.09</td><td style=\"text-align: right;\">                 468</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            130.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:03:52 (running for 00:01:05.06)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         48.8393</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  130.09</td><td style=\"text-align: right;\">                 468</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            130.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:03:57 (running for 00:01:10.07)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         48.8393</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  130.09</td><td style=\"text-align: right;\">                 468</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            130.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:04:02 (running for 00:01:15.08)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         48.8393</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  130.09</td><td style=\"text-align: right;\">                 468</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            130.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-04-03\n",
      "  done: false\n",
      "  episode_len_mean: 164.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 164.53\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 338\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 384.4\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 384.4\n",
      "    episode_reward_min: 125.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 298\n",
      "      - 500\n",
      "      - 385\n",
      "      - 357\n",
      "      - 347\n",
      "      - 500\n",
      "      - 500\n",
      "      - 129\n",
      "      - 478\n",
      "      - 451\n",
      "      - 125\n",
      "      - 499\n",
      "      - 410\n",
      "      - 500\n",
      "      - 500\n",
      "      - 494\n",
      "      - 264\n",
      "      - 366\n",
      "      - 239\n",
      "      - 346\n",
      "      episode_reward:\n",
      "      - 298.0\n",
      "      - 500.0\n",
      "      - 385.0\n",
      "      - 357.0\n",
      "      - 347.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 129.0\n",
      "      - 478.0\n",
      "      - 451.0\n",
      "      - 125.0\n",
      "      - 499.0\n",
      "      - 410.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 494.0\n",
      "      - 264.0\n",
      "      - 366.0\n",
      "      - 239.0\n",
      "      - 346.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10419220104190537\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10858886600476228\n",
      "      mean_inference_ms: 1.4771344071877412\n",
      "      mean_raw_obs_processing_ms: 0.13046517391148396\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5533980131149292\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009624061174690723\n",
      "          model: {}\n",
      "          policy_loss: -0.014212304726243019\n",
      "          total_loss: 493.15447998046875\n",
      "          vf_explained_var: 0.3040914237499237\n",
      "          vf_loss: 493.16729736328125\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.083333333333334\n",
      "    ram_util_percent: 14.699999999999996\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08614217889199395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0894956111760803\n",
      "    mean_inference_ms: 1.2289469473594294\n",
      "    mean_raw_obs_processing_ms: 0.13145915670384883\n",
      "  time_since_restore: 69.93320608139038\n",
      "  time_this_iter_s: 21.09395122528076\n",
      "  time_total_s: 69.93320608139038\n",
      "  timers:\n",
      "    learn_throughput: 1113.74\n",
      "    learn_time_ms: 3591.503\n",
      "    load_throughput: 9262357.011\n",
      "    load_time_ms: 0.432\n",
      "    sample_throughput: 452.099\n",
      "    sample_time_ms: 8847.629\n",
      "    update_time_ms: 2.323\n",
      "  timestamp: 1641218643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:04:07 (running for 00:01:20.20)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         69.9332</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  164.53</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            164.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-04-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 200.09\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 348\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5473538041114807\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005386812146753073\n",
      "          model: {}\n",
      "          policy_loss: -0.015734072774648666\n",
      "          total_loss: 363.15887451171875\n",
      "          vf_explained_var: 0.3116520941257477\n",
      "          vf_loss: 363.173828125\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.24\n",
      "    ram_util_percent: 14.699999999999998\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571869856103553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08923405249165679\n",
      "    mean_inference_ms: 1.2237997307964061\n",
      "    mean_raw_obs_processing_ms: 0.1295224818766246\n",
      "  time_since_restore: 76.7877676486969\n",
      "  time_this_iter_s: 6.8545615673065186\n",
      "  time_total_s: 76.7877676486969\n",
      "  timers:\n",
      "    learn_throughput: 1114.855\n",
      "    learn_time_ms: 3587.91\n",
      "    load_throughput: 8810901.943\n",
      "    load_time_ms: 0.454\n",
      "    sample_throughput: 377.704\n",
      "    sample_time_ms: 10590.309\n",
      "    update_time_ms: 2.317\n",
      "  timestamp: 1641218650\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:04:13 (running for 00:01:26.10)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         76.7878</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  200.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            200.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:04:18 (running for 00:01:31.10)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         76.7878</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  200.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            200.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:04:23 (running for 00:01:36.12)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         76.7878</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  200.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            200.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:04:28 (running for 00:01:41.12)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         76.7878</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  200.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            200.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-04-32\n",
      "  done: false\n",
      "  episode_len_mean: 230.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 230.39\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 357\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 439.3\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 439.3\n",
      "    episode_reward_min: 326.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 455\n",
      "      - 500\n",
      "      - 499\n",
      "      - 368\n",
      "      - 444\n",
      "      - 500\n",
      "      - 326\n",
      "      - 400\n",
      "      - 477\n",
      "      - 438\n",
      "      - 484\n",
      "      - 389\n",
      "      - 398\n",
      "      - 492\n",
      "      - 498\n",
      "      - 500\n",
      "      - 356\n",
      "      - 425\n",
      "      - 419\n",
      "      - 418\n",
      "      episode_reward:\n",
      "      - 455.0\n",
      "      - 500.0\n",
      "      - 499.0\n",
      "      - 368.0\n",
      "      - 444.0\n",
      "      - 500.0\n",
      "      - 326.0\n",
      "      - 400.0\n",
      "      - 477.0\n",
      "      - 438.0\n",
      "      - 484.0\n",
      "      - 389.0\n",
      "      - 398.0\n",
      "      - 492.0\n",
      "      - 498.0\n",
      "      - 500.0\n",
      "      - 356.0\n",
      "      - 425.0\n",
      "      - 419.0\n",
      "      - 418.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10359101089309472\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10825800656606037\n",
      "      mean_inference_ms: 1.469265066360381\n",
      "      mean_raw_obs_processing_ms: 0.12926868794290872\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5391750335693359\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0011333998991176486\n",
      "          model: {}\n",
      "          policy_loss: -0.004315619822591543\n",
      "          total_loss: 278.4325866699219\n",
      "          vf_explained_var: 0.39446210861206055\n",
      "          vf_loss: 278.4367370605469\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.512121212121214\n",
      "    ram_util_percent: 14.7\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08555317328245637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08908416915850634\n",
      "    mean_inference_ms: 1.221571767338322\n",
      "    mean_raw_obs_processing_ms: 0.1283460202410181\n",
      "  time_since_restore: 99.55124139785767\n",
      "  time_this_iter_s: 22.763473749160767\n",
      "  time_total_s: 99.55124139785767\n",
      "  timers:\n",
      "    learn_throughput: 1115.152\n",
      "    learn_time_ms: 3586.954\n",
      "    load_throughput: 8476552.229\n",
      "    load_time_ms: 0.472\n",
      "    sample_throughput: 394.549\n",
      "    sample_time_ms: 10138.17\n",
      "    update_time_ms: 2.342\n",
      "  timestamp: 1641218672\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:04:34 (running for 00:01:46.91)<br>Memory usage on this node: 9.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         99.5512</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  230.39</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            230.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:04:39 (running for 00:01:51.92)<br>Memory usage on this node: 9.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         99.5512</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  230.39</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            230.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-04-39\n",
      "  done: false\n",
      "  episode_len_mean: 265.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 265.61\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 367\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5366741418838501\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005787991918623447\n",
      "          model: {}\n",
      "          policy_loss: -0.006539617199450731\n",
      "          total_loss: 211.89959716796875\n",
      "          vf_explained_var: 0.3987596035003662\n",
      "          vf_loss: 211.9057159423828\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.577777777777776\n",
      "    ram_util_percent: 14.933333333333334\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08557139640813415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08889463500127427\n",
      "    mean_inference_ms: 1.2210524372038\n",
      "    mean_raw_obs_processing_ms: 0.12779308597202949\n",
      "  time_since_restore: 106.22643852233887\n",
      "  time_this_iter_s: 6.675197124481201\n",
      "  time_total_s: 106.22643852233887\n",
      "  timers:\n",
      "    learn_throughput: 1115.13\n",
      "    learn_time_ms: 3587.025\n",
      "    load_throughput: 8366761.456\n",
      "    load_time_ms: 0.478\n",
      "    sample_throughput: 347.315\n",
      "    sample_time_ms: 11516.92\n",
      "    update_time_ms: 2.334\n",
      "  timestamp: 1641218679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:04:44 (running for 00:01:57.63)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         106.226</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  265.61</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            265.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:04:49 (running for 00:02:02.64)<br>Memory usage on this node: 9.6/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         106.226</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  265.61</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            265.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:04:54 (running for 00:02:07.65)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         106.226</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  265.61</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            265.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:04:59 (running for 00:02:12.66)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         106.226</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  265.61</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            265.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-05-04\n",
      "  done: false\n",
      "  episode_len_mean: 296.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 296.25\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 375\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 490.85\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 490.85\n",
      "    episode_reward_min: 328.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 328\n",
      "      - 489\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 328.0\n",
      "      - 489.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10438377872777478\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10862827733891708\n",
      "      mean_inference_ms: 1.4726777227386596\n",
      "      mean_raw_obs_processing_ms: 0.12928194216206784\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5295220613479614\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0038418364711105824\n",
      "          model: {}\n",
      "          policy_loss: -0.0006827535107731819\n",
      "          total_loss: 160.09071350097656\n",
      "          vf_explained_var: 0.31545019149780273\n",
      "          vf_loss: 160.0911102294922\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.816666666666668\n",
      "    ram_util_percent: 15.211111111111116\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08555639815079796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08879070404086743\n",
      "    mean_inference_ms: 1.2204396206284813\n",
      "    mean_raw_obs_processing_ms: 0.12716585773834224\n",
      "  time_since_restore: 130.9145851135254\n",
      "  time_this_iter_s: 24.688146591186523\n",
      "  time_total_s: 130.9145851135254\n",
      "  timers:\n",
      "    learn_throughput: 1117.854\n",
      "    learn_time_ms: 3578.285\n",
      "    load_throughput: 8603700.513\n",
      "    load_time_ms: 0.465\n",
      "    sample_throughput: 362.115\n",
      "    sample_time_ms: 11046.202\n",
      "    update_time_ms: 2.294\n",
      "  timestamp: 1641218704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:05:05 (running for 00:02:18.37)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         130.915</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  296.25</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            296.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:05:10 (running for 00:02:23.38)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         130.915</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  296.25</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            296.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-05-11\n",
      "  done: false\n",
      "  episode_len_mean: 328.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 328.9\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 383\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5371277332305908\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00666995532810688\n",
      "          model: {}\n",
      "          policy_loss: -0.003983824513852596\n",
      "          total_loss: 316.42291259765625\n",
      "          vf_explained_var: 0.21692527830600739\n",
      "          vf_loss: 316.4266052246094\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.41111111111111\n",
      "    ram_util_percent: 15.199999999999998\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08542178384490012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0887962068387122\n",
      "    mean_inference_ms: 1.2189509552021869\n",
      "    mean_raw_obs_processing_ms: 0.1260542661579706\n",
      "  time_since_restore: 137.7296233177185\n",
      "  time_this_iter_s: 6.815038204193115\n",
      "  time_total_s: 137.7296233177185\n",
      "  timers:\n",
      "    learn_throughput: 1123.942\n",
      "    learn_time_ms: 3558.902\n",
      "    load_throughput: 7970552.52\n",
      "    load_time_ms: 0.502\n",
      "    sample_throughput: 305.151\n",
      "    sample_time_ms: 13108.268\n",
      "    update_time_ms: 2.16\n",
      "  timestamp: 1641218711\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:05:16 (running for 00:02:29.21)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">          137.73</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   328.9</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             328.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:05:21 (running for 00:02:34.22)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">          137.73</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   328.9</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             328.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:05:26 (running for 00:02:39.23)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">          137.73</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   328.9</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             328.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:05:31 (running for 00:02:44.24)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">          137.73</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   328.9</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             328.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-05-34\n",
      "  done: false\n",
      "  episode_len_mean: 357.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 357.43\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 391\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 468.4\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 468.4\n",
      "    episode_reward_min: 96.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 96\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 272\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 96.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 272.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10403236390738033\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10854895972815477\n",
      "      mean_inference_ms: 1.4706466621858083\n",
      "      mean_raw_obs_processing_ms: 0.12903874566439588\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5230477452278137\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00531935365870595\n",
      "          model: {}\n",
      "          policy_loss: -0.0042682974599301815\n",
      "          total_loss: 167.99142456054688\n",
      "          vf_explained_var: 0.47336119413375854\n",
      "          vf_loss: 167.99549865722656\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.670588235294119\n",
      "    ram_util_percent: 15.2\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0852636959671391\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08877552574340805\n",
      "    mean_inference_ms: 1.2171264748876753\n",
      "    mean_raw_obs_processing_ms: 0.12489364432425544\n",
      "  time_since_restore: 161.39606714248657\n",
      "  time_this_iter_s: 23.666443824768066\n",
      "  time_total_s: 161.39606714248657\n",
      "  timers:\n",
      "    learn_throughput: 1122.616\n",
      "    learn_time_ms: 3563.107\n",
      "    load_throughput: 7903342.755\n",
      "    load_time_ms: 0.506\n",
      "    sample_throughput: 306.09\n",
      "    sample_time_ms: 13068.073\n",
      "    update_time_ms: 2.15\n",
      "  timestamp: 1641218734\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:05:37 (running for 00:02:49.93)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         161.396</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  357.43</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            357.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-05-41\n",
      "  done: false\n",
      "  episode_len_mean: 386.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 386.02\n",
      "  episode_reward_min: 46.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 399\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5129420757293701\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007610874250531197\n",
      "          model: {}\n",
      "          policy_loss: -0.004805939272046089\n",
      "          total_loss: 255.95689392089844\n",
      "          vf_explained_var: 0.3979282081127167\n",
      "          vf_loss: 255.96139526367188\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.77\n",
      "    ram_util_percent: 15.199999999999998\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0852034803623139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08870309874930371\n",
      "    mean_inference_ms: 1.2162119487905576\n",
      "    mean_raw_obs_processing_ms: 0.12412120764020189\n",
      "  time_since_restore: 168.15056681632996\n",
      "  time_this_iter_s: 6.754499673843384\n",
      "  time_total_s: 168.15056681632996\n",
      "  timers:\n",
      "    learn_throughput: 1122.473\n",
      "    learn_time_ms: 3563.559\n",
      "    load_throughput: 7457866.287\n",
      "    load_time_ms: 0.536\n",
      "    sample_throughput: 278.073\n",
      "    sample_time_ms: 14384.693\n",
      "    update_time_ms: 2.13\n",
      "  timestamp: 1641218741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:05:42 (running for 00:02:55.73)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         168.151</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  386.02</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  46</td><td style=\"text-align: right;\">            386.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:05:47 (running for 00:03:00.73)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         168.151</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  386.02</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  46</td><td style=\"text-align: right;\">            386.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:05:52 (running for 00:03:05.75)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         168.151</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  386.02</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  46</td><td style=\"text-align: right;\">            386.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:05:57 (running for 00:03:10.75)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         168.151</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  386.02</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  46</td><td style=\"text-align: right;\">            386.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:06:02 (running for 00:03:15.76)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         168.151</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  386.02</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  46</td><td style=\"text-align: right;\">            386.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-06-06\n",
      "  done: false\n",
      "  episode_len_mean: 411.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 411.82\n",
      "  episode_reward_min: 151.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 407\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 495.85\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 495.85\n",
      "    episode_reward_min: 417.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 417\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 417.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10451842787540899\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10888174577231764\n",
      "      mean_inference_ms: 1.4745014024475993\n",
      "      mean_raw_obs_processing_ms: 0.1291573821782182\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4962209463119507\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.001887325313873589\n",
      "          model: {}\n",
      "          policy_loss: 0.0006623755325563252\n",
      "          total_loss: 367.32769775390625\n",
      "          vf_explained_var: 0.07844223827123642\n",
      "          vf_loss: 367.3269348144531\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.98611111111111\n",
      "    ram_util_percent: 15.200000000000001\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08523968097683056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08862041889299808\n",
      "    mean_inference_ms: 1.216292297932043\n",
      "    mean_raw_obs_processing_ms: 0.1237141961647156\n",
      "  time_since_restore: 193.27887725830078\n",
      "  time_this_iter_s: 25.128310441970825\n",
      "  time_total_s: 193.27887725830078\n",
      "  timers:\n",
      "    learn_throughput: 1121.1\n",
      "    learn_time_ms: 3567.924\n",
      "    load_throughput: 6982651.184\n",
      "    load_time_ms: 0.573\n",
      "    sample_throughput: 277.989\n",
      "    sample_time_ms: 14389.071\n",
      "    update_time_ms: 2.124\n",
      "  timestamp: 1641218766\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:06:07 (running for 00:03:20.90)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         193.279</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  411.82</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 151</td><td style=\"text-align: right;\">            411.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:06:13 (running for 00:03:25.91)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         193.279</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  411.82</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 151</td><td style=\"text-align: right;\">            411.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-06-13\n",
      "  done: false\n",
      "  episode_len_mean: 433.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 433.26\n",
      "  episode_reward_min: 183.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 415\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5242279171943665\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004563682246953249\n",
      "          model: {}\n",
      "          policy_loss: -0.0028273649513721466\n",
      "          total_loss: 273.6688232421875\n",
      "          vf_explained_var: 0.2010912299156189\n",
      "          vf_loss: 273.67156982421875\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.211111111111112\n",
      "    ram_util_percent: 15.199999999999998\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08517731439846923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08865648450892305\n",
      "    mean_inference_ms: 1.2155435508958836\n",
      "    mean_raw_obs_processing_ms: 0.12294328558056254\n",
      "  time_since_restore: 200.02891850471497\n",
      "  time_this_iter_s: 6.750041246414185\n",
      "  time_total_s: 200.02891850471497\n",
      "  timers:\n",
      "    learn_throughput: 1121.374\n",
      "    learn_time_ms: 3567.051\n",
      "    load_throughput: 7401277.572\n",
      "    load_time_ms: 0.54\n",
      "    sample_throughput: 263.764\n",
      "    sample_time_ms: 15165.074\n",
      "    update_time_ms: 2.148\n",
      "  timestamp: 1641218773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:06:18 (running for 00:03:31.69)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         200.029</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  433.26</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 183</td><td style=\"text-align: right;\">            433.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:06:23 (running for 00:03:36.70)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         200.029</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  433.26</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 183</td><td style=\"text-align: right;\">            433.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:06:28 (running for 00:03:41.70)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         200.029</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  433.26</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 183</td><td style=\"text-align: right;\">            433.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:06:33 (running for 00:03:46.72)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         200.029</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  433.26</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 183</td><td style=\"text-align: right;\">            433.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-06-38\n",
      "  done: false\n",
      "  episode_len_mean: 446.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 446.15\n",
      "  episode_reward_min: 183.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 423\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 494.95\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 494.95\n",
      "    episode_reward_min: 399.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 399\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 399.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10414777282340922\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10864103677841878\n",
      "      mean_inference_ms: 1.4706357558067638\n",
      "      mean_raw_obs_processing_ms: 0.12884183231380444\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5123995542526245\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005246020387858152\n",
      "          model: {}\n",
      "          policy_loss: -0.0010477987816557288\n",
      "          total_loss: 389.2311096191406\n",
      "          vf_explained_var: 0.2459811270236969\n",
      "          vf_loss: 389.23211669921875\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.155555555555555\n",
      "    ram_util_percent: 15.200000000000001\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0851920726725619\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08862454268787673\n",
      "    mean_inference_ms: 1.21555213432931\n",
      "    mean_raw_obs_processing_ms: 0.1225304911312247\n",
      "  time_since_restore: 224.6600697040558\n",
      "  time_this_iter_s: 24.63115119934082\n",
      "  time_total_s: 224.6600697040558\n",
      "  timers:\n",
      "    learn_throughput: 1121.373\n",
      "    learn_time_ms: 3567.056\n",
      "    load_throughput: 7402257.225\n",
      "    load_time_ms: 0.54\n",
      "    sample_throughput: 263.935\n",
      "    sample_time_ms: 15155.252\n",
      "    update_time_ms: 2.197\n",
      "  timestamp: 1641218798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:06:39 (running for 00:03:52.37)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">          224.66</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  446.15</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 183</td><td style=\"text-align: right;\">            446.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:06:44 (running for 00:03:57.38)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">          224.66</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  446.15</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 183</td><td style=\"text-align: right;\">            446.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-06-45\n",
      "  done: false\n",
      "  episode_len_mean: 460.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 460.43\n",
      "  episode_reward_min: 183.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 431\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5092872381210327\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007927567698061466\n",
      "          model: {}\n",
      "          policy_loss: -0.003821692895144224\n",
      "          total_loss: 240.9246063232422\n",
      "          vf_explained_var: 0.4408414363861084\n",
      "          vf_loss: 240.92835998535156\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.22222222222222\n",
      "    ram_util_percent: 15.199999999999998\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08517407757360687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08863017982943643\n",
      "    mean_inference_ms: 1.2152506235452774\n",
      "    mean_raw_obs_processing_ms: 0.12202872822629442\n",
      "  time_since_restore: 231.4970166683197\n",
      "  time_this_iter_s: 6.836946964263916\n",
      "  time_total_s: 231.4970166683197\n",
      "  timers:\n",
      "    learn_throughput: 1121.306\n",
      "    learn_time_ms: 3567.268\n",
      "    load_throughput: 7457866.287\n",
      "    load_time_ms: 0.536\n",
      "    sample_throughput: 257.801\n",
      "    sample_time_ms: 15515.862\n",
      "    update_time_ms: 2.171\n",
      "  timestamp: 1641218805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:06:50 (running for 00:04:03.25)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         231.497</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  460.43</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 183</td><td style=\"text-align: right;\">            460.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:06:55 (running for 00:04:08.26)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         231.497</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  460.43</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 183</td><td style=\"text-align: right;\">            460.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:07:00 (running for 00:04:13.27)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         231.497</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  460.43</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 183</td><td style=\"text-align: right;\">            460.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:07:05 (running for 00:04:18.28)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         231.497</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  460.43</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 183</td><td style=\"text-align: right;\">            460.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-07-09\n",
      "  done: false\n",
      "  episode_len_mean: 470.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 470.2\n",
      "  episode_reward_min: 197.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 439\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10378173512994895\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10819615104799825\n",
      "      mean_inference_ms: 1.4652776683677176\n",
      "      mean_raw_obs_processing_ms: 0.12847057022249328\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.486228346824646\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004122937098145485\n",
      "          model: {}\n",
      "          policy_loss: -0.001634713145904243\n",
      "          total_loss: 276.7503967285156\n",
      "          vf_explained_var: 0.3518225848674774\n",
      "          vf_loss: 276.7519836425781\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.488571428571428\n",
      "    ram_util_percent: 15.2\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0850930601855725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08857518286645366\n",
      "    mean_inference_ms: 1.214211898595452\n",
      "    mean_raw_obs_processing_ms: 0.12152144392519632\n",
      "  time_since_restore: 255.9911286830902\n",
      "  time_this_iter_s: 24.494112014770508\n",
      "  time_total_s: 255.9911286830902\n",
      "  timers:\n",
      "    learn_throughput: 1122.286\n",
      "    learn_time_ms: 3564.154\n",
      "    load_throughput: 7508264.041\n",
      "    load_time_ms: 0.533\n",
      "    sample_throughput: 258.031\n",
      "    sample_time_ms: 15502.023\n",
      "    update_time_ms: 2.202\n",
      "  timestamp: 1641218829\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:07:10 (running for 00:04:23.79)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         255.991</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">   470.2</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 197</td><td style=\"text-align: right;\">             470.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:07:15 (running for 00:04:28.80)<br>Memory usage on this node: 9.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         255.991</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">   470.2</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 197</td><td style=\"text-align: right;\">             470.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-07-16\n",
      "  done: false\n",
      "  episode_len_mean: 479.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 479.67\n",
      "  episode_reward_min: 301.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 447\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004687500186264515\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.47082483768463135\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008522473275661469\n",
      "          model: {}\n",
      "          policy_loss: -0.0013112289598211646\n",
      "          total_loss: 319.82659912109375\n",
      "          vf_explained_var: 0.10837756842374802\n",
      "          vf_loss: 319.827880859375\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.739999999999998\n",
      "    ram_util_percent: 15.199999999999998\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08506156700795865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08853830386256582\n",
      "    mean_inference_ms: 1.2137618799236711\n",
      "    mean_raw_obs_processing_ms: 0.12118309558750662\n",
      "  time_since_restore: 262.81436109542847\n",
      "  time_this_iter_s: 6.823232412338257\n",
      "  time_total_s: 262.81436109542847\n",
      "  timers:\n",
      "    learn_throughput: 1122.938\n",
      "    learn_time_ms: 3562.085\n",
      "    load_throughput: 7421248.286\n",
      "    load_time_ms: 0.539\n",
      "    sample_throughput: 254.701\n",
      "    sample_time_ms: 15704.704\n",
      "    update_time_ms: 2.249\n",
      "  timestamp: 1641218836\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:07:21 (running for 00:04:34.66)<br>Memory usage on this node: 9.6/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         262.814</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  479.67</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            479.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:07:26 (running for 00:04:39.67)<br>Memory usage on this node: 9.6/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         262.814</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  479.67</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            479.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:07:31 (running for 00:04:44.68)<br>Memory usage on this node: 9.6/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         262.814</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  479.67</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            479.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:07:36 (running for 00:04:49.69)<br>Memory usage on this node: 9.6/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         262.814</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  479.67</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            479.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:07:41 (running for 00:04:54.69)<br>Memory usage on this node: 9.6/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         262.814</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  479.67</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            479.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-07-42\n",
      "  done: false\n",
      "  episode_len_mean: 485.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 485.78\n",
      "  episode_reward_min: 301.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 455\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1039983627326477\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10853457281779563\n",
      "      mean_inference_ms: 1.4690219331021976\n",
      "      mean_raw_obs_processing_ms: 0.12874712562998203\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004687500186264515\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.47128868103027344\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003936439752578735\n",
      "          model: {}\n",
      "          policy_loss: 0.0010589719749987125\n",
      "          total_loss: 455.78314208984375\n",
      "          vf_explained_var: -0.05514096841216087\n",
      "          vf_loss: 455.78204345703125\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.554054054054054\n",
      "    ram_util_percent: 15.28378378378378\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08501494108608094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08845245875626065\n",
      "    mean_inference_ms: 1.2130609051742272\n",
      "    mean_raw_obs_processing_ms: 0.12091058145505018\n",
      "  time_since_restore: 288.32518672943115\n",
      "  time_this_iter_s: 25.510825634002686\n",
      "  time_total_s: 288.32518672943115\n",
      "  timers:\n",
      "    learn_throughput: 1121.05\n",
      "    learn_time_ms: 3568.082\n",
      "    load_throughput: 7532873.563\n",
      "    load_time_ms: 0.531\n",
      "    sample_throughput: 254.332\n",
      "    sample_time_ms: 15727.448\n",
      "    update_time_ms: 2.323\n",
      "  timestamp: 1641218862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:07:47 (running for 00:05:00.23)<br>Memory usage on this node: 9.6/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         288.325</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  485.78</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            485.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-07-49\n",
      "  done: false\n",
      "  episode_len_mean: 490.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 490.01\n",
      "  episode_reward_min: 301.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 463\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023437500931322575\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.47698354721069336\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0047984798438847065\n",
      "          model: {}\n",
      "          policy_loss: -0.0006879530847072601\n",
      "          total_loss: 499.99786376953125\n",
      "          vf_explained_var: 0.13831651210784912\n",
      "          vf_loss: 499.99853515625\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.27777777777778\n",
      "    ram_util_percent: 15.300000000000002\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08498411643948402\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0883828183959159\n",
      "    mean_inference_ms: 1.2126203085685214\n",
      "    mean_raw_obs_processing_ms: 0.1206833143388496\n",
      "  time_since_restore: 295.0958046913147\n",
      "  time_this_iter_s: 6.770617961883545\n",
      "  time_total_s: 295.0958046913147\n",
      "  timers:\n",
      "    learn_throughput: 1119.777\n",
      "    learn_time_ms: 3572.139\n",
      "    load_throughput: 7652442.985\n",
      "    load_time_ms: 0.523\n",
      "    sample_throughput: 253.535\n",
      "    sample_time_ms: 15776.913\n",
      "    update_time_ms: 2.401\n",
      "  timestamp: 1641218869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:07:53 (running for 00:05:06.03)<br>Memory usage on this node: 9.6/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         295.096</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  490.01</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            490.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:07:58 (running for 00:05:11.04)<br>Memory usage on this node: 9.7/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         295.096</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  490.01</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            490.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:08:03 (running for 00:05:16.05)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         295.096</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  490.01</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            490.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:08:08 (running for 00:05:21.06)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         295.096</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  490.01</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            490.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:08:13 (running for 00:05:26.07)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         295.096</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  490.01</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            490.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-08-13\n",
      "  done: false\n",
      "  episode_len_mean: 494.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 494.04\n",
      "  episode_reward_min: 301.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 471\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10398758663071099\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10860021071970312\n",
      "      mean_inference_ms: 1.467067385849446\n",
      "      mean_raw_obs_processing_ms: 0.12872744366273867\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.44751331210136414\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005207758862525225\n",
      "          model: {}\n",
      "          policy_loss: -0.0014769152039662004\n",
      "          total_loss: 508.75994873046875\n",
      "          vf_explained_var: -0.12145107984542847\n",
      "          vf_loss: 508.7613830566406\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.794444444444444\n",
      "    ram_util_percent: 15.725000000000001\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08495669371953544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08835353880709226\n",
      "    mean_inference_ms: 1.2123494414687537\n",
      "    mean_raw_obs_processing_ms: 0.12044908972544924\n",
      "  time_since_restore: 319.9055714607239\n",
      "  time_this_iter_s: 24.80976676940918\n",
      "  time_total_s: 319.9055714607239\n",
      "  timers:\n",
      "    learn_throughput: 1121.564\n",
      "    learn_time_ms: 3566.449\n",
      "    load_throughput: 8097893.619\n",
      "    load_time_ms: 0.494\n",
      "    sample_throughput: 253.196\n",
      "    sample_time_ms: 15798.066\n",
      "    update_time_ms: 2.453\n",
      "  timestamp: 1641218893\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:08:19 (running for 00:05:31.90)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         319.906</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  494.04</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            494.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-08-21\n",
      "  done: false\n",
      "  episode_len_mean: 494.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 494.74\n",
      "  episode_reward_min: 301.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 479\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4273841381072998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0035229744389653206\n",
      "          model: {}\n",
      "          policy_loss: 2.8932990971952677e-05\n",
      "          total_loss: 513.3271484375\n",
      "          vf_explained_var: -0.16931083798408508\n",
      "          vf_loss: 513.3270874023438\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.270000000000003\n",
      "    ram_util_percent: 16.1\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0849576518057562\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08835138219631865\n",
      "    mean_inference_ms: 1.2123759100677487\n",
      "    mean_raw_obs_processing_ms: 0.12029157908743951\n",
      "  time_since_restore: 327.0281367301941\n",
      "  time_this_iter_s: 7.122565269470215\n",
      "  time_total_s: 327.0281367301941\n",
      "  timers:\n",
      "    learn_throughput: 1120.083\n",
      "    learn_time_ms: 3571.165\n",
      "    load_throughput: 8035065.134\n",
      "    load_time_ms: 0.498\n",
      "    sample_throughput: 251.121\n",
      "    sample_time_ms: 15928.597\n",
      "    update_time_ms: 2.481\n",
      "  timestamp: 1641218901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:08:24 (running for 00:05:37.06)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         327.028</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  494.74</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            494.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:08:29 (running for 00:05:42.07)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         327.028</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  494.74</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            494.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:08:34 (running for 00:05:47.07)<br>Memory usage on this node: 10.1/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         327.028</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  494.74</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            494.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:08:39 (running for 00:05:52.08)<br>Memory usage on this node: 9.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         327.028</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  494.74</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            494.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:08:44 (running for 00:05:57.09)<br>Memory usage on this node: 9.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         327.028</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  494.74</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 301</td><td style=\"text-align: right;\">            494.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-08-46\n",
      "  done: false\n",
      "  episode_len_mean: 497.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.35\n",
      "  episode_reward_min: 319.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 487\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10437089455763027\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10900126286816818\n",
      "      mean_inference_ms: 1.470011145586835\n",
      "      mean_raw_obs_processing_ms: 0.1290778086197522\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.44096314907073975\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0060227313078939915\n",
      "          model: {}\n",
      "          policy_loss: -0.003832670161500573\n",
      "          total_loss: 466.02789306640625\n",
      "          vf_explained_var: -0.025495218113064766\n",
      "          vf_loss: 466.03167724609375\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.633333333333336\n",
      "    ram_util_percent: 16.008333333333333\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08498740099717134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08837922905551737\n",
      "    mean_inference_ms: 1.2126949041690178\n",
      "    mean_raw_obs_processing_ms: 0.12019532399665785\n",
      "  time_since_restore: 352.60818672180176\n",
      "  time_this_iter_s: 25.580049991607666\n",
      "  time_total_s: 352.60818672180176\n",
      "  timers:\n",
      "    learn_throughput: 1118.902\n",
      "    learn_time_ms: 3574.932\n",
      "    load_throughput: 8118662.473\n",
      "    load_time_ms: 0.493\n",
      "    sample_throughput: 250.848\n",
      "    sample_time_ms: 15945.885\n",
      "    update_time_ms: 2.539\n",
      "  timestamp: 1641218926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:08:49 (running for 00:06:02.68)<br>Memory usage on this node: 9.9/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         352.608</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">  497.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 319</td><td style=\"text-align: right;\">            497.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-08-53\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 495\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4272977411746979\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0032196741085499525\n",
      "          model: {}\n",
      "          policy_loss: -0.0005767709226347506\n",
      "          total_loss: 510.8578186035156\n",
      "          vf_explained_var: -0.13966988027095795\n",
      "          vf_loss: 510.8584289550781\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.11\n",
      "    ram_util_percent: 15.969999999999999\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08505613248483819\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08844964924985362\n",
      "    mean_inference_ms: 1.2134933609350684\n",
      "    mean_raw_obs_processing_ms: 0.12015787646001934\n",
      "  time_since_restore: 359.4879832267761\n",
      "  time_this_iter_s: 6.879796504974365\n",
      "  time_total_s: 359.4879832267761\n",
      "  timers:\n",
      "    learn_throughput: 1119.808\n",
      "    learn_time_ms: 3572.041\n",
      "    load_throughput: 7653490.26\n",
      "    load_time_ms: 0.523\n",
      "    sample_throughput: 250.092\n",
      "    sample_time_ms: 15994.088\n",
      "    update_time_ms: 2.611\n",
      "  timestamp: 1641218933\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:08:55 (running for 00:06:08.60)<br>Memory usage on this node: 10.1/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         359.488</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:09:00 (running for 00:06:13.61)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         359.488</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:09:05 (running for 00:06:18.62)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         359.488</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:09:10 (running for 00:06:23.63)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         359.488</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:09:15 (running for 00:06:28.64)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         359.488</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-09-19\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 503\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10470626927118024\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10943868652450381\n",
      "      mean_inference_ms: 1.472916995680786\n",
      "      mean_raw_obs_processing_ms: 0.12935875834434002\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0002929687616415322\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4326760768890381\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037711968179792166\n",
      "          model: {}\n",
      "          policy_loss: -1.970324410649482e-05\n",
      "          total_loss: 452.84014892578125\n",
      "          vf_explained_var: 0.041707735508680344\n",
      "          vf_loss: 452.8401794433594\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.783783783783782\n",
      "    ram_util_percent: 16.31891891891891\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08513656913698521\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08853071585099294\n",
      "    mean_inference_ms: 1.2143864933354909\n",
      "    mean_raw_obs_processing_ms: 0.12014701414313325\n",
      "  time_since_restore: 385.1092128753662\n",
      "  time_this_iter_s: 25.621229648590088\n",
      "  time_total_s: 385.1092128753662\n",
      "  timers:\n",
      "    learn_throughput: 1117.319\n",
      "    learn_time_ms: 3579.999\n",
      "    load_throughput: 7345862.779\n",
      "    load_time_ms: 0.545\n",
      "    sample_throughput: 250.104\n",
      "    sample_time_ms: 15993.338\n",
      "    update_time_ms: 2.615\n",
      "  timestamp: 1641218959\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:09:21 (running for 00:06:34.28)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         385.109</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:09:26 (running for 00:06:39.29)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         385.109</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-09-26\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 511\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001464843808207661\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4448593556880951\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0036000448744744062\n",
      "          model: {}\n",
      "          policy_loss: -0.0015051986556500196\n",
      "          total_loss: 500.0770568847656\n",
      "          vf_explained_var: -0.16130271553993225\n",
      "          vf_loss: 500.07855224609375\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.71\n",
      "    ram_util_percent: 16.300000000000004\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08525711472499972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0886551838479516\n",
      "    mean_inference_ms: 1.2157331162790306\n",
      "    mean_raw_obs_processing_ms: 0.12019524876923668\n",
      "  time_since_restore: 392.2682750225067\n",
      "  time_this_iter_s: 7.159062147140503\n",
      "  time_total_s: 392.2682750225067\n",
      "  timers:\n",
      "    learn_throughput: 1114.322\n",
      "    learn_time_ms: 3589.627\n",
      "    load_throughput: 7326295.197\n",
      "    load_time_ms: 0.546\n",
      "    sample_throughput: 248.244\n",
      "    sample_time_ms: 16113.181\n",
      "    update_time_ms: 2.615\n",
      "  timestamp: 1641218966\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:09:31 (running for 00:06:44.48)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         392.268</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:09:36 (running for 00:06:49.49)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         392.268</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:09:41 (running for 00:06:54.50)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         392.268</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:09:46 (running for 00:06:59.50)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         392.268</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:09:51 (running for 00:07:04.52)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         392.268</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-09-52\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 519\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10499257485336659\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10970074687022222\n",
      "      mean_inference_ms: 1.4742904209589376\n",
      "      mean_raw_obs_processing_ms: 0.1295605294873567\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.324219041038305e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.40209901332855225\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006361774634569883\n",
      "          model: {}\n",
      "          policy_loss: 0.0007337695569731295\n",
      "          total_loss: 504.7364807128906\n",
      "          vf_explained_var: -0.14675606787204742\n",
      "          vf_loss: 504.7357482910156\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.98888888888889\n",
      "    ram_util_percent: 16.391666666666666\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08537920401126049\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0887812427364166\n",
      "    mean_inference_ms: 1.2170331807078576\n",
      "    mean_raw_obs_processing_ms: 0.12025618147282646\n",
      "  time_since_restore: 417.7934567928314\n",
      "  time_this_iter_s: 25.525181770324707\n",
      "  time_total_s: 417.7934567928314\n",
      "  timers:\n",
      "    learn_throughput: 1111.759\n",
      "    learn_time_ms: 3597.901\n",
      "    load_throughput: 7300154.904\n",
      "    load_time_ms: 0.548\n",
      "    sample_throughput: 247.85\n",
      "    sample_time_ms: 16138.782\n",
      "    update_time_ms: 2.544\n",
      "  timestamp: 1641218992\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:09:57 (running for 00:07:10.06)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         417.793</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-09-59\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 527\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.324219041038305e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.40920424461364746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002381604630500078\n",
      "          model: {}\n",
      "          policy_loss: 0.00015152654668781906\n",
      "          total_loss: 480.72210693359375\n",
      "          vf_explained_var: -0.13251666724681854\n",
      "          vf_loss: 480.72198486328125\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.299999999999997\n",
      "    ram_util_percent: 16.400000000000002\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08550424231595997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08890934167287196\n",
      "    mean_inference_ms: 1.2183282693186042\n",
      "    mean_raw_obs_processing_ms: 0.12033534538465485\n",
      "  time_since_restore: 424.87086033821106\n",
      "  time_this_iter_s: 7.077403545379639\n",
      "  time_total_s: 424.87086033821106\n",
      "  timers:\n",
      "    learn_throughput: 1109.604\n",
      "    learn_time_ms: 3604.888\n",
      "    load_throughput: 7477810.661\n",
      "    load_time_ms: 0.535\n",
      "    sample_throughput: 246.233\n",
      "    sample_time_ms: 16244.787\n",
      "    update_time_ms: 2.546\n",
      "  timestamp: 1641218999\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:10:02 (running for 00:07:15.18)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         424.871</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:10:07 (running for 00:07:20.18)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         424.871</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:10:12 (running for 00:07:25.19)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         424.871</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:10:17 (running for 00:07:30.20)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         424.871</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:10:22 (running for 00:07:35.21)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         424.871</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-10-25\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 535\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10519632889353221\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10998729546079238\n",
      "      mean_inference_ms: 1.4768183822782324\n",
      "      mean_raw_obs_processing_ms: 0.12985516467811864\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.662109520519152e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3790639042854309\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0021645778324455023\n",
      "          model: {}\n",
      "          policy_loss: -8.63347013364546e-05\n",
      "          total_loss: 369.4381103515625\n",
      "          vf_explained_var: 0.13970281183719635\n",
      "          vf_loss: 369.43817138671875\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.950000000000003\n",
      "    ram_util_percent: 16.188888888888883\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08564168551581734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0890500422688131\n",
      "    mean_inference_ms: 1.2197415608309674\n",
      "    mean_raw_obs_processing_ms: 0.12043776440358808\n",
      "  time_since_restore: 450.6688210964203\n",
      "  time_this_iter_s: 25.79796075820923\n",
      "  time_total_s: 450.6688210964203\n",
      "  timers:\n",
      "    learn_throughput: 1106.884\n",
      "    learn_time_ms: 3613.749\n",
      "    load_throughput: 6936746.878\n",
      "    load_time_ms: 0.577\n",
      "    sample_throughput: 246.114\n",
      "    sample_time_ms: 16252.657\n",
      "    update_time_ms: 2.454\n",
      "  timestamp: 1641219025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:10:28 (running for 00:07:41.03)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         450.669</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-10-32\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 543\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.831054760259576e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.35397395491600037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003669464262202382\n",
      "          model: {}\n",
      "          policy_loss: 0.0020771734416484833\n",
      "          total_loss: 306.1070556640625\n",
      "          vf_explained_var: 0.11881746351718903\n",
      "          vf_loss: 306.10498046875\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.56\n",
      "    ram_util_percent: 16.300000000000004\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08579489786876814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08920570261995692\n",
      "    mean_inference_ms: 1.2212952164831268\n",
      "    mean_raw_obs_processing_ms: 0.12056070431634038\n",
      "  time_since_restore: 457.5709397792816\n",
      "  time_this_iter_s: 6.902118682861328\n",
      "  time_total_s: 457.5709397792816\n",
      "  timers:\n",
      "    learn_throughput: 1106.909\n",
      "    learn_time_ms: 3613.668\n",
      "    load_throughput: 6622149.595\n",
      "    load_time_ms: 0.604\n",
      "    sample_throughput: 245.497\n",
      "    sample_time_ms: 16293.505\n",
      "    update_time_ms: 2.527\n",
      "  timestamp: 1641219032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:10:34 (running for 00:07:46.97)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         457.571</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:10:39 (running for 00:07:51.98)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         457.571</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:10:44 (running for 00:07:56.99)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         457.571</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:10:49 (running for 00:08:02.00)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         457.571</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:10:54 (running for 00:08:07.01)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         457.571</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-10-57\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 551\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10525211426233905\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11001754164530235\n",
      "      mean_inference_ms: 1.477397086261321\n",
      "      mean_raw_obs_processing_ms: 0.12992744332424166\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.15527380129788e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3955031633377075\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005982265807688236\n",
      "          model: {}\n",
      "          policy_loss: -0.006582131143659353\n",
      "          total_loss: 430.3440856933594\n",
      "          vf_explained_var: 0.11550504714250565\n",
      "          vf_loss: 430.3506774902344\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.547222222222224\n",
      "    ram_util_percent: 16.333333333333332\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08592650786792298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08933768013103943\n",
      "    mean_inference_ms: 1.2225268195384877\n",
      "    mean_raw_obs_processing_ms: 0.12066028394664259\n",
      "  time_since_restore: 482.8866620063782\n",
      "  time_this_iter_s: 25.315722227096558\n",
      "  time_total_s: 482.8866620063782\n",
      "  timers:\n",
      "    learn_throughput: 1104.392\n",
      "    learn_time_ms: 3621.901\n",
      "    load_throughput: 6514917.676\n",
      "    load_time_ms: 0.614\n",
      "    sample_throughput: 245.45\n",
      "    sample_time_ms: 16296.598\n",
      "    update_time_ms: 2.454\n",
      "  timestamp: 1641219057\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:10:59 (running for 00:08:12.33)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         482.887</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-11-04\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 559\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.15527380129788e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.36912479996681213\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00375203974545002\n",
      "          model: {}\n",
      "          policy_loss: -0.00028518273029476404\n",
      "          total_loss: 267.64056396484375\n",
      "          vf_explained_var: 0.3376942276954651\n",
      "          vf_loss: 267.640869140625\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.66\n",
      "    ram_util_percent: 16.4\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08605415978830827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08946525260319654\n",
      "    mean_inference_ms: 1.2236732799860568\n",
      "    mean_raw_obs_processing_ms: 0.12075519178157565\n",
      "  time_since_restore: 489.84035754203796\n",
      "  time_this_iter_s: 6.95369553565979\n",
      "  time_total_s: 489.84035754203796\n",
      "  timers:\n",
      "    learn_throughput: 1104.308\n",
      "    learn_time_ms: 3622.177\n",
      "    load_throughput: 6610408.195\n",
      "    load_time_ms: 0.605\n",
      "    sample_throughput: 245.003\n",
      "    sample_time_ms: 16326.348\n",
      "    update_time_ms: 2.495\n",
      "  timestamp: 1641219064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:11:05 (running for 00:08:18.32)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">          489.84</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:11:10 (running for 00:08:23.33)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">          489.84</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:11:15 (running for 00:08:28.34)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">          489.84</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:11:20 (running for 00:08:33.35)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">          489.84</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:11:25 (running for 00:08:38.36)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">          489.84</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-11-30\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 567\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10545593671915696\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11019647079835677\n",
      "      mean_inference_ms: 1.4793061404952081\n",
      "      mean_raw_obs_processing_ms: 0.13010217142522756\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.57763690064894e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3780837059020996\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0030176681466400623\n",
      "          model: {}\n",
      "          policy_loss: -0.005992711056023836\n",
      "          total_loss: 195.24624633789062\n",
      "          vf_explained_var: 0.5741875171661377\n",
      "          vf_loss: 195.25222778320312\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.559459459459458\n",
      "    ram_util_percent: 16.399999999999995\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08620918111188271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08962118443729875\n",
      "    mean_inference_ms: 1.2251054367570353\n",
      "    mean_raw_obs_processing_ms: 0.12088584669138079\n",
      "  time_since_restore: 515.6253080368042\n",
      "  time_this_iter_s: 25.784950494766235\n",
      "  time_total_s: 515.6253080368042\n",
      "  timers:\n",
      "    learn_throughput: 1104.236\n",
      "    learn_time_ms: 3622.413\n",
      "    load_throughput: 6478439.974\n",
      "    load_time_ms: 0.617\n",
      "    sample_throughput: 244.833\n",
      "    sample_time_ms: 16337.637\n",
      "    update_time_ms: 2.481\n",
      "  timestamp: 1641219090\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:11:31 (running for 00:08:44.16)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         515.625</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:11:36 (running for 00:08:49.18)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         515.625</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-11-37\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 575\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.28881845032447e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.37484490871429443\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004371098708361387\n",
      "          model: {}\n",
      "          policy_loss: -0.008112717419862747\n",
      "          total_loss: 236.13209533691406\n",
      "          vf_explained_var: 0.3968356251716614\n",
      "          vf_loss: 236.14022827148438\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.14\n",
      "    ram_util_percent: 16.4\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08635856006107019\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08977110877235131\n",
      "    mean_inference_ms: 1.2264426666516675\n",
      "    mean_raw_obs_processing_ms: 0.12100294525359231\n",
      "  time_since_restore: 522.653461933136\n",
      "  time_this_iter_s: 7.028153896331787\n",
      "  time_total_s: 522.653461933136\n",
      "  timers:\n",
      "    learn_throughput: 1102.461\n",
      "    learn_time_ms: 3628.247\n",
      "    load_throughput: 6772652.995\n",
      "    load_time_ms: 0.591\n",
      "    sample_throughput: 244.532\n",
      "    sample_time_ms: 16357.781\n",
      "    update_time_ms: 2.376\n",
      "  timestamp: 1641219097\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:11:41 (running for 00:08:54.23)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         522.653</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:11:46 (running for 00:08:59.24)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         522.653</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:11:51 (running for 00:09:04.25)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         522.653</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:11:56 (running for 00:09:09.26)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         522.653</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:12:01 (running for 00:09:14.27)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         522.653</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-12-03\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 583\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1055698281852336\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11036336660263975\n",
      "      mean_inference_ms: 1.4807301991485073\n",
      "      mean_raw_obs_processing_ms: 0.13036408170709285\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.144409225162235e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3724224269390106\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.001537024276331067\n",
      "          model: {}\n",
      "          policy_loss: 0.001436397316865623\n",
      "          total_loss: 310.681396484375\n",
      "          vf_explained_var: 0.1321326047182083\n",
      "          vf_loss: 310.679931640625\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.394594594594594\n",
      "    ram_util_percent: 16.399999999999995\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08650183333934827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08991451223794593\n",
      "    mean_inference_ms: 1.2277218409628718\n",
      "    mean_raw_obs_processing_ms: 0.1211141995107657\n",
      "  time_since_restore: 548.3374421596527\n",
      "  time_this_iter_s: 25.683980226516724\n",
      "  time_total_s: 548.3374421596527\n",
      "  timers:\n",
      "    learn_throughput: 1103.138\n",
      "    learn_time_ms: 3626.021\n",
      "    load_throughput: 7002761.499\n",
      "    load_time_ms: 0.571\n",
      "    sample_throughput: 244.327\n",
      "    sample_time_ms: 16371.476\n",
      "    update_time_ms: 2.298\n",
      "  timestamp: 1641219123\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:12:07 (running for 00:09:19.97)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         548.337</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-12-10\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 591\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.722046125811175e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3670140504837036\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004765572026371956\n",
      "          model: {}\n",
      "          policy_loss: -0.001426817150786519\n",
      "          total_loss: 253.39633178710938\n",
      "          vf_explained_var: 0.14656348526477814\n",
      "          vf_loss: 253.3977813720703\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.009999999999998\n",
      "    ram_util_percent: 16.45\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08664274676648376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09005666962915886\n",
      "    mean_inference_ms: 1.2289917192076356\n",
      "    mean_raw_obs_processing_ms: 0.12122483136955622\n",
      "  time_since_restore: 555.3021757602692\n",
      "  time_this_iter_s: 6.964733600616455\n",
      "  time_total_s: 555.3021757602692\n",
      "  timers:\n",
      "    learn_throughput: 1104.271\n",
      "    learn_time_ms: 3622.298\n",
      "    load_throughput: 7170363.279\n",
      "    load_time_ms: 0.558\n",
      "    sample_throughput: 244.605\n",
      "    sample_time_ms: 16352.876\n",
      "    update_time_ms: 2.287\n",
      "  timestamp: 1641219130\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:12:13 (running for 00:09:25.96)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         555.302</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:12:18 (running for 00:09:30.97)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         555.302</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:12:23 (running for 00:09:35.98)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         555.302</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:12:28 (running for 00:09:40.99)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         555.302</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:12:33 (running for 00:09:46.00)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         555.302</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-12-35\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 599\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1056883084264603\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11049499303083751\n",
      "      mean_inference_ms: 1.4814367549218157\n",
      "      mean_raw_obs_processing_ms: 0.13050825155298593\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8610230629055877e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.35019469261169434\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0026284728664904833\n",
      "          model: {}\n",
      "          policy_loss: -0.0006408520857803524\n",
      "          total_loss: 273.556884765625\n",
      "          vf_explained_var: 0.08581683039665222\n",
      "          vf_loss: 273.5575256347656\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.15555555555556\n",
      "    ram_util_percent: 16.40555555555555\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08677691028480318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09019320351133775\n",
      "    mean_inference_ms: 1.230197951118256\n",
      "    mean_raw_obs_processing_ms: 0.1213281124267737\n",
      "  time_since_restore: 580.9502477645874\n",
      "  time_this_iter_s: 25.648072004318237\n",
      "  time_total_s: 580.9502477645874\n",
      "  timers:\n",
      "    learn_throughput: 1103.751\n",
      "    learn_time_ms: 3624.006\n",
      "    load_throughput: 7122873.397\n",
      "    load_time_ms: 0.562\n",
      "    sample_throughput: 244.573\n",
      "    sample_time_ms: 16355.03\n",
      "    update_time_ms: 2.339\n",
      "  timestamp: 1641219155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:12:38 (running for 00:09:51.66)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">          580.95</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-12-42\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 607\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.36929523944854736\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0021188026294112206\n",
      "          model: {}\n",
      "          policy_loss: 0.0016992143355309963\n",
      "          total_loss: 518.653564453125\n",
      "          vf_explained_var: -0.11859649419784546\n",
      "          vf_loss: 518.65185546875\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.472727272727273\n",
      "    ram_util_percent: 16.400000000000002\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08691064602767888\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0903306602371648\n",
      "    mean_inference_ms: 1.2314374904692087\n",
      "    mean_raw_obs_processing_ms: 0.12143528794586679\n",
      "  time_since_restore: 587.9842629432678\n",
      "  time_this_iter_s: 7.03401517868042\n",
      "  time_total_s: 587.9842629432678\n",
      "  timers:\n",
      "    learn_throughput: 1104.308\n",
      "    learn_time_ms: 3622.178\n",
      "    load_throughput: 6837517.219\n",
      "    load_time_ms: 0.585\n",
      "    sample_throughput: 244.525\n",
      "    sample_time_ms: 16358.24\n",
      "    update_time_ms: 2.26\n",
      "  timestamp: 1641219162\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:12:43 (running for 00:09:56.74)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         587.984</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:12:48 (running for 00:10:01.75)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         587.984</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:12:53 (running for 00:10:06.75)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         587.984</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:12:58 (running for 00:10:11.76)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         587.984</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:13:03 (running for 00:10:16.77)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         587.984</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-13-08\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 615\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10588397681044988\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11070721510961497\n",
      "      mean_inference_ms: 1.4836034114506993\n",
      "      mean_raw_obs_processing_ms: 0.13064209459160023\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3557019829750061\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0033295017201453447\n",
      "          model: {}\n",
      "          policy_loss: 0.002405454870313406\n",
      "          total_loss: 482.20745849609375\n",
      "          vf_explained_var: -0.17013874650001526\n",
      "          vf_loss: 482.2050476074219\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.180555555555557\n",
      "    ram_util_percent: 16.43333333333333\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08700857717823936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.090432398087041\n",
      "    mean_inference_ms: 1.2322679721808205\n",
      "    mean_raw_obs_processing_ms: 0.12150360449753067\n",
      "  time_since_restore: 613.6633629798889\n",
      "  time_this_iter_s: 25.679100036621094\n",
      "  time_total_s: 613.6633629798889\n",
      "  timers:\n",
      "    learn_throughput: 1104.332\n",
      "    learn_time_ms: 3622.099\n",
      "    load_throughput: 7105376.927\n",
      "    load_time_ms: 0.563\n",
      "    sample_throughput: 244.964\n",
      "    sample_time_ms: 16328.897\n",
      "    update_time_ms: 2.394\n",
      "  timestamp: 1641219188\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:13:09 (running for 00:10:22.47)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         613.663</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:13:14 (running for 00:10:27.48)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         613.663</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-13-15\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 623\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5762788286319847e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3603213131427765\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003602476790547371\n",
      "          model: {}\n",
      "          policy_loss: -0.0006965534412302077\n",
      "          total_loss: 389.4333190917969\n",
      "          vf_explained_var: 0.01961229369044304\n",
      "          vf_loss: 389.4340515136719\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.57\n",
      "    ram_util_percent: 16.5\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08709779304776952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09052600993587923\n",
      "    mean_inference_ms: 1.2330118950540385\n",
      "    mean_raw_obs_processing_ms: 0.1215630679271405\n",
      "  time_since_restore: 620.7084271907806\n",
      "  time_this_iter_s: 7.045064210891724\n",
      "  time_total_s: 620.7084271907806\n",
      "  timers:\n",
      "    learn_throughput: 1104.315\n",
      "    learn_time_ms: 3622.156\n",
      "    load_throughput: 7821545.921\n",
      "    load_time_ms: 0.511\n",
      "    sample_throughput: 244.514\n",
      "    sample_time_ms: 16358.975\n",
      "    update_time_ms: 2.285\n",
      "  timestamp: 1641219195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:13:19 (running for 00:10:32.56)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         620.708</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:13:24 (running for 00:10:37.57)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         620.708</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:13:29 (running for 00:10:42.58)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         620.708</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:13:34 (running for 00:10:47.58)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         620.708</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:13:39 (running for 00:10:52.60)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         620.708</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-13-41\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 631\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10607204638613028\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11083818217352788\n",
      "      mean_inference_ms: 1.4849761483757247\n",
      "      mean_raw_obs_processing_ms: 0.130786341452465\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7881394143159923e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3451884686946869\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004126070998609066\n",
      "          model: {}\n",
      "          policy_loss: 0.00044744188198819757\n",
      "          total_loss: 417.306396484375\n",
      "          vf_explained_var: 0.09019102901220322\n",
      "          vf_loss: 417.30596923828125\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.991891891891893\n",
      "    ram_util_percent: 16.5\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08718426195090867\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0906155088692412\n",
      "    mean_inference_ms: 1.233713997322273\n",
      "    mean_raw_obs_processing_ms: 0.12161935451050133\n",
      "  time_since_restore: 646.4065420627594\n",
      "  time_this_iter_s: 25.69811487197876\n",
      "  time_total_s: 646.4065420627594\n",
      "  timers:\n",
      "    learn_throughput: 1105.671\n",
      "    learn_time_ms: 3617.713\n",
      "    load_throughput: 8130859.746\n",
      "    load_time_ms: 0.492\n",
      "    sample_throughput: 244.394\n",
      "    sample_time_ms: 16366.984\n",
      "    update_time_ms: 2.366\n",
      "  timestamp: 1641219221\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:13:45 (running for 00:10:58.31)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         646.407</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-13-48\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 639\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.940697071579962e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.36183249950408936\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005347676575183868\n",
      "          model: {}\n",
      "          policy_loss: -0.00434082793071866\n",
      "          total_loss: 322.6539611816406\n",
      "          vf_explained_var: 0.3572748899459839\n",
      "          vf_loss: 322.65826416015625\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.11\n",
      "    ram_util_percent: 16.5\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08726081858043465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09069669967043546\n",
      "    mean_inference_ms: 1.234299610588092\n",
      "    mean_raw_obs_processing_ms: 0.1216650497154798\n",
      "  time_since_restore: 653.3494100570679\n",
      "  time_this_iter_s: 6.942867994308472\n",
      "  time_total_s: 653.3494100570679\n",
      "  timers:\n",
      "    learn_throughput: 1106.27\n",
      "    learn_time_ms: 3615.755\n",
      "    load_throughput: 8046626.379\n",
      "    load_time_ms: 0.497\n",
      "    sample_throughput: 243.925\n",
      "    sample_time_ms: 16398.475\n",
      "    update_time_ms: 2.303\n",
      "  timestamp: 1641219228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:13:51 (running for 00:11:04.29)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         653.349</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:13:56 (running for 00:11:09.30)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         653.349</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:14:01 (running for 00:11:14.31)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         653.349</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:14:06 (running for 00:11:19.31)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         653.349</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:14:11 (running for 00:11:24.32)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         653.349</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-14-13\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 647\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1061088163550302\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11085446856347887\n",
      "      mean_inference_ms: 1.484814775875183\n",
      "      mean_raw_obs_processing_ms: 0.13079590742619226\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.940697071579962e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3259623348712921\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002341450657695532\n",
      "          model: {}\n",
      "          policy_loss: -0.0005966773605905473\n",
      "          total_loss: 266.6317138671875\n",
      "          vf_explained_var: 0.2870287597179413\n",
      "          vf_loss: 266.6322937011719\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.313888888888883\n",
      "    ram_util_percent: 16.5\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0873466946784431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09078742129548147\n",
      "    mean_inference_ms: 1.2350146993750184\n",
      "    mean_raw_obs_processing_ms: 0.12172125902783083\n",
      "  time_since_restore: 678.7247068881989\n",
      "  time_this_iter_s: 25.37529683113098\n",
      "  time_total_s: 678.7247068881989\n",
      "  timers:\n",
      "    learn_throughput: 1106.7\n",
      "    learn_time_ms: 3614.349\n",
      "    load_throughput: 8468636.616\n",
      "    load_time_ms: 0.472\n",
      "    sample_throughput: 244.105\n",
      "    sample_time_ms: 16386.359\n",
      "    update_time_ms: 2.249\n",
      "  timestamp: 1641219253\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:14:16 (running for 00:11:29.72)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         678.725</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-14-20\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 655\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.470348535789981e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3050708472728729\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0025695841759443283\n",
      "          model: {}\n",
      "          policy_loss: 0.0008815221954137087\n",
      "          total_loss: 427.5550537109375\n",
      "          vf_explained_var: 0.07741489261388779\n",
      "          vf_loss: 427.55419921875\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.110000000000003\n",
      "    ram_util_percent: 16.5\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08743022024017458\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09087657899591531\n",
      "    mean_inference_ms: 1.235711881601713\n",
      "    mean_raw_obs_processing_ms: 0.12177177916045594\n",
      "  time_since_restore: 685.6850938796997\n",
      "  time_this_iter_s: 6.9603869915008545\n",
      "  time_total_s: 685.6850938796997\n",
      "  timers:\n",
      "    learn_throughput: 1106.66\n",
      "    learn_time_ms: 3614.479\n",
      "    load_throughput: 8561988.262\n",
      "    load_time_ms: 0.467\n",
      "    sample_throughput: 244.67\n",
      "    sample_time_ms: 16348.527\n",
      "    update_time_ms: 2.28\n",
      "  timestamp: 1641219260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:14:22 (running for 00:11:35.72)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         685.685</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:14:27 (running for 00:11:40.72)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         685.685</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:14:32 (running for 00:11:45.73)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         685.685</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:14:37 (running for 00:11:50.74)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         685.685</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:14:42 (running for 00:11:55.75)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         685.685</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-14-46\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 663\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10623819924716056\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11101581328886365\n",
      "      mean_inference_ms: 1.4861504213387733\n",
      "      mean_raw_obs_processing_ms: 0.13095910336162567\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2351742678949904e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.29071423411369324\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002136287745088339\n",
      "          model: {}\n",
      "          policy_loss: -0.0022150888107717037\n",
      "          total_loss: 388.80462646484375\n",
      "          vf_explained_var: -0.16201525926589966\n",
      "          vf_loss: 388.8069152832031\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.561111111111114\n",
      "    ram_util_percent: 16.5\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08748880726457355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09093977844397941\n",
      "    mean_inference_ms: 1.2361284469461626\n",
      "    mean_raw_obs_processing_ms: 0.12179369453564971\n",
      "  time_since_restore: 711.1449530124664\n",
      "  time_this_iter_s: 25.459859132766724\n",
      "  time_total_s: 711.1449530124664\n",
      "  timers:\n",
      "    learn_throughput: 1107.803\n",
      "    learn_time_ms: 3610.752\n",
      "    load_throughput: 8674879.007\n",
      "    load_time_ms: 0.461\n",
      "    sample_throughput: 245.145\n",
      "    sample_time_ms: 16316.887\n",
      "    update_time_ms: 2.364\n",
      "  timestamp: 1641219286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:14:48 (running for 00:12:01.23)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         711.145</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:14:53 (running for 00:12:06.24)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         711.145</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-14-53\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 671\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1175871339474952e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2858695089817047\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0018558389274403453\n",
      "          model: {}\n",
      "          policy_loss: -0.0009869373170658946\n",
      "          total_loss: 502.1064453125\n",
      "          vf_explained_var: -0.3364027738571167\n",
      "          vf_loss: 502.107421875\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.863636363636363\n",
      "    ram_util_percent: 16.5\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08754044496570952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.090996093007189\n",
      "    mean_inference_ms: 1.2364871926767955\n",
      "    mean_raw_obs_processing_ms: 0.12181264608147106\n",
      "  time_since_restore: 718.1796553134918\n",
      "  time_this_iter_s: 7.034702301025391\n",
      "  time_total_s: 718.1796553134918\n",
      "  timers:\n",
      "    learn_throughput: 1107.588\n",
      "    learn_time_ms: 3611.45\n",
      "    load_throughput: 8253660.648\n",
      "    load_time_ms: 0.485\n",
      "    sample_throughput: 244.905\n",
      "    sample_time_ms: 16332.846\n",
      "    update_time_ms: 2.384\n",
      "  timestamp: 1641219293\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:14:58 (running for 00:12:11.30)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">          718.18</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:15:03 (running for 00:12:16.31)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">          718.18</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:15:08 (running for 00:12:21.32)<br>Memory usage on this node: 10.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">          718.18</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:15:13 (running for 00:12:26.33)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">          718.18</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:15:18 (running for 00:12:31.34)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">          718.18</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-15-18\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 679\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10624159934256153\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11107029376192853\n",
      "      mean_inference_ms: 1.4859826599174526\n",
      "      mean_raw_obs_processing_ms: 0.1309432397015963\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.587935669737476e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.30724063515663147\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0058899810537695885\n",
      "          model: {}\n",
      "          policy_loss: -0.002079958561807871\n",
      "          total_loss: 498.74627685546875\n",
      "          vf_explained_var: -0.17767241597175598\n",
      "          vf_loss: 498.7483825683594\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.741666666666664\n",
      "    ram_util_percent: 16.455555555555552\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08759044571357089\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09105246343133584\n",
      "    mean_inference_ms: 1.2368774822780066\n",
      "    mean_raw_obs_processing_ms: 0.12183293827404303\n",
      "  time_since_restore: 743.6088602542877\n",
      "  time_this_iter_s: 25.4292049407959\n",
      "  time_total_s: 743.6088602542877\n",
      "  timers:\n",
      "    learn_throughput: 1108.736\n",
      "    learn_time_ms: 3607.713\n",
      "    load_throughput: 8069460.824\n",
      "    load_time_ms: 0.496\n",
      "    sample_throughput: 244.945\n",
      "    sample_time_ms: 16330.176\n",
      "    update_time_ms: 2.333\n",
      "  timestamp: 1641219318\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:15:23 (running for 00:12:36.79)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         743.609</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-15-25\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 687\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.587935669737476e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.30698439478874207\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0043533253483474255\n",
      "          model: {}\n",
      "          policy_loss: -0.0005037983064539731\n",
      "          total_loss: 526.5659790039062\n",
      "          vf_explained_var: -0.21300554275512695\n",
      "          vf_loss: 526.5665283203125\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.65\n",
      "    ram_util_percent: 16.300000000000004\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08764353568927621\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0911130577479566\n",
      "    mean_inference_ms: 1.2373334449624525\n",
      "    mean_raw_obs_processing_ms: 0.1218605815414816\n",
      "  time_since_restore: 750.633403301239\n",
      "  time_this_iter_s: 7.024543046951294\n",
      "  time_total_s: 750.633403301239\n",
      "  timers:\n",
      "    learn_throughput: 1109.173\n",
      "    learn_time_ms: 3606.289\n",
      "    load_throughput: 8818510.381\n",
      "    load_time_ms: 0.454\n",
      "    sample_throughput: 245.204\n",
      "    sample_time_ms: 16312.964\n",
      "    update_time_ms: 2.35\n",
      "  timestamp: 1641219325\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:15:28 (running for 00:12:41.85)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         750.633</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:15:33 (running for 00:12:46.86)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         750.633</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:15:38 (running for 00:12:51.87)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         750.633</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:15:43 (running for 00:12:56.88)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         750.633</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:15:48 (running for 00:13:01.88)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         750.633</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-15-51\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 695\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10637323248784014\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11122005036582339\n",
      "      mean_inference_ms: 1.4874508070335621\n",
      "      mean_raw_obs_processing_ms: 0.13110674275723033\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.793967834868738e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3103032410144806\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002802379196509719\n",
      "          model: {}\n",
      "          policy_loss: -0.0006990529946051538\n",
      "          total_loss: 473.7447509765625\n",
      "          vf_explained_var: -0.16833209991455078\n",
      "          vf_loss: 473.74542236328125\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.31891891891892\n",
      "    ram_util_percent: 16.299999999999994\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08768123266795595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09115753403921403\n",
      "    mean_inference_ms: 1.2376105858534188\n",
      "    mean_raw_obs_processing_ms: 0.12187466223117227\n",
      "  time_since_restore: 776.3574464321136\n",
      "  time_this_iter_s: 25.724043130874634\n",
      "  time_total_s: 776.3574464321136\n",
      "  timers:\n",
      "    learn_throughput: 1110.358\n",
      "    learn_time_ms: 3602.443\n",
      "    load_throughput: 8563736.41\n",
      "    load_time_ms: 0.467\n",
      "    sample_throughput: 245.118\n",
      "    sample_time_ms: 16318.672\n",
      "    update_time_ms: 2.301\n",
      "  timestamp: 1641219351\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:15:54 (running for 00:13:07.63)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         776.357</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-15-58\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 703\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.396983917434369e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3085900545120239\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003068174235522747\n",
      "          model: {}\n",
      "          policy_loss: -0.0007702215807512403\n",
      "          total_loss: 418.3497009277344\n",
      "          vf_explained_var: 0.2022346407175064\n",
      "          vf_loss: 418.3504638671875\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.155555555555555\n",
      "    ram_util_percent: 16.3\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08771423251806416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09119729826421817\n",
      "    mean_inference_ms: 1.2378396341172546\n",
      "    mean_raw_obs_processing_ms: 0.1218827008666099\n",
      "  time_since_restore: 783.1664071083069\n",
      "  time_this_iter_s: 6.808960676193237\n",
      "  time_total_s: 783.1664071083069\n",
      "  timers:\n",
      "    learn_throughput: 1109.451\n",
      "    learn_time_ms: 3605.387\n",
      "    load_throughput: 8052033.02\n",
      "    load_time_ms: 0.497\n",
      "    sample_throughput: 245.561\n",
      "    sample_time_ms: 16289.215\n",
      "    update_time_ms: 2.273\n",
      "  timestamp: 1641219358\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:16:00 (running for 00:13:13.47)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         783.166</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:16:05 (running for 00:13:18.48)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         783.166</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:16:10 (running for 00:13:23.49)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         783.166</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:16:15 (running for 00:13:28.50)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         783.166</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:16:20 (running for 00:13:33.51)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         783.166</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-16-24\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 711\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10646272409028532\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11130667185261442\n",
      "      mean_inference_ms: 1.4886784945351337\n",
      "      mean_raw_obs_processing_ms: 0.13121245245244864\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.984919587171845e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2988628149032593\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0038751191459596157\n",
      "          model: {}\n",
      "          policy_loss: -0.0044125947169959545\n",
      "          total_loss: 262.65570068359375\n",
      "          vf_explained_var: 0.3586840331554413\n",
      "          vf_loss: 262.6601257324219\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.89459459459459\n",
      "    ram_util_percent: 16.299999999999994\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0877523538000445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09124196859595692\n",
      "    mean_inference_ms: 1.2381390074052883\n",
      "    mean_raw_obs_processing_ms: 0.12189728207866704\n",
      "  time_since_restore: 808.9431092739105\n",
      "  time_this_iter_s: 25.776702165603638\n",
      "  time_total_s: 808.9431092739105\n",
      "  timers:\n",
      "    learn_throughput: 1109.74\n",
      "    learn_time_ms: 3604.448\n",
      "    load_throughput: 7601475.239\n",
      "    load_time_ms: 0.526\n",
      "    sample_throughput: 245.498\n",
      "    sample_time_ms: 16293.442\n",
      "    update_time_ms: 2.293\n",
      "  timestamp: 1641219384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:16:26 (running for 00:13:39.30)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         808.943</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-16-31\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 719\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4924597935859225e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3045998215675354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002932280767709017\n",
      "          model: {}\n",
      "          policy_loss: -0.0009422831353731453\n",
      "          total_loss: 236.30889892578125\n",
      "          vf_explained_var: 0.43865102529525757\n",
      "          vf_loss: 236.30984497070312\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.92\n",
      "    ram_util_percent: 16.300000000000004\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0877852690379942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09128097757282162\n",
      "    mean_inference_ms: 1.2383865060618562\n",
      "    mean_raw_obs_processing_ms: 0.1219096848315607\n",
      "  time_since_restore: 815.7978346347809\n",
      "  time_this_iter_s: 6.854725360870361\n",
      "  time_total_s: 815.7978346347809\n",
      "  timers:\n",
      "    learn_throughput: 1108.655\n",
      "    learn_time_ms: 3607.976\n",
      "    load_throughput: 7590813.501\n",
      "    load_time_ms: 0.527\n",
      "    sample_throughput: 245.589\n",
      "    sample_time_ms: 16287.364\n",
      "    update_time_ms: 2.339\n",
      "  timestamp: 1641219391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:16:32 (running for 00:13:45.21)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         815.798</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:16:37 (running for 00:13:50.21)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         815.798</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:16:42 (running for 00:13:55.22)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         815.798</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:16:47 (running for 00:14:00.23)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         815.798</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:16:52 (running for 00:14:05.24)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         815.798</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-16-56\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 727\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10646790189452764\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11130670555458581\n",
      "      mean_inference_ms: 1.4885520958398548\n",
      "      mean_raw_obs_processing_ms: 0.13119117625627874\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7462298967929613e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2929556369781494\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0036417797673493624\n",
      "          model: {}\n",
      "          policy_loss: 0.0006403282168321311\n",
      "          total_loss: 364.22332763671875\n",
      "          vf_explained_var: 0.1612871140241623\n",
      "          vf_loss: 364.2226867675781\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.497222222222224\n",
      "    ram_util_percent: 16.299999999999997\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08781462185584622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09131641892989631\n",
      "    mean_inference_ms: 1.238599249914936\n",
      "    mean_raw_obs_processing_ms: 0.12191928724323092\n",
      "  time_since_restore: 841.1693041324615\n",
      "  time_this_iter_s: 25.371469497680664\n",
      "  time_total_s: 841.1693041324615\n",
      "  timers:\n",
      "    learn_throughput: 1108.234\n",
      "    learn_time_ms: 3609.345\n",
      "    load_throughput: 7359719.249\n",
      "    load_time_ms: 0.543\n",
      "    sample_throughput: 245.602\n",
      "    sample_time_ms: 16286.527\n",
      "    update_time_ms: 2.356\n",
      "  timestamp: 1641219416\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:16:57 (running for 00:14:10.62)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         841.169</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:17:02 (running for 00:14:15.63)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         841.169</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-17-03\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 735\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.731149483964806e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2944302558898926\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0026655904948711395\n",
      "          model: {}\n",
      "          policy_loss: 0.0034651835449039936\n",
      "          total_loss: 405.4422912597656\n",
      "          vf_explained_var: 0.045616716146469116\n",
      "          vf_loss: 405.4388427734375\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.56\n",
      "    ram_util_percent: 16.300000000000004\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08784606963343493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09135371013717535\n",
      "    mean_inference_ms: 1.2388481268411822\n",
      "    mean_raw_obs_processing_ms: 0.12193107237958521\n",
      "  time_since_restore: 848.1801111698151\n",
      "  time_this_iter_s: 7.010807037353516\n",
      "  time_total_s: 848.1801111698151\n",
      "  timers:\n",
      "    learn_throughput: 1107.841\n",
      "    learn_time_ms: 3610.626\n",
      "    load_throughput: 6761462.137\n",
      "    load_time_ms: 0.592\n",
      "    sample_throughput: 245.489\n",
      "    sample_time_ms: 16294.033\n",
      "    update_time_ms: 2.358\n",
      "  timestamp: 1641219423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:17:07 (running for 00:14:20.67)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">          848.18</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:17:12 (running for 00:14:25.69)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">          848.18</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:17:17 (running for 00:14:30.69)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">          848.18</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:17:22 (running for 00:14:35.70)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">          848.18</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:17:27 (running for 00:14:40.71)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">          848.18</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-17-29\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 743\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10652521435192346\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11140433754005268\n",
      "      mean_inference_ms: 1.4893827794244467\n",
      "      mean_raw_obs_processing_ms: 0.1312632797814923\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.365574741982403e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2822948694229126\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0034656962379813194\n",
      "          model: {}\n",
      "          policy_loss: -0.0015561378095299006\n",
      "          total_loss: 262.8936462402344\n",
      "          vf_explained_var: 0.34416207671165466\n",
      "          vf_loss: 262.8951721191406\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.336111111111112\n",
      "    ram_util_percent: 16.299999999999997\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08786443681804698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0913763975099147\n",
      "    mean_inference_ms: 1.2389274909212076\n",
      "    mean_raw_obs_processing_ms: 0.12192724134457172\n",
      "  time_since_restore: 873.5743823051453\n",
      "  time_this_iter_s: 25.3942711353302\n",
      "  time_total_s: 873.5743823051453\n",
      "  timers:\n",
      "    learn_throughput: 1106.931\n",
      "    learn_time_ms: 3613.594\n",
      "    load_throughput: 6724874.138\n",
      "    load_time_ms: 0.595\n",
      "    sample_throughput: 245.52\n",
      "    sample_time_ms: 16291.952\n",
      "    update_time_ms: 2.283\n",
      "  timestamp: 1641219449\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:17:33 (running for 00:14:46.12)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         873.574</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-17-35\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 751\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1827873709912016e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2643151879310608\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.001963440328836441\n",
      "          model: {}\n",
      "          policy_loss: 0.0002141735894838348\n",
      "          total_loss: 255.84165954589844\n",
      "          vf_explained_var: 0.2884228527545929\n",
      "          vf_loss: 255.84146118164062\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.12\n",
      "    ram_util_percent: 16.300000000000004\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08787638376013593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09139191779728685\n",
      "    mean_inference_ms: 1.2389283891564324\n",
      "    mean_raw_obs_processing_ms: 0.12191876114465025\n",
      "  time_since_restore: 880.3148188591003\n",
      "  time_this_iter_s: 6.740436553955078\n",
      "  time_total_s: 880.3148188591003\n",
      "  timers:\n",
      "    learn_throughput: 1107.926\n",
      "    learn_time_ms: 3610.35\n",
      "    load_throughput: 6879855.655\n",
      "    load_time_ms: 0.581\n",
      "    sample_throughput: 245.966\n",
      "    sample_time_ms: 16262.425\n",
      "    update_time_ms: 2.351\n",
      "  timestamp: 1641219455\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:17:38 (running for 00:14:51.89)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         880.315</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:17:44 (running for 00:14:56.90)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         880.315</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:17:49 (running for 00:15:01.91)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         880.315</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:17:54 (running for 00:15:06.92)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         880.315</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:17:59 (running for 00:15:11.93)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         880.315</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-18-01\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 759\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10653715623565434\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1114145904944183\n",
      "      mean_inference_ms: 1.4893152954058242\n",
      "      mean_raw_obs_processing_ms: 0.13129538521323533\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0913936854956008e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2734183073043823\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002007606904953718\n",
      "          model: {}\n",
      "          policy_loss: 0.0005391266895458102\n",
      "          total_loss: 348.59906005859375\n",
      "          vf_explained_var: 0.11574853956699371\n",
      "          vf_loss: 348.5985107421875\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.183333333333337\n",
      "    ram_util_percent: 16.302777777777774\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08788973105034024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09140806176695056\n",
      "    mean_inference_ms: 1.2389553220575666\n",
      "    mean_raw_obs_processing_ms: 0.12191301854790243\n",
      "  time_since_restore: 905.7756633758545\n",
      "  time_this_iter_s: 25.46084451675415\n",
      "  time_total_s: 905.7756633758545\n",
      "  timers:\n",
      "    learn_throughput: 1107.09\n",
      "    learn_time_ms: 3613.076\n",
      "    load_throughput: 7092760.632\n",
      "    load_time_ms: 0.564\n",
      "    sample_throughput: 246.105\n",
      "    sample_time_ms: 16253.238\n",
      "    update_time_ms: 2.337\n",
      "  timestamp: 1641219481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:18:04 (running for 00:15:17.41)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         905.776</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-18-08\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 767\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.456968427478004e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.28623276948928833\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0011566926259547472\n",
      "          model: {}\n",
      "          policy_loss: 0.0007847628439776599\n",
      "          total_loss: 399.17938232421875\n",
      "          vf_explained_var: 0.06788268685340881\n",
      "          vf_loss: 399.1785888671875\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.72727272727273\n",
      "    ram_util_percent: 16.300000000000004\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08790903120200028\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09143008739089872\n",
      "    mean_inference_ms: 1.2390522618260897\n",
      "    mean_raw_obs_processing_ms: 0.12191564513227834\n",
      "  time_since_restore: 912.7664420604706\n",
      "  time_this_iter_s: 6.990778684616089\n",
      "  time_total_s: 912.7664420604706\n",
      "  timers:\n",
      "    learn_throughput: 1107.66\n",
      "    learn_time_ms: 3611.216\n",
      "    load_throughput: 7019168.27\n",
      "    load_time_ms: 0.57\n",
      "    sample_throughput: 245.998\n",
      "    sample_time_ms: 16260.301\n",
      "    update_time_ms: 2.378\n",
      "  timestamp: 1641219488\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:18:09 (running for 00:15:22.43)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         912.766</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:18:14 (running for 00:15:27.44)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         912.766</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:18:19 (running for 00:15:32.45)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         912.766</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:18:24 (running for 00:15:37.46)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         912.766</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:18:29 (running for 00:15:42.47)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         912.766</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-18-33\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 775\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10649691488614814\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11137389439301512\n",
      "      mean_inference_ms: 1.4886551977248612\n",
      "      mean_raw_obs_processing_ms: 0.13124891575584754\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.728484213739002e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.304677814245224\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002418044488877058\n",
      "          model: {}\n",
      "          policy_loss: -0.0022848446387797594\n",
      "          total_loss: 326.0654296875\n",
      "          vf_explained_var: 0.26932594180107117\n",
      "          vf_loss: 326.0677185058594\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.25714285714286\n",
      "    ram_util_percent: 16.299999999999997\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08792043219783086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09144281497832062\n",
      "    mean_inference_ms: 1.2390472864034312\n",
      "    mean_raw_obs_processing_ms: 0.12191001100760382\n",
      "  time_since_restore: 937.9640934467316\n",
      "  time_this_iter_s: 25.197651386260986\n",
      "  time_total_s: 937.9640934467316\n",
      "  timers:\n",
      "    learn_throughput: 1106.845\n",
      "    learn_time_ms: 3613.876\n",
      "    load_throughput: 7271047.933\n",
      "    load_time_ms: 0.55\n",
      "    sample_throughput: 245.913\n",
      "    sample_time_ms: 16265.916\n",
      "    update_time_ms: 2.39\n",
      "  timestamp: 1641219513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:18:34 (running for 00:15:47.68)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         937.964</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:18:39 (running for 00:15:52.69)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         937.964</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-18-40\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 783\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.364242106869501e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2931421399116516\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00583111634477973\n",
      "          model: {}\n",
      "          policy_loss: -0.003428358118981123\n",
      "          total_loss: 415.772705078125\n",
      "          vf_explained_var: 0.34304162859916687\n",
      "          vf_loss: 415.7761535644531\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.19090909090909\n",
      "    ram_util_percent: 16.300000000000004\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08793230648332177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09145569515566099\n",
      "    mean_inference_ms: 1.239037537237342\n",
      "    mean_raw_obs_processing_ms: 0.12190523299745058\n",
      "  time_since_restore: 945.0359477996826\n",
      "  time_this_iter_s: 7.07185435295105\n",
      "  time_total_s: 945.0359477996826\n",
      "  timers:\n",
      "    learn_throughput: 1107.319\n",
      "    learn_time_ms: 3612.327\n",
      "    load_throughput: 7351978.966\n",
      "    load_time_ms: 0.544\n",
      "    sample_throughput: 246.408\n",
      "    sample_time_ms: 16233.26\n",
      "    update_time_ms: 2.359\n",
      "  timestamp: 1641219520\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:18:44 (running for 00:15:57.80)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         945.036</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:18:49 (running for 00:16:02.80)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         945.036</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:18:54 (running for 00:16:07.81)<br>Memory usage on this node: 10.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         945.036</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:18:59 (running for 00:16:12.82)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         945.036</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:19:04 (running for 00:16:17.83)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         945.036</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-19-06\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 791\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10658361506572718\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11147632343270271\n",
      "      mean_inference_ms: 1.4896347435986357\n",
      "      mean_raw_obs_processing_ms: 0.13132399776111112\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.364242106869501e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2999780774116516\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0012296244967728853\n",
      "          model: {}\n",
      "          policy_loss: 0.0002112656511599198\n",
      "          total_loss: 320.9894104003906\n",
      "          vf_explained_var: 0.04409024864435196\n",
      "          vf_loss: 320.9892272949219\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.16111111111111\n",
      "    ram_util_percent: 16.216666666666672\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08794108040315343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09146527457198815\n",
      "    mean_inference_ms: 1.2390012344560608\n",
      "    mean_raw_obs_processing_ms: 0.12189750287655986\n",
      "  time_since_restore: 970.734194278717\n",
      "  time_this_iter_s: 25.698246479034424\n",
      "  time_total_s: 970.734194278717\n",
      "  timers:\n",
      "    learn_throughput: 1105.215\n",
      "    learn_time_ms: 3619.206\n",
      "    load_throughput: 7321818.975\n",
      "    load_time_ms: 0.546\n",
      "    sample_throughput: 246.666\n",
      "    sample_time_ms: 16216.237\n",
      "    update_time_ms: 2.282\n",
      "  timestamp: 1641219546\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:19:10 (running for 00:16:23.55)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         970.734</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-19-13\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 799\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.821210534347505e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3071805238723755\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002126883016899228\n",
      "          model: {}\n",
      "          policy_loss: 0.0006371597992256284\n",
      "          total_loss: 458.7169494628906\n",
      "          vf_explained_var: -0.00525866262614727\n",
      "          vf_loss: 458.7162780761719\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.52\n",
      "    ram_util_percent: 16.1\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08795592854147401\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09148122185217689\n",
      "    mean_inference_ms: 1.239030915169223\n",
      "    mean_raw_obs_processing_ms: 0.12189555321141463\n",
      "  time_since_restore: 977.766544342041\n",
      "  time_this_iter_s: 7.032350063323975\n",
      "  time_total_s: 977.766544342041\n",
      "  timers:\n",
      "    learn_throughput: 1105.202\n",
      "    learn_time_ms: 3619.247\n",
      "    load_throughput: 7131957.15\n",
      "    load_time_ms: 0.561\n",
      "    sample_throughput: 246.278\n",
      "    sample_time_ms: 16241.799\n",
      "    update_time_ms: 2.306\n",
      "  timestamp: 1641219553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:19:15 (running for 00:16:28.62)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         977.767</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:19:20 (running for 00:16:33.63)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         977.767</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:19:25 (running for 00:16:38.64)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         977.767</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:19:30 (running for 00:16:43.64)<br>Memory usage on this node: 10.0/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         977.767</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:19:35 (running for 00:16:48.66)<br>Memory usage on this node: 10.1/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         977.767</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-19-39\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 807\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10661601238983469\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.11151285504739727\n",
      "      mean_inference_ms: 1.4899399493450882\n",
      "      mean_raw_obs_processing_ms: 0.1313382001689078\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4106052671737525e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3085753619670868\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003451548283919692\n",
      "          model: {}\n",
      "          policy_loss: -0.0011773123405873775\n",
      "          total_loss: 499.7523498535156\n",
      "          vf_explained_var: -0.14150802791118622\n",
      "          vf_loss: 499.7535400390625\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.71351351351351\n",
      "    ram_util_percent: 16.11081081081082\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08797070542598458\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09149662924453932\n",
      "    mean_inference_ms: 1.2390627442264377\n",
      "    mean_raw_obs_processing_ms: 0.12189603905308402\n",
      "  time_since_restore: 1003.2507879734039\n",
      "  time_this_iter_s: 25.484243631362915\n",
      "  time_total_s: 1003.2507879734039\n",
      "  timers:\n",
      "    learn_throughput: 1106.713\n",
      "    learn_time_ms: 3614.308\n",
      "    load_throughput: 7181105.166\n",
      "    load_time_ms: 0.557\n",
      "    sample_throughput: 246.293\n",
      "    sample_time_ms: 16240.802\n",
      "    update_time_ms: 2.329\n",
      "  timestamp: 1641219579\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:19:41 (running for 00:16:54.16)<br>Memory usage on this node: 10.1/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         1003.25</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_d4692_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_15-19-46\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 815\n",
      "  experiment_id: e5c7a783119c469b8af00631cc9e6e80\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7053026335868762e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2822449207305908\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0028023086488246918\n",
      "          model: {}\n",
      "          policy_loss: 0.0004042913787998259\n",
      "          total_loss: 483.69140625\n",
      "          vf_explained_var: -0.19469986855983734\n",
      "          vf_loss: 483.6910095214844\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.830000000000002\n",
      "    ram_util_percent: 16.189999999999998\n",
      "  pid: 18803\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08798311748106695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09151075460924804\n",
      "    mean_inference_ms: 1.2390699402063843\n",
      "    mean_raw_obs_processing_ms: 0.12189232770578291\n",
      "  time_since_restore: 1010.2484793663025\n",
      "  time_this_iter_s: 6.99769139289856\n",
      "  time_total_s: 1010.2484793663025\n",
      "  timers:\n",
      "    learn_throughput: 1104.22\n",
      "    learn_time_ms: 3622.465\n",
      "    load_throughput: 7413378.11\n",
      "    load_time_ms: 0.54\n",
      "    sample_throughput: 246.245\n",
      "    sample_time_ms: 16244.004\n",
      "    update_time_ms: 2.35\n",
      "  timestamp: 1641219586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: d4692_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:19:46 (running for 00:16:59.19)<br>Memory usage on this node: 10.1/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1010.25</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:19:51 (running for 00:17:04.20)<br>Memory usage on this node: 10.1/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1010.25</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 15:19:55,310\tWARNING tune.py:582 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 15:19:55 (running for 00:17:08.21)<br>Memory usage on this node: 10.1/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/33.63 GiB heap, 0.0/16.81 GiB objects<br>Result logdir: /home/dibya/Dropbox/rl_course/deep_rl_get_started_fast/17_tensorboard_visualization/cartpole_v1/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d4692_00000</td><td>RUNNING </td><td>192.168.0.90:18803</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1010.25</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m 2022-01-03 15:19:55,331\tERROR worker.py:431 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"python/ray/_raylet.pyx\", line 759, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"python/ray/_raylet.pyx\", line 580, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"python/ray/_raylet.pyx\", line 618, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"python/ray/_raylet.pyx\", line 625, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"python/ray/_raylet.pyx\", line 629, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"python/ray/_raylet.pyx\", line 578, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 609, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/tune/trainable.py\", line 255, in train_buffered\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/tune/trainable.py\", line 314, in train\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 867, in step\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     result = self.step_attempt()\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 953, in step_attempt\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     evaluation_metrics = self.evaluate()\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 1060, in evaluate\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     self.evaluation_workers.local_worker().sample()\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 757, in sample\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 103, in next\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 265, in get_data\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 656, in _env_runner\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     eval_results = _do_policy_eval(\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 1070, in _do_policy_eval\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     policy.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/policy/tf_policy.py\", line 297, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     fetched = builder.get(to_fetch)\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/utils/tf_run_builder.py\", line 42, in get\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     self._executed = run_timeline(\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/utils/tf_run_builder.py\", line 92, in run_timeline\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 970, in run\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1166, in _run\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/framework/tensor_shape.py\", line 1149, in is_compatible_with\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     other = as_shape(other)\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/framework/tensor_shape.py\", line 1312, in as_shape\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     if isinstance(shape, TensorShape):\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/worker.py\", line 428, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(PPO pid=18803)\u001b[0m SystemExit: 1\n",
      "2022-01-03 15:19:55,527\tERROR tune.py:622 -- Trials did not complete: [PPO_CartPole-v1_d4692_00000]\n",
      "2022-01-03 15:19:55,529\tINFO tune.py:626 -- Total run time: 1028.43 seconds (1028.21 seconds for the tuning loop).\n",
      "2022-01-03 15:19:55,529\tWARNING tune.py:630 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f4ce4a35e80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"CartPole-v1\",\n",
    "                 \"evaluation_interval\": 2, \n",
    "                 \"evaluation_num_episodes\": 20,\n",
    "                 },\n",
    "         local_dir=\"cartpole_v1\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b92a33-e459-4321-93e2-402794f364dc",
   "metadata": {},
   "source": [
    "### An experiment creates the following files and folders in the results directory\n",
    "\n",
    "```\n",
    "<results dir>\n",
    "└──PPO    # algorithm name\n",
    "    ├── basic-variant-state-2021-06-11_16-01-54.json\n",
    "    ├── experiment_state-2021-06-11_16-01-54.json\n",
    "    └── PPO_CartPole-v1_9424e_00000_0_2021-06-11_16-01-54    # training and evaluation data\n",
    "        ├── events.out.tfevents.1623420114.devbox-x299\n",
    "        ├── params.json\n",
    "        ├── params.pkl\n",
    "        ├── progress.csv\n",
    "        └── result.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1afbd9d-b321-4556-ac9d-3ae509a3e9ba",
   "metadata": {},
   "source": [
    "## `tensorboard` can visualize the training and evaluation data in real time\n",
    "\n",
    "<img src=\"images/graph.png\" width=\"400\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
