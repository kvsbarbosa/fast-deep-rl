{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7804cb7-8f9a-455e-a513-74f0043672a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Solving RL problems with `ray[rllib]`\n",
    "\n",
    "<img src=\"images/cartpole.jpg\" width=\"500\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210f6c2-4645-4ab7-a45f-f8f6a667e667",
   "metadata": {},
   "source": [
    "## Step 1: Initialize `ray`\n",
    "\n",
    "- `ray` is a package providing distributed computing primitives. `rllib` is built on `ray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a8302b-1ae1-42c0-9852-ece04bc5a6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.90',\n",
       " 'raylet_ip_address': '192.168.0.90',\n",
       " 'redis_address': '192.168.0.90:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2022-01-03_13-36-30_217833_10780/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-01-03_13-36-30_217833_10780/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-01-03_13-36-30_217833_10780',\n",
       " 'metrics_export_port': 61790,\n",
       " 'node_id': '792c2cebf718247142f97bf17da59a5a2878c4f6d0078bf9f8316b56'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb463a9-bef9-4af1-92db-976332c42797",
   "metadata": {},
   "source": [
    "## Step 2: Run an **experiment** to solve RL problems\n",
    "\n",
    "An experiment involves four things\n",
    "- A **RL environment** (e.g. `CartPole-v1`)\n",
    "- A **RL algorithm** to learn in that environment (e.g. Proximal Policy Optimization (PPO))\n",
    "- **Configuration** (algorithm config, experiment config, environment config etc.)\n",
    "- An **experiment runner** (called `tune`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a94021-5497-47fa-bbdf-116ef940b1b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m 2022-01-03 13:39:11,432\tINFO trainer.py:722 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also want to then set `eager_tracing=True` in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m 2022-01-03 13:39:11,432\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m 2022-01-03 13:39:11,432\tINFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m 2022-01-03 13:39:13,349\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m 2022-01-03 13:39:14,152\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:39:14 (running for 00:00:05.26)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m 2022-01-03 13:39:14,730\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:39:15 (running for 00:00:06.26)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m 2022-01-03 13:39:18,079\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:39:20 (running for 00:00:11.27)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-39-21\n",
      "  done: false\n",
      "  episode_len_mean: 21.64673913043478\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 68.0\n",
      "  episode_reward_mean: 21.64673913043478\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 184\n",
      "  episodes_total: 184\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6663591265678406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02796749770641327\n",
      "          model: {}\n",
      "          policy_loss: -0.044631849974393845\n",
      "          total_loss: 192.7428741455078\n",
      "          vf_explained_var: 0.016500303521752357\n",
      "          vf_loss: 192.78192138671875\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.6\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08415044438037805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08783216149866603\n",
      "    mean_inference_ms: 1.1976684727341464\n",
      "    mean_raw_obs_processing_ms: 0.13945035242551035\n",
      "  time_since_restore: 7.004729747772217\n",
      "  time_this_iter_s: 7.004729747772217\n",
      "  time_total_s: 7.004729747772217\n",
      "  timers:\n",
      "    learn_throughput: 1096.856\n",
      "    learn_time_ms: 3646.788\n",
      "    load_throughput: 4805848.181\n",
      "    load_time_ms: 0.832\n",
      "    sample_throughput: 1194.813\n",
      "    sample_time_ms: 3347.805\n",
      "    update_time_ms: 2.581\n",
      "  timestamp: 1641213561\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:39:25 (running for 00:00:16.30)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.00473</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 21.6467</td><td style=\"text-align: right;\">                  68</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">           21.6467</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-39-28\n",
      "  done: false\n",
      "  episode_len_mean: 43.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 197.0\n",
      "  episode_reward_mean: 43.97\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 263\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6105722188949585\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017157860100269318\n",
      "          model: {}\n",
      "          policy_loss: -0.03345935046672821\n",
      "          total_loss: 474.6767272949219\n",
      "          vf_explained_var: 0.07448326796293259\n",
      "          vf_loss: 474.70501708984375\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.22222222222222\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08353832184344874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08746307010396\n",
      "    mean_inference_ms: 1.1908449920235136\n",
      "    mean_raw_obs_processing_ms: 0.13293023941489374\n",
      "  time_since_restore: 13.813632488250732\n",
      "  time_this_iter_s: 6.808902740478516\n",
      "  time_total_s: 13.813632488250732\n",
      "  timers:\n",
      "    learn_throughput: 1119.519\n",
      "    learn_time_ms: 3572.965\n",
      "    load_throughput: 5407644.158\n",
      "    load_time_ms: 0.74\n",
      "    sample_throughput: 772.793\n",
      "    sample_time_ms: 5176.029\n",
      "    update_time_ms: 2.449\n",
      "  timestamp: 1641213568\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:39:31 (running for 00:00:22.15)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         13.8136</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   43.97</td><td style=\"text-align: right;\">                 197</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">             43.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-39-35\n",
      "  done: false\n",
      "  episode_len_mean: 71.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 248.0\n",
      "  episode_reward_mean: 71.13\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 299\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5665245056152344\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010042975656688213\n",
      "          model: {}\n",
      "          policy_loss: -0.018269281834363937\n",
      "          total_loss: 652.2549438476562\n",
      "          vf_explained_var: 0.08383574336767197\n",
      "          vf_loss: 652.2701416015625\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.270000000000001\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08273471655855025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08678408880703603\n",
      "    mean_inference_ms: 1.1819810363526664\n",
      "    mean_raw_obs_processing_ms: 0.12916021271610703\n",
      "  time_since_restore: 20.33048129081726\n",
      "  time_this_iter_s: 6.516848802566528\n",
      "  time_total_s: 20.33048129081726\n",
      "  timers:\n",
      "    learn_throughput: 1131.837\n",
      "    learn_time_ms: 3534.078\n",
      "    load_throughput: 6586187.909\n",
      "    load_time_ms: 0.607\n",
      "    sample_throughput: 707.977\n",
      "    sample_time_ms: 5649.9\n",
      "    update_time_ms: 2.588\n",
      "  timestamp: 1641213575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:39:37 (running for 00:00:27.71)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         20.3305</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   71.13</td><td style=\"text-align: right;\">                 248</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             71.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-39-41\n",
      "  done: false\n",
      "  episode_len_mean: 95.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.0\n",
      "  episode_reward_mean: 95.06\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 322\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5408810973167419\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005866415333002806\n",
      "          model: {}\n",
      "          policy_loss: -0.01100907102227211\n",
      "          total_loss: 570.3348388671875\n",
      "          vf_explained_var: 0.2864651679992676\n",
      "          vf_loss: 570.3441162109375\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.0\n",
      "    ram_util_percent: 11.922222222222224\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08130777903701851\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08524807341336635\n",
      "    mean_inference_ms: 1.1640005463722982\n",
      "    mean_raw_obs_processing_ms: 0.12588914913693874\n",
      "  time_since_restore: 27.06061053276062\n",
      "  time_this_iter_s: 6.730129241943359\n",
      "  time_total_s: 27.06061053276062\n",
      "  timers:\n",
      "    learn_throughput: 1131.003\n",
      "    learn_time_ms: 3536.685\n",
      "    load_throughput: 6724335.07\n",
      "    load_time_ms: 0.595\n",
      "    sample_throughput: 677.067\n",
      "    sample_time_ms: 5907.83\n",
      "    update_time_ms: 2.757\n",
      "  timestamp: 1641213581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:39:42 (running for 00:00:33.47)<br>Memory usage on this node: 7.6/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         27.0606</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   95.06</td><td style=\"text-align: right;\">                 271</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             95.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:39:47 (running for 00:00:38.48)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         27.0606</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   95.06</td><td style=\"text-align: right;\">                 271</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             95.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-39-48\n",
      "  done: false\n",
      "  episode_len_mean: 126.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 403.0\n",
      "  episode_reward_mean: 126.15\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 343\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5414741039276123\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006212709471583366\n",
      "          model: {}\n",
      "          policy_loss: -0.012014788575470448\n",
      "          total_loss: 357.6759948730469\n",
      "          vf_explained_var: 0.5299631953239441\n",
      "          vf_loss: 357.6861572265625\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.949999999999996\n",
      "    ram_util_percent: 12.089999999999998\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08138375974683702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08537285634881106\n",
      "    mean_inference_ms: 1.1657312087018885\n",
      "    mean_raw_obs_processing_ms: 0.12412442532683636\n",
      "  time_since_restore: 33.6943895816803\n",
      "  time_this_iter_s: 6.633779048919678\n",
      "  time_total_s: 33.6943895816803\n",
      "  timers:\n",
      "    learn_throughput: 1134.39\n",
      "    learn_time_ms: 3526.123\n",
      "    load_throughput: 6705521.982\n",
      "    load_time_ms: 0.597\n",
      "    sample_throughput: 658.597\n",
      "    sample_time_ms: 6073.521\n",
      "    update_time_ms: 2.723\n",
      "  timestamp: 1641213588\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:39:53 (running for 00:00:44.16)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         33.6944</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  126.15</td><td style=\"text-align: right;\">                 403</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            126.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-39-55\n",
      "  done: false\n",
      "  episode_len_mean: 157.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 476.0\n",
      "  episode_reward_mean: 157.89\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 356\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5308369994163513\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009827823378145695\n",
      "          model: {}\n",
      "          policy_loss: -0.017479119822382927\n",
      "          total_loss: 382.46868896484375\n",
      "          vf_explained_var: 0.4310585558414459\n",
      "          vf_loss: 382.4832458496094\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.244444444444444\n",
      "    ram_util_percent: 12.1\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08144546549884352\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08546615655129046\n",
      "    mean_inference_ms: 1.1668955370900183\n",
      "    mean_raw_obs_processing_ms: 0.12291818295893474\n",
      "  time_since_restore: 40.25473761558533\n",
      "  time_this_iter_s: 6.560348033905029\n",
      "  time_total_s: 40.25473761558533\n",
      "  timers:\n",
      "    learn_throughput: 1137.972\n",
      "    learn_time_ms: 3515.026\n",
      "    load_throughput: 6972590.982\n",
      "    load_time_ms: 0.574\n",
      "    sample_throughput: 648.687\n",
      "    sample_time_ms: 6166.306\n",
      "    update_time_ms: 2.759\n",
      "  timestamp: 1641213595\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:39:59 (running for 00:00:49.76)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         40.2547</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  157.89</td><td style=\"text-align: right;\">                 476</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            157.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-40-02\n",
      "  done: false\n",
      "  episode_len_mean: 194.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 194.74\n",
      "  episode_reward_min: 20.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 366\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5359815359115601\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003213357413187623\n",
      "          model: {}\n",
      "          policy_loss: -0.00845080055296421\n",
      "          total_loss: 345.37432861328125\n",
      "          vf_explained_var: 0.36814582347869873\n",
      "          vf_loss: 345.38177490234375\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.130000000000003\n",
      "    ram_util_percent: 12.069999999999999\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08137164567653392\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08539413898313437\n",
      "    mean_inference_ms: 1.1660835648136387\n",
      "    mean_raw_obs_processing_ms: 0.12190819355411504\n",
      "  time_since_restore: 47.06625008583069\n",
      "  time_this_iter_s: 6.811512470245361\n",
      "  time_total_s: 47.06625008583069\n",
      "  timers:\n",
      "    learn_throughput: 1138.656\n",
      "    learn_time_ms: 3512.913\n",
      "    load_throughput: 6988010.948\n",
      "    load_time_ms: 0.572\n",
      "    sample_throughput: 639.111\n",
      "    sample_time_ms: 6258.689\n",
      "    update_time_ms: 2.675\n",
      "  timestamp: 1641213602\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:40:05 (running for 00:00:55.61)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         47.0663</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  194.74</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  20</td><td style=\"text-align: right;\">            194.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-40-08\n",
      "  done: false\n",
      "  episode_len_mean: 222.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 222.53\n",
      "  episode_reward_min: 20.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 374\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5514851212501526\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0034927264787256718\n",
      "          model: {}\n",
      "          policy_loss: -0.006180190946906805\n",
      "          total_loss: 327.150146484375\n",
      "          vf_explained_var: 0.15332654118537903\n",
      "          vf_loss: 327.1558532714844\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.966666666666665\n",
      "    ram_util_percent: 12.044444444444444\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08097507111468298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08494919980632142\n",
      "    mean_inference_ms: 1.1606428391880828\n",
      "    mean_raw_obs_processing_ms: 0.1209069588639111\n",
      "  time_since_restore: 53.55703377723694\n",
      "  time_this_iter_s: 6.49078369140625\n",
      "  time_total_s: 53.55703377723694\n",
      "  timers:\n",
      "    learn_throughput: 1138.616\n",
      "    learn_time_ms: 3513.036\n",
      "    load_throughput: 7404299.001\n",
      "    load_time_ms: 0.54\n",
      "    sample_throughput: 635.822\n",
      "    sample_time_ms: 6291.067\n",
      "    update_time_ms: 2.7\n",
      "  timestamp: 1641213608\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:40:10 (running for 00:01:01.13)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">          53.557</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  222.53</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  20</td><td style=\"text-align: right;\">            222.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-40-15\n",
      "  done: false\n",
      "  episode_len_mean: 252.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 252.38\n",
      "  episode_reward_min: 20.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 382\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5647332072257996\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007649588864296675\n",
      "          model: {}\n",
      "          policy_loss: -0.0074668750166893005\n",
      "          total_loss: 374.8133850097656\n",
      "          vf_explained_var: 0.173770010471344\n",
      "          vf_loss: 374.82025146484375\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.719999999999999\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08076525884682859\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0847165135918683\n",
      "    mean_inference_ms: 1.1578330524851932\n",
      "    mean_raw_obs_processing_ms: 0.12008746880722714\n",
      "  time_since_restore: 60.34099245071411\n",
      "  time_this_iter_s: 6.783958673477173\n",
      "  time_total_s: 60.34099245071411\n",
      "  timers:\n",
      "    learn_throughput: 1139.178\n",
      "    learn_time_ms: 3511.303\n",
      "    load_throughput: 7326295.197\n",
      "    load_time_ms: 0.546\n",
      "    sample_throughput: 629.775\n",
      "    sample_time_ms: 6351.471\n",
      "    update_time_ms: 2.563\n",
      "  timestamp: 1641213615\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:40:16 (running for 00:01:06.95)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">          60.341</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  252.38</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  20</td><td style=\"text-align: right;\">            252.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:40:21 (running for 00:01:11.96)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">          60.341</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  252.38</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  20</td><td style=\"text-align: right;\">            252.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-40-22\n",
      "  done: false\n",
      "  episode_len_mean: 285.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 285.96\n",
      "  episode_reward_min: 22.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 391\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5340165495872498\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0032838049810379744\n",
      "          model: {}\n",
      "          policy_loss: -0.0001432914868928492\n",
      "          total_loss: 567.168212890625\n",
      "          vf_explained_var: -0.018672805279493332\n",
      "          vf_loss: 567.1680908203125\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.05\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08108803572815126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08508239123458289\n",
      "    mean_inference_ms: 1.1624303311178328\n",
      "    mean_raw_obs_processing_ms: 0.1197775409365449\n",
      "  time_since_restore: 67.07948637008667\n",
      "  time_this_iter_s: 6.738493919372559\n",
      "  time_total_s: 67.07948637008667\n",
      "  timers:\n",
      "    learn_throughput: 1139.545\n",
      "    learn_time_ms: 3510.174\n",
      "    load_throughput: 7264436.458\n",
      "    load_time_ms: 0.551\n",
      "    sample_throughput: 625.648\n",
      "    sample_time_ms: 6393.37\n",
      "    update_time_ms: 2.605\n",
      "  timestamp: 1641213622\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:40:27 (running for 00:01:17.73)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         67.0795</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  285.96</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">            285.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-40-28\n",
      "  done: false\n",
      "  episode_len_mean: 316.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 316.4\n",
      "  episode_reward_min: 32.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 399\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.569813072681427\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0026031602174043655\n",
      "          model: {}\n",
      "          policy_loss: -0.0005075505468994379\n",
      "          total_loss: 531.6728515625\n",
      "          vf_explained_var: -0.010295074433088303\n",
      "          vf_loss: 531.6732788085938\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.155555555555557\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0813536901920087\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08538457860260977\n",
      "    mean_inference_ms: 1.1662351133657847\n",
      "    mean_raw_obs_processing_ms: 0.11944964135031279\n",
      "  time_since_restore: 73.74050617218018\n",
      "  time_this_iter_s: 6.661019802093506\n",
      "  time_total_s: 73.74050617218018\n",
      "  timers:\n",
      "    learn_throughput: 1144.53\n",
      "    learn_time_ms: 3494.884\n",
      "    load_throughput: 7611821.605\n",
      "    load_time_ms: 0.525\n",
      "    sample_throughput: 594.404\n",
      "    sample_time_ms: 6729.432\n",
      "    update_time_ms: 2.608\n",
      "  timestamp: 1641213628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:40:32 (running for 00:01:23.44)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         73.7405</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">   316.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">             316.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-40-35\n",
      "  done: false\n",
      "  episode_len_mean: 342.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 342.68\n",
      "  episode_reward_min: 39.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 407\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5800459384918213\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0032933764159679413\n",
      "          model: {}\n",
      "          policy_loss: -0.0005361199146136642\n",
      "          total_loss: 502.0644226074219\n",
      "          vf_explained_var: 0.035069212317466736\n",
      "          vf_loss: 502.06488037109375\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.540000000000001\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08103991723856917\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08503385872052974\n",
      "    mean_inference_ms: 1.1619329584807245\n",
      "    mean_raw_obs_processing_ms: 0.11860114434200343\n",
      "  time_since_restore: 80.22243785858154\n",
      "  time_this_iter_s: 6.481931686401367\n",
      "  time_total_s: 80.22243785858154\n",
      "  timers:\n",
      "    learn_throughput: 1146.298\n",
      "    learn_time_ms: 3489.495\n",
      "    load_throughput: 8187602.362\n",
      "    load_time_ms: 0.489\n",
      "    sample_throughput: 598.216\n",
      "    sample_time_ms: 6686.549\n",
      "    update_time_ms: 2.583\n",
      "  timestamp: 1641213635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:40:38 (running for 00:01:28.94)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         80.2224</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  342.68</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  39</td><td style=\"text-align: right;\">            342.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-40-42\n",
      "  done: false\n",
      "  episode_len_mean: 366.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 366.17\n",
      "  episode_reward_min: 39.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 416\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5499919056892395\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009402944706380367\n",
      "          model: {}\n",
      "          policy_loss: -0.005568421445786953\n",
      "          total_loss: 437.74261474609375\n",
      "          vf_explained_var: 0.031414251774549484\n",
      "          vf_loss: 437.7480773925781\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.677777777777777\n",
      "    ram_util_percent: 12.02222222222222\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08115485867509235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0851627133235942\n",
      "    mean_inference_ms: 1.1635646981976373\n",
      "    mean_raw_obs_processing_ms: 0.11814779143626034\n",
      "  time_since_restore: 86.84777641296387\n",
      "  time_this_iter_s: 6.625338554382324\n",
      "  time_total_s: 86.84777641296387\n",
      "  timers:\n",
      "    learn_throughput: 1145.077\n",
      "    learn_time_ms: 3493.215\n",
      "    load_throughput: 7749291.455\n",
      "    load_time_ms: 0.516\n",
      "    sample_throughput: 598.178\n",
      "    sample_time_ms: 6686.974\n",
      "    update_time_ms: 2.563\n",
      "  timestamp: 1641213642\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:40:44 (running for 00:01:34.61)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         86.8478</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  366.17</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  39</td><td style=\"text-align: right;\">            366.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-40-48\n",
      "  done: false\n",
      "  episode_len_mean: 394.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 394.07\n",
      "  episode_reward_min: 39.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 425\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5567706823348999\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008467133156955242\n",
      "          model: {}\n",
      "          policy_loss: -0.004910553339868784\n",
      "          total_loss: 415.7817077636719\n",
      "          vf_explained_var: 0.004876916296780109\n",
      "          vf_loss: 415.7865905761719\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.45\n",
      "    ram_util_percent: 12.03\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08134322833467493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08537547549982394\n",
      "    mean_inference_ms: 1.1662704658526954\n",
      "    mean_raw_obs_processing_ms: 0.1178150918225723\n",
      "  time_since_restore: 93.66650223731995\n",
      "  time_this_iter_s: 6.818725824356079\n",
      "  time_total_s: 93.66650223731995\n",
      "  timers:\n",
      "    learn_throughput: 1145.44\n",
      "    learn_time_ms: 3492.108\n",
      "    load_throughput: 7766510.508\n",
      "    load_time_ms: 0.515\n",
      "    sample_throughput: 596.936\n",
      "    sample_time_ms: 6700.886\n",
      "    update_time_ms: 2.507\n",
      "  timestamp: 1641213648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:40:49 (running for 00:01:40.47)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         93.6665</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  394.07</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  39</td><td style=\"text-align: right;\">            394.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:40:54 (running for 00:01:45.48)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         93.6665</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  394.07</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  39</td><td style=\"text-align: right;\">            394.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-40-55\n",
      "  done: false\n",
      "  episode_len_mean: 420.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 420.11\n",
      "  episode_reward_min: 39.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 433\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5898487567901611\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0032153381034731865\n",
      "          model: {}\n",
      "          policy_loss: -0.001305061625316739\n",
      "          total_loss: 503.4804382324219\n",
      "          vf_explained_var: 0.03905607759952545\n",
      "          vf_loss: 503.4817199707031\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.066666666666666\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08107613544672577\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08508685295280038\n",
      "    mean_inference_ms: 1.1628688046254054\n",
      "    mean_raw_obs_processing_ms: 0.11710467324938208\n",
      "  time_since_restore: 100.2765998840332\n",
      "  time_this_iter_s: 6.610097646713257\n",
      "  time_total_s: 100.2765998840332\n",
      "  timers:\n",
      "    learn_throughput: 1144.826\n",
      "    learn_time_ms: 3493.98\n",
      "    load_throughput: 7816080.13\n",
      "    load_time_ms: 0.512\n",
      "    sample_throughput: 597.403\n",
      "    sample_time_ms: 6695.646\n",
      "    update_time_ms: 2.484\n",
      "  timestamp: 1641213655\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:41:00 (running for 00:01:51.13)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         100.277</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  420.11</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  39</td><td style=\"text-align: right;\">            420.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-41-02\n",
      "  done: false\n",
      "  episode_len_mean: 442.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 442.89\n",
      "  episode_reward_min: 39.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 441\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004687500186264515\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5899381041526794\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0036167073994874954\n",
      "          model: {}\n",
      "          policy_loss: 0.0004902082146145403\n",
      "          total_loss: 545.7775268554688\n",
      "          vf_explained_var: 0.08752446621656418\n",
      "          vf_loss: 545.7770385742188\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.169999999999998\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0814132090057486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08546435612314779\n",
      "    mean_inference_ms: 1.1675808925677116\n",
      "    mean_raw_obs_processing_ms: 0.11706280232615328\n",
      "  time_since_restore: 107.02618670463562\n",
      "  time_this_iter_s: 6.749586820602417\n",
      "  time_total_s: 107.02618670463562\n",
      "  timers:\n",
      "    learn_throughput: 1143.208\n",
      "    learn_time_ms: 3498.927\n",
      "    load_throughput: 7679062.614\n",
      "    load_time_ms: 0.521\n",
      "    sample_throughput: 596.017\n",
      "    sample_time_ms: 6711.22\n",
      "    update_time_ms: 2.46\n",
      "  timestamp: 1641213662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:41:06 (running for 00:01:56.92)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         107.026</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  442.89</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  39</td><td style=\"text-align: right;\">            442.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-41-08\n",
      "  done: false\n",
      "  episode_len_mean: 453.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 453.79\n",
      "  episode_reward_min: 39.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 451\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023437500931322575\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5610551238059998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00781817827373743\n",
      "          model: {}\n",
      "          policy_loss: -0.005211701150983572\n",
      "          total_loss: 421.1143798828125\n",
      "          vf_explained_var: 0.1872728019952774\n",
      "          vf_loss: 421.11956787109375\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.41111111111111\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08130910633130238\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08535710942033159\n",
      "    mean_inference_ms: 1.1664936901695035\n",
      "    mean_raw_obs_processing_ms: 0.11648177829821531\n",
      "  time_since_restore: 113.55782747268677\n",
      "  time_this_iter_s: 6.5316407680511475\n",
      "  time_total_s: 113.55782747268677\n",
      "  timers:\n",
      "    learn_throughput: 1143.896\n",
      "    learn_time_ms: 3496.822\n",
      "    load_throughput: 7696676.759\n",
      "    load_time_ms: 0.52\n",
      "    sample_throughput: 597.888\n",
      "    sample_time_ms: 6690.213\n",
      "    update_time_ms: 2.524\n",
      "  timestamp: 1641213668\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:41:11 (running for 00:02:02.49)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         113.558</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  453.79</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  39</td><td style=\"text-align: right;\">            453.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-41-15\n",
      "  done: false\n",
      "  episode_len_mean: 464.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 464.29\n",
      "  episode_reward_min: 39.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 459\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023437500931322575\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5712805986404419\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004599640611559153\n",
      "          model: {}\n",
      "          policy_loss: -0.0021797791123390198\n",
      "          total_loss: 355.96728515625\n",
      "          vf_explained_var: 0.14474022388458252\n",
      "          vf_loss: 355.96942138671875\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.66\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08140791680475995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0854727323310456\n",
      "    mean_inference_ms: 1.1681170874044884\n",
      "    mean_raw_obs_processing_ms: 0.11625969837710613\n",
      "  time_since_restore: 120.25446653366089\n",
      "  time_this_iter_s: 6.696639060974121\n",
      "  time_total_s: 120.25446653366089\n",
      "  timers:\n",
      "    learn_throughput: 1144.507\n",
      "    learn_time_ms: 3494.955\n",
      "    load_throughput: 7358428.07\n",
      "    load_time_ms: 0.544\n",
      "    sample_throughput: 596.053\n",
      "    sample_time_ms: 6710.815\n",
      "    update_time_ms: 2.459\n",
      "  timestamp: 1641213675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:41:17 (running for 00:02:08.22)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         120.254</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  464.29</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  39</td><td style=\"text-align: right;\">            464.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-41-22\n",
      "  done: false\n",
      "  episode_len_mean: 469.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 469.34\n",
      "  episode_reward_min: 39.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 467\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5803890824317932\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005978395696729422\n",
      "          model: {}\n",
      "          policy_loss: -0.001656494103372097\n",
      "          total_loss: 479.11749267578125\n",
      "          vf_explained_var: -0.03898642212152481\n",
      "          vf_loss: 479.119140625\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.8\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0815323576804837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08561270801247649\n",
      "    mean_inference_ms: 1.1700628541113784\n",
      "    mean_raw_obs_processing_ms: 0.11611601231629272\n",
      "  time_since_restore: 127.18543887138367\n",
      "  time_this_iter_s: 6.930972337722778\n",
      "  time_total_s: 127.18543887138367\n",
      "  timers:\n",
      "    learn_throughput: 1145.148\n",
      "    learn_time_ms: 3492.998\n",
      "    load_throughput: 7332699.301\n",
      "    load_time_ms: 0.546\n",
      "    sample_throughput: 594.755\n",
      "    sample_time_ms: 6725.456\n",
      "    update_time_ms: 2.535\n",
      "  timestamp: 1641213682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:41:23 (running for 00:02:14.19)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         127.185</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  469.34</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  39</td><td style=\"text-align: right;\">            469.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:41:28 (running for 00:02:19.20)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         127.185</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  469.34</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  39</td><td style=\"text-align: right;\">            469.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-41-29\n",
      "  done: false\n",
      "  episode_len_mean: 471.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 471.59\n",
      "  episode_reward_min: 39.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 475\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5981844067573547\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005319596733897924\n",
      "          model: {}\n",
      "          policy_loss: -0.0003460199513938278\n",
      "          total_loss: 539.0498046875\n",
      "          vf_explained_var: -0.055875200778245926\n",
      "          vf_loss: 539.0501098632812\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.044444444444444\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08162556508445569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08571877757927092\n",
      "    mean_inference_ms: 1.1715869326389659\n",
      "    mean_raw_obs_processing_ms: 0.115994576767953\n",
      "  time_since_restore: 133.77163791656494\n",
      "  time_this_iter_s: 6.586199045181274\n",
      "  time_total_s: 133.77163791656494\n",
      "  timers:\n",
      "    learn_throughput: 1145.585\n",
      "    learn_time_ms: 3491.665\n",
      "    load_throughput: 7562414.244\n",
      "    load_time_ms: 0.529\n",
      "    sample_throughput: 596.104\n",
      "    sample_time_ms: 6710.236\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1641213689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:41:34 (running for 00:02:24.81)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         133.772</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  471.59</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  39</td><td style=\"text-align: right;\">            471.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-41-35\n",
      "  done: false\n",
      "  episode_len_mean: 470.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 470.12\n",
      "  episode_reward_min: 103.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 485\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5744273066520691\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003393118502572179\n",
      "          model: {}\n",
      "          policy_loss: 0.0013910011621192098\n",
      "          total_loss: 588.284423828125\n",
      "          vf_explained_var: -0.04091992974281311\n",
      "          vf_loss: 588.2830200195312\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.379999999999999\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08162967468542245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08572395593074766\n",
      "    mean_inference_ms: 1.1719172825135906\n",
      "    mean_raw_obs_processing_ms: 0.11575632212846991\n",
      "  time_since_restore: 140.2826907634735\n",
      "  time_this_iter_s: 6.511052846908569\n",
      "  time_total_s: 140.2826907634735\n",
      "  timers:\n",
      "    learn_throughput: 1146.464\n",
      "    learn_time_ms: 3488.99\n",
      "    load_throughput: 8122986.346\n",
      "    load_time_ms: 0.492\n",
      "    sample_throughput: 597.438\n",
      "    sample_time_ms: 6695.251\n",
      "    update_time_ms: 2.449\n",
      "  timestamp: 1641213695\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:41:39 (running for 00:02:30.36)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         140.283</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  470.12</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 103</td><td style=\"text-align: right;\">            470.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-41-42\n",
      "  done: false\n",
      "  episode_len_mean: 467.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 467.65\n",
      "  episode_reward_min: 103.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 493\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5612112283706665\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007541084196418524\n",
      "          model: {}\n",
      "          policy_loss: -0.005620027892291546\n",
      "          total_loss: 317.69781494140625\n",
      "          vf_explained_var: 0.22571057081222534\n",
      "          vf_loss: 317.70343017578125\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.733333333333333\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08167612940660497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0857726992872092\n",
      "    mean_inference_ms: 1.172727864100544\n",
      "    mean_raw_obs_processing_ms: 0.11564990852212226\n",
      "  time_since_restore: 146.91126656532288\n",
      "  time_this_iter_s: 6.628575801849365\n",
      "  time_total_s: 146.91126656532288\n",
      "  timers:\n",
      "    learn_throughput: 1145.301\n",
      "    learn_time_ms: 3492.53\n",
      "    load_throughput: 7668182.275\n",
      "    load_time_ms: 0.522\n",
      "    sample_throughput: 596.743\n",
      "    sample_time_ms: 6703.055\n",
      "    update_time_ms: 2.5\n",
      "  timestamp: 1641213702\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:41:45 (running for 00:02:36.03)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         146.911</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  467.65</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 103</td><td style=\"text-align: right;\">            467.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-41-49\n",
      "  done: false\n",
      "  episode_len_mean: 467.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 467.65\n",
      "  episode_reward_min: 103.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 501\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5558075904846191\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0072825136594474316\n",
      "          model: {}\n",
      "          policy_loss: -0.001537785748951137\n",
      "          total_loss: 480.6811828613281\n",
      "          vf_explained_var: -0.06036938726902008\n",
      "          vf_loss: 480.6827392578125\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.66\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08172862663646809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0858283510389294\n",
      "    mean_inference_ms: 1.1736378971247852\n",
      "    mean_raw_obs_processing_ms: 0.11557085296311911\n",
      "  time_since_restore: 153.6742172241211\n",
      "  time_this_iter_s: 6.762950658798218\n",
      "  time_total_s: 153.6742172241211\n",
      "  timers:\n",
      "    learn_throughput: 1145.311\n",
      "    learn_time_ms: 3492.502\n",
      "    load_throughput: 7766870.052\n",
      "    load_time_ms: 0.515\n",
      "    sample_throughput: 595.043\n",
      "    sample_time_ms: 6722.203\n",
      "    update_time_ms: 2.497\n",
      "  timestamp: 1641213709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:41:51 (running for 00:02:41.83)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         153.674</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  467.65</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 103</td><td style=\"text-align: right;\">            467.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-41-55\n",
      "  done: false\n",
      "  episode_len_mean: 463.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 463.99\n",
      "  episode_reward_min: 103.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 511\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5813219547271729\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012521774508059025\n",
      "          model: {}\n",
      "          policy_loss: -0.00892722699791193\n",
      "          total_loss: 342.2606201171875\n",
      "          vf_explained_var: 0.2783268392086029\n",
      "          vf_loss: 342.2695617675781\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.466666666666665\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08175774820873734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0858577505068154\n",
      "    mean_inference_ms: 1.174201181698404\n",
      "    mean_raw_obs_processing_ms: 0.11547061131861916\n",
      "  time_since_restore: 160.35565400123596\n",
      "  time_this_iter_s: 6.681436777114868\n",
      "  time_total_s: 160.35565400123596\n",
      "  timers:\n",
      "    learn_throughput: 1146.929\n",
      "    learn_time_ms: 3487.576\n",
      "    load_throughput: 7754664.202\n",
      "    load_time_ms: 0.516\n",
      "    sample_throughput: 595.813\n",
      "    sample_time_ms: 6713.518\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1641213715\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:41:57 (running for 00:02:47.55)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         160.356</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">  463.99</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 103</td><td style=\"text-align: right;\">            463.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:42:02 (running for 00:02:52.56)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         160.356</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">  463.99</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 103</td><td style=\"text-align: right;\">            463.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-42-02\n",
      "  done: false\n",
      "  episode_len_mean: 471.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 471.79\n",
      "  episode_reward_min: 103.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 519\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5722456574440002\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0063661839812994\n",
      "          model: {}\n",
      "          policy_loss: -0.002771718893200159\n",
      "          total_loss: 411.03179931640625\n",
      "          vf_explained_var: 0.08734791725873947\n",
      "          vf_loss: 411.0345764160156\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.379999999999999\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0818112724504771\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08591774825793311\n",
      "    mean_inference_ms: 1.1751675778864956\n",
      "    mean_raw_obs_processing_ms: 0.11543718796466175\n",
      "  time_since_restore: 166.89983248710632\n",
      "  time_this_iter_s: 6.544178485870361\n",
      "  time_total_s: 166.89983248710632\n",
      "  timers:\n",
      "    learn_throughput: 1147.323\n",
      "    learn_time_ms: 3486.378\n",
      "    load_throughput: 8176031.189\n",
      "    load_time_ms: 0.489\n",
      "    sample_throughput: 596.766\n",
      "    sample_time_ms: 6702.793\n",
      "    update_time_ms: 2.425\n",
      "  timestamp: 1641213722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:42:07 (running for 00:02:58.14)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">           166.9</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  471.79</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 103</td><td style=\"text-align: right;\">            471.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-42-09\n",
      "  done: false\n",
      "  episode_len_mean: 471.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 471.86\n",
      "  episode_reward_min: 103.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 527\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.551196277141571\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005431749392300844\n",
      "          model: {}\n",
      "          policy_loss: -0.001215668162330985\n",
      "          total_loss: 454.0567626953125\n",
      "          vf_explained_var: 0.03504559397697449\n",
      "          vf_loss: 454.0579528808594\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.700000000000001\n",
      "    ram_util_percent: 12.033333333333333\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08181583795058332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08592427560426874\n",
      "    mean_inference_ms: 1.175459026677066\n",
      "    mean_raw_obs_processing_ms: 0.11535091490396074\n",
      "  time_since_restore: 173.6436424255371\n",
      "  time_this_iter_s: 6.743809938430786\n",
      "  time_total_s: 173.6436424255371\n",
      "  timers:\n",
      "    learn_throughput: 1147.829\n",
      "    learn_time_ms: 3484.838\n",
      "    load_throughput: 8043540.128\n",
      "    load_time_ms: 0.497\n",
      "    sample_throughput: 596.798\n",
      "    sample_time_ms: 6702.431\n",
      "    update_time_ms: 2.356\n",
      "  timestamp: 1641213729\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:42:13 (running for 00:03:03.92)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         173.644</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">  471.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 103</td><td style=\"text-align: right;\">            471.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-42-16\n",
      "  done: false\n",
      "  episode_len_mean: 471.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 471.86\n",
      "  episode_reward_min: 103.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 535\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5692477226257324\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005617249757051468\n",
      "          model: {}\n",
      "          policy_loss: -0.0013889643596485257\n",
      "          total_loss: 428.63238525390625\n",
      "          vf_explained_var: 0.10854756832122803\n",
      "          vf_loss: 428.6337585449219\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.920000000000002\n",
      "    ram_util_percent: 12.040000000000001\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08183563285950811\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08594637819978512\n",
      "    mean_inference_ms: 1.1758845370956654\n",
      "    mean_raw_obs_processing_ms: 0.11529601561232287\n",
      "  time_since_restore: 180.35238194465637\n",
      "  time_this_iter_s: 6.708739519119263\n",
      "  time_total_s: 180.35238194465637\n",
      "  timers:\n",
      "    learn_throughput: 1148.166\n",
      "    learn_time_ms: 3483.817\n",
      "    load_throughput: 8056286.194\n",
      "    load_time_ms: 0.497\n",
      "    sample_throughput: 595.302\n",
      "    sample_time_ms: 6719.278\n",
      "    update_time_ms: 2.295\n",
      "  timestamp: 1641213736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:42:19 (running for 00:03:09.66)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         180.352</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  471.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 103</td><td style=\"text-align: right;\">            471.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-42-22\n",
      "  done: false\n",
      "  episode_len_mean: 473.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 473.37\n",
      "  episode_reward_min: 103.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 543\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5218449831008911\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00573698990046978\n",
      "          model: {}\n",
      "          policy_loss: -0.0002649179077707231\n",
      "          total_loss: 369.30255126953125\n",
      "          vf_explained_var: 0.21435509622097015\n",
      "          vf_loss: 369.3028259277344\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.49\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0818597581300883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08597325228805287\n",
      "    mean_inference_ms: 1.1763351630719625\n",
      "    mean_raw_obs_processing_ms: 0.11525159452387447\n",
      "  time_since_restore: 187.0088288784027\n",
      "  time_this_iter_s: 6.656446933746338\n",
      "  time_total_s: 187.0088288784027\n",
      "  timers:\n",
      "    learn_throughput: 1149.268\n",
      "    learn_time_ms: 3480.477\n",
      "    load_throughput: 8041997.891\n",
      "    load_time_ms: 0.497\n",
      "    sample_throughput: 595.633\n",
      "    sample_time_ms: 6715.547\n",
      "    update_time_ms: 2.354\n",
      "  timestamp: 1641213742\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:42:24 (running for 00:03:15.35)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         187.009</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">  473.37</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 103</td><td style=\"text-align: right;\">            473.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-42-29\n",
      "  done: false\n",
      "  episode_len_mean: 484.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 484.09\n",
      "  episode_reward_min: 105.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 551\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5402454137802124\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007040547206997871\n",
      "          model: {}\n",
      "          policy_loss: -0.0027080182917416096\n",
      "          total_loss: 311.1156311035156\n",
      "          vf_explained_var: 0.2556372582912445\n",
      "          vf_loss: 311.1183166503906\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.922222222222224\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08192541187564359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08604841014972774\n",
      "    mean_inference_ms: 1.177389873234481\n",
      "    mean_raw_obs_processing_ms: 0.11526133893458575\n",
      "  time_since_restore: 193.66080570220947\n",
      "  time_this_iter_s: 6.651976823806763\n",
      "  time_total_s: 193.66080570220947\n",
      "  timers:\n",
      "    learn_throughput: 1148.675\n",
      "    learn_time_ms: 3482.273\n",
      "    load_throughput: 8532378.579\n",
      "    load_time_ms: 0.469\n",
      "    sample_throughput: 598.515\n",
      "    sample_time_ms: 6683.208\n",
      "    update_time_ms: 2.329\n",
      "  timestamp: 1641213749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:42:30 (running for 00:03:21.04)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         193.661</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  484.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 105</td><td style=\"text-align: right;\">            484.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:42:35 (running for 00:03:26.05)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         193.661</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  484.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 105</td><td style=\"text-align: right;\">            484.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-42-36\n",
      "  done: false\n",
      "  episode_len_mean: 486.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 486.86\n",
      "  episode_reward_min: 105.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 559\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5258514881134033\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004464397672563791\n",
      "          model: {}\n",
      "          policy_loss: 0.0003494231204967946\n",
      "          total_loss: 510.3366394042969\n",
      "          vf_explained_var: -0.06618722528219223\n",
      "          vf_loss: 510.3363037109375\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.160000000000002\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08193618053497474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08606175861022361\n",
      "    mean_inference_ms: 1.1776665307927623\n",
      "    mean_raw_obs_processing_ms: 0.11520828777886845\n",
      "  time_since_restore: 200.44229984283447\n",
      "  time_this_iter_s: 6.781494140625\n",
      "  time_total_s: 200.44229984283447\n",
      "  timers:\n",
      "    learn_throughput: 1148.406\n",
      "    learn_time_ms: 3483.089\n",
      "    load_throughput: 8301032.111\n",
      "    load_time_ms: 0.482\n",
      "    sample_throughput: 596.7\n",
      "    sample_time_ms: 6703.531\n",
      "    update_time_ms: 2.326\n",
      "  timestamp: 1641213756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:42:41 (running for 00:03:31.87)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         200.442</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">  486.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 105</td><td style=\"text-align: right;\">            486.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-42-43\n",
      "  done: false\n",
      "  episode_len_mean: 486.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 486.86\n",
      "  episode_reward_min: 105.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 567\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0002929687616415322\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5264471173286438\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003849866334348917\n",
      "          model: {}\n",
      "          policy_loss: 0.0006517216679640114\n",
      "          total_loss: 545.7387084960938\n",
      "          vf_explained_var: -0.01630960777401924\n",
      "          vf_loss: 545.738037109375\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.15\n",
      "    ram_util_percent: 12.0\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08190988201786535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0860355732100273\n",
      "    mean_inference_ms: 1.1774107566053624\n",
      "    mean_raw_obs_processing_ms: 0.11511634318298991\n",
      "  time_since_restore: 207.15555548667908\n",
      "  time_this_iter_s: 6.7132556438446045\n",
      "  time_total_s: 207.15555548667908\n",
      "  timers:\n",
      "    learn_throughput: 1147.352\n",
      "    learn_time_ms: 3486.287\n",
      "    load_throughput: 7758250.173\n",
      "    load_time_ms: 0.516\n",
      "    sample_throughput: 595.019\n",
      "    sample_time_ms: 6722.47\n",
      "    update_time_ms: 2.327\n",
      "  timestamp: 1641213763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:42:47 (running for 00:03:37.62)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         207.156</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  486.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 105</td><td style=\"text-align: right;\">            486.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-42-49\n",
      "  done: false\n",
      "  episode_len_mean: 486.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 486.86\n",
      "  episode_reward_min: 105.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 575\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001464843808207661\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5416557192802429\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0035247569903731346\n",
      "          model: {}\n",
      "          policy_loss: -0.00046945587382651865\n",
      "          total_loss: 545.5198974609375\n",
      "          vf_explained_var: -0.0627073347568512\n",
      "          vf_loss: 545.5203857421875\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.033333333333335\n",
      "    ram_util_percent: 12.033333333333333\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08189794571304995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08602534501604941\n",
      "    mean_inference_ms: 1.177355988422575\n",
      "    mean_raw_obs_processing_ms: 0.11504187193735806\n",
      "  time_since_restore: 213.94784450531006\n",
      "  time_this_iter_s: 6.7922890186309814\n",
      "  time_total_s: 213.94784450531006\n",
      "  timers:\n",
      "    learn_throughput: 1146.227\n",
      "    learn_time_ms: 3489.711\n",
      "    load_throughput: 7723250.012\n",
      "    load_time_ms: 0.518\n",
      "    sample_throughput: 593.579\n",
      "    sample_time_ms: 6738.787\n",
      "    update_time_ms: 2.356\n",
      "  timestamp: 1641213769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:42:52 (running for 00:03:43.45)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         213.948</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">  486.86</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 105</td><td style=\"text-align: right;\">            486.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-42-56\n",
      "  done: false\n",
      "  episode_len_mean: 489.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 489.33\n",
      "  episode_reward_min: 105.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 583\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.324219041038305e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.50007563829422\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005137373693287373\n",
      "          model: {}\n",
      "          policy_loss: 4.097018245374784e-05\n",
      "          total_loss: 515.6433715820312\n",
      "          vf_explained_var: -0.06309071183204651\n",
      "          vf_loss: 515.643310546875\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.25\n",
      "    ram_util_percent: 12.099999999999998\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08186825882313879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08599343970861278\n",
      "    mean_inference_ms: 1.1770133718250113\n",
      "    mean_raw_obs_processing_ms: 0.11495191181446765\n",
      "  time_since_restore: 220.65630531311035\n",
      "  time_this_iter_s: 6.708460807800293\n",
      "  time_total_s: 220.65630531311035\n",
      "  timers:\n",
      "    learn_throughput: 1145.949\n",
      "    learn_time_ms: 3490.557\n",
      "    load_throughput: 7717920.692\n",
      "    load_time_ms: 0.518\n",
      "    sample_throughput: 593.862\n",
      "    sample_time_ms: 6735.573\n",
      "    update_time_ms: 2.306\n",
      "  timestamp: 1641213776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:42:58 (running for 00:03:49.19)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         220.656</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">  489.33</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 105</td><td style=\"text-align: right;\">            489.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-43-03\n",
      "  done: false\n",
      "  episode_len_mean: 494.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 494.6\n",
      "  episode_reward_min: 290.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 591\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.324219041038305e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.49026063084602356\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0048093427903950214\n",
      "          model: {}\n",
      "          policy_loss: -0.001734600169584155\n",
      "          total_loss: 528.8831176757812\n",
      "          vf_explained_var: -0.15633000433444977\n",
      "          vf_loss: 528.8848876953125\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.344444444444443\n",
      "    ram_util_percent: 12.1\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08186133998178421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08598768994601982\n",
      "    mean_inference_ms: 1.1770252933582137\n",
      "    mean_raw_obs_processing_ms: 0.11488578062538128\n",
      "  time_since_restore: 227.12775802612305\n",
      "  time_this_iter_s: 6.471452713012695\n",
      "  time_total_s: 227.12775802612305\n",
      "  timers:\n",
      "    learn_throughput: 1145.114\n",
      "    learn_time_ms: 3493.103\n",
      "    load_throughput: 7716855.71\n",
      "    load_time_ms: 0.518\n",
      "    sample_throughput: 595.918\n",
      "    sample_time_ms: 6712.331\n",
      "    update_time_ms: 2.326\n",
      "  timestamp: 1641213783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:04 (running for 00:03:54.70)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         227.128</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">   494.6</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 290</td><td style=\"text-align: right;\">             494.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:09 (running for 00:03:59.71)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         227.128</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">   494.6</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 290</td><td style=\"text-align: right;\">             494.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-43-09\n",
      "  done: false\n",
      "  episode_len_mean: 495.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 495.75\n",
      "  episode_reward_min: 290.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 599\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.662109520519152e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.48286598920822144\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0036757809575647116\n",
      "          model: {}\n",
      "          policy_loss: 3.896733687724918e-05\n",
      "          total_loss: 520.688720703125\n",
      "          vf_explained_var: -0.17857114970684052\n",
      "          vf_loss: 520.688720703125\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.199999999999998\n",
      "    ram_util_percent: 12.099999999999998\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08185321858601266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08598176120172857\n",
      "    mean_inference_ms: 1.1770370353782822\n",
      "    mean_raw_obs_processing_ms: 0.11482170493690692\n",
      "  time_since_restore: 233.83875799179077\n",
      "  time_this_iter_s: 6.710999965667725\n",
      "  time_total_s: 233.83875799179077\n",
      "  timers:\n",
      "    learn_throughput: 1145.838\n",
      "    learn_time_ms: 3490.896\n",
      "    load_throughput: 7348436.775\n",
      "    load_time_ms: 0.544\n",
      "    sample_throughput: 594.028\n",
      "    sample_time_ms: 6733.684\n",
      "    update_time_ms: 2.388\n",
      "  timestamp: 1641213789\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:14 (running for 00:04:05.46)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         233.839</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  495.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 290</td><td style=\"text-align: right;\">            495.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-03 13:43:16,662\tWARNING tune.py:582 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_24a1e_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-43-16\n",
      "  done: false\n",
      "  episode_len_mean: 497.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.59\n",
      "  episode_reward_min: 290.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 607\n",
      "  experiment_id: a4b4f475ff524abc86d9f584e8fcba05\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.831054760259576e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5355268120765686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003950898069888353\n",
      "          model: {}\n",
      "          policy_loss: -0.0031078020110726357\n",
      "          total_loss: 454.7011413574219\n",
      "          vf_explained_var: 0.01892552711069584\n",
      "          vf_loss: 454.7042236328125\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.47777777777778\n",
      "    ram_util_percent: 12.1\n",
      "  pid: 11129\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08181696845718284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08594351067341023\n",
      "    mean_inference_ms: 1.1766330836481287\n",
      "    mean_raw_obs_processing_ms: 0.11472448071836644\n",
      "  time_since_restore: 240.56040740013123\n",
      "  time_this_iter_s: 6.721649408340454\n",
      "  time_total_s: 240.56040740013123\n",
      "  timers:\n",
      "    learn_throughput: 1145.016\n",
      "    learn_time_ms: 3493.401\n",
      "    load_throughput: 7453890.172\n",
      "    load_time_ms: 0.537\n",
      "    sample_throughput: 594.656\n",
      "    sample_time_ms: 6726.584\n",
      "    update_time_ms: 2.393\n",
      "  timestamp: 1641213796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 24a1e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:16 (running for 00:04:07.21)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_24a1e_00000</td><td>RUNNING </td><td>192.168.0.90:11129</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">          240.56</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">  497.59</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 290</td><td style=\"text-align: right;\">            497.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m 2022-01-03 13:43:16,714\tERROR worker.py:431 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"python/ray/_raylet.pyx\", line 759, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"python/ray/_raylet.pyx\", line 580, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"python/ray/_raylet.pyx\", line 618, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"python/ray/_raylet.pyx\", line 625, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"python/ray/_raylet.pyx\", line 629, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"python/ray/_raylet.pyx\", line 578, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 609, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/tune/trainable.py\", line 255, in train_buffered\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     result = self.train()\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/tune/trainable.py\", line 314, in train\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 867, in step\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     result = self.step_attempt()\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 920, in step_attempt\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     step_results = next(self.train_exec_impl)\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     return next(self.built_iterator)\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   [Previous line repeated 1 more time]\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 876, in apply_flatten\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     for item in it:\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   [Previous line repeated 1 more time]\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 471, in base_iterator\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     yield ray.get(futures, timeout=timeout)\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/worker.py\", line 1706, in get\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     values, debugger_breakpoint = worker.get_objects(\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/worker.py\", line 353, in get_objects\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     data_metadata_pairs = self.core_worker.get_objects(\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/worker.py\", line 428, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(PPO pid=11129)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m 2022-01-03 13:43:16,712\tERROR worker.py:431 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"python/ray/_raylet.pyx\", line 759, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"python/ray/_raylet.pyx\", line 580, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"python/ray/_raylet.pyx\", line 618, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"python/ray/_raylet.pyx\", line 625, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"python/ray/_raylet.pyx\", line 629, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"python/ray/_raylet.pyx\", line 578, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 609, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 381, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 757, in sample\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 103, in next\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 265, in get_data\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 656, in _env_runner\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     eval_results = _do_policy_eval(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 1070, in _do_policy_eval\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     policy.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/policy/tf_policy.py\", line 297, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     fetched = builder.get(to_fetch)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/utils/tf_run_builder.py\", line 42, in get\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     self._executed = run_timeline(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/utils/tf_run_builder.py\", line 92, in run_timeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 970, in run\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1193, in _run\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     results = self._do_run(handle, final_targets, final_fetches,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1373, in _do_run\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     return self._do_call(_run_fn, feeds, fetches, targets, options,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1380, in _do_call\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     return fn(*args)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1363, in _run_fn\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1456, in _call_tf_sessionrun\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/worker.py\", line 428, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11116)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m 2022-01-03 13:43:16,712\tERROR worker.py:431 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"python/ray/_raylet.pyx\", line 759, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"python/ray/_raylet.pyx\", line 580, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"python/ray/_raylet.pyx\", line 618, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"python/ray/_raylet.pyx\", line 625, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"python/ray/_raylet.pyx\", line 629, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"python/ray/_raylet.pyx\", line 578, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 609, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     return next(self.local_it)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 381, in gen_rollouts\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     yield self.sample()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 757, in sample\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 103, in next\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 265, in get_data\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 656, in _env_runner\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     eval_results = _do_policy_eval(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 1070, in _do_policy_eval\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     policy.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/policy/tf_policy.py\", line 297, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     fetched = builder.get(to_fetch)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/utils/tf_run_builder.py\", line 42, in get\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     self._executed = run_timeline(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/rllib/utils/tf_run_builder.py\", line 92, in run_timeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     fetches = sess.run(ops, feed_dict=feed_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 970, in run\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     result = self._run(None, fetches, feed_dict, options_ptr,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1193, in _run\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     results = self._do_run(handle, final_targets, final_fetches,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1373, in _do_run\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     return self._do_call(_run_fn, feeds, fetches, targets, options,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1380, in _do_call\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     return fn(*args)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1363, in _run_fn\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\", line 1456, in _call_tf_sessionrun\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m   File \"/home/dibya/miniconda3/envs/deep_rl_get_started_fast_python3.9/lib/python3.9/site-packages/ray/worker.py\", line 428, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11130)\u001b[0m SystemExit: 1\n",
      "2022-01-03 13:43:16,909\tERROR tune.py:622 -- Trials did not complete: [PPO_CartPole-v1_24a1e_00000]\n",
      "2022-01-03 13:43:16,910\tINFO tune.py:626 -- Total run time: 249.19 seconds (247.21 seconds for the tuning loop).\n",
      "2022-01-03 13:43:16,911\tWARNING tune.py:630 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fb371024640>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"CartPole-v1\",\n",
    "                 # other configurations go here, if none provided, then default configurations will be used\n",
    "                 }\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c39ff-9dca-476c-8452-272f495c7695",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration\n",
    "\n",
    "These configurations are applied in sequence\n",
    "\n",
    "1. [Common config](https://docs.ray.io/en/master/rllib-training.html#common-parameters)\n",
    "2. [Algorithm specific config (overrides common config)](https://docs.ray.io/en/master/rllib-algorithms.html#ppo)\n",
    "3. User defined config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a495bd-1aa6-41b8-8d63-548c77748d42",
   "metadata": {},
   "source": [
    "### Anatomy of an experiment\n",
    "\n",
    "<img src=\"images/ex/2.png\" width=\"750\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185adb02-d8b2-43c7-bed2-7efb3f89fe67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=11117)\u001b[0m 2022-01-03 13:43:18,891\tINFO trainer.py:722 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also want to then set `eager_tracing=True` in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=11117)\u001b[0m 2022-01-03 13:43:18,891\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=11117)\u001b[0m 2022-01-03 13:43:18,891\tINFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11132)\u001b[0m 2022-01-03 13:43:20,793\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=11117)\u001b[0m 2022-01-03 13:43:21,485\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=11117)\u001b[0m 2022-01-03 13:43:21,980\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:22 (running for 00:00:05.69)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=11117)\u001b[0m 2022-01-03 13:43:22,625\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:23 (running for 00:00:06.70)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=11117)\u001b[0m 2022-01-03 13:43:25,775\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:28 (running for 00:00:11.71)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-43-29\n",
      "  done: false\n",
      "  episode_len_mean: 22.429378531073446\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 106.0\n",
      "  episode_reward_mean: 22.429378531073446\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 177\n",
      "  episodes_total: 177\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6658107042312622\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028826190158724785\n",
      "          model: {}\n",
      "          policy_loss: -0.04175363481044769\n",
      "          total_loss: 234.28335571289062\n",
      "          vf_explained_var: 0.026140272617340088\n",
      "          vf_loss: 234.3193359375\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.040000000000001\n",
      "    ram_util_percent: 11.89\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07999153254498818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0827080066954169\n",
      "    mean_inference_ms: 1.1430074741710328\n",
      "    mean_raw_obs_processing_ms: 0.1307997557624138\n",
      "  time_since_restore: 6.783697605133057\n",
      "  time_this_iter_s: 6.783697605133057\n",
      "  time_total_s: 6.783697605133057\n",
      "  timers:\n",
      "    learn_throughput: 1106.175\n",
      "    learn_time_ms: 3616.064\n",
      "    load_throughput: 6579300.392\n",
      "    load_time_ms: 0.608\n",
      "    sample_throughput: 1054.176\n",
      "    sample_time_ms: 3794.432\n",
      "    update_time_ms: 3.04\n",
      "  timestamp: 1641213809\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:34 (running for 00:00:17.52)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          6.7837</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.4294</td><td style=\"text-align: right;\">                 106</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">           22.4294</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-43-38\n",
      "  done: false\n",
      "  episode_len_mean: 43.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 179.0\n",
      "  episode_reward_mean: 43.34\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 83\n",
      "  episodes_total: 260\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 74.05\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 234.0\n",
      "    episode_reward_mean: 74.05\n",
      "    episode_reward_min: 11.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 32\n",
      "      - 118\n",
      "      - 157\n",
      "      - 50\n",
      "      - 38\n",
      "      - 46\n",
      "      - 70\n",
      "      - 18\n",
      "      - 62\n",
      "      - 11\n",
      "      - 81\n",
      "      - 18\n",
      "      - 234\n",
      "      - 24\n",
      "      - 97\n",
      "      - 171\n",
      "      - 21\n",
      "      - 34\n",
      "      - 106\n",
      "      - 93\n",
      "      episode_reward:\n",
      "      - 32.0\n",
      "      - 118.0\n",
      "      - 157.0\n",
      "      - 50.0\n",
      "      - 38.0\n",
      "      - 46.0\n",
      "      - 70.0\n",
      "      - 18.0\n",
      "      - 62.0\n",
      "      - 11.0\n",
      "      - 81.0\n",
      "      - 18.0\n",
      "      - 234.0\n",
      "      - 24.0\n",
      "      - 97.0\n",
      "      - 171.0\n",
      "      - 21.0\n",
      "      - 34.0\n",
      "      - 106.0\n",
      "      - 93.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1034942554880572\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10477450855991297\n",
      "      mean_inference_ms: 1.421327050398236\n",
      "      mean_raw_obs_processing_ms: 0.134013925004102\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6172718405723572\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016548188403248787\n",
      "          model: {}\n",
      "          policy_loss: -0.02616402879357338\n",
      "          total_loss: 411.5581970214844\n",
      "          vf_explained_var: 0.06493451446294785\n",
      "          vf_loss: 411.5793762207031\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.823076923076922\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07969391794581968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08267410284811398\n",
      "    mean_inference_ms: 1.1404213830478316\n",
      "    mean_raw_obs_processing_ms: 0.1258646349669711\n",
      "  time_since_restore: 16.069972276687622\n",
      "  time_this_iter_s: 9.286274671554565\n",
      "  time_total_s: 16.069972276687622\n",
      "  timers:\n",
      "    learn_throughput: 1123.892\n",
      "    learn_time_ms: 3559.06\n",
      "    load_throughput: 6836681.337\n",
      "    load_time_ms: 0.585\n",
      "    sample_throughput: 760.484\n",
      "    sample_time_ms: 5259.809\n",
      "    update_time_ms: 3.044\n",
      "  timestamp: 1641213818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:39 (running for 00:00:22.85)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">           16.07</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   43.34</td><td style=\"text-align: right;\">                 179</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             43.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:44 (running for 00:00:27.86)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">           16.07</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   43.34</td><td style=\"text-align: right;\">                 179</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             43.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-43-45\n",
      "  done: false\n",
      "  episode_len_mean: 66.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 206.0\n",
      "  episode_reward_mean: 66.58\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 306\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5684974789619446\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010665263049304485\n",
      "          model: {}\n",
      "          policy_loss: -0.019676998257637024\n",
      "          total_loss: 515.532958984375\n",
      "          vf_explained_var: 0.09138352423906326\n",
      "          vf_loss: 515.5494384765625\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.4\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07940025778872509\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08240435142986875\n",
      "    mean_inference_ms: 1.136490450824648\n",
      "    mean_raw_obs_processing_ms: 0.12282318029898445\n",
      "  time_since_restore: 22.610326051712036\n",
      "  time_this_iter_s: 6.540353775024414\n",
      "  time_total_s: 22.610326051712036\n",
      "  timers:\n",
      "    learn_throughput: 1131.933\n",
      "    learn_time_ms: 3533.778\n",
      "    load_throughput: 7349831.776\n",
      "    load_time_ms: 0.544\n",
      "    sample_throughput: 604.227\n",
      "    sample_time_ms: 6620.025\n",
      "    update_time_ms: 2.673\n",
      "  timestamp: 1641213825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:50 (running for 00:00:33.43)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         22.6103</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   66.58</td><td style=\"text-align: right;\">                 206</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             66.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:43:55 (running for 00:00:38.43)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         22.6103</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   66.58</td><td style=\"text-align: right;\">                 206</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             66.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:00 (running for 00:00:43.44)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         22.6103</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   66.58</td><td style=\"text-align: right;\">                 206</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             66.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-44-02\n",
      "  done: false\n",
      "  episode_len_mean: 96.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 360.0\n",
      "  episode_reward_mean: 96.8\n",
      "  episode_reward_min: 12.0\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 328\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 293.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 293.0\n",
      "    episode_reward_min: 91.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 91\n",
      "      - 121\n",
      "      - 221\n",
      "      - 203\n",
      "      - 271\n",
      "      - 285\n",
      "      - 325\n",
      "      - 343\n",
      "      - 275\n",
      "      - 403\n",
      "      - 385\n",
      "      - 500\n",
      "      - 268\n",
      "      - 471\n",
      "      - 299\n",
      "      - 301\n",
      "      - 307\n",
      "      - 285\n",
      "      - 238\n",
      "      - 268\n",
      "      episode_reward:\n",
      "      - 91.0\n",
      "      - 121.0\n",
      "      - 221.0\n",
      "      - 203.0\n",
      "      - 271.0\n",
      "      - 285.0\n",
      "      - 325.0\n",
      "      - 343.0\n",
      "      - 275.0\n",
      "      - 403.0\n",
      "      - 385.0\n",
      "      - 500.0\n",
      "      - 268.0\n",
      "      - 471.0\n",
      "      - 299.0\n",
      "      - 301.0\n",
      "      - 307.0\n",
      "      - 285.0\n",
      "      - 238.0\n",
      "      - 268.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10427221294580773\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1071216429458656\n",
      "      mean_inference_ms: 1.4545869775313314\n",
      "      mean_raw_obs_processing_ms: 0.12946557492074579\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5436378121376038\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008456626906991005\n",
      "          model: {}\n",
      "          policy_loss: -0.0228178258985281\n",
      "          total_loss: 670.0186767578125\n",
      "          vf_explained_var: 0.18131940066814423\n",
      "          vf_loss: 670.0390014648438\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.25\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07878085177973043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08179089944402097\n",
      "    mean_inference_ms: 1.1281064536294392\n",
      "    mean_raw_obs_processing_ms: 0.12036713776419888\n",
      "  time_since_restore: 39.70125675201416\n",
      "  time_this_iter_s: 17.090930700302124\n",
      "  time_total_s: 39.70125675201416\n",
      "  timers:\n",
      "    learn_throughput: 1137.099\n",
      "    learn_time_ms: 3517.724\n",
      "    load_throughput: 8051453.389\n",
      "    load_time_ms: 0.497\n",
      "    sample_throughput: 606.929\n",
      "    sample_time_ms: 6590.555\n",
      "    update_time_ms: 2.693\n",
      "  timestamp: 1641213842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:05 (running for 00:00:48.56)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         39.7013</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">    96.8</td><td style=\"text-align: right;\">                 360</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">              96.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-44-09\n",
      "  done: false\n",
      "  episode_len_mean: 131.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 131.82\n",
      "  episode_reward_min: 14.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 340\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5397278070449829\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006242944393306971\n",
      "          model: {}\n",
      "          policy_loss: -0.01558975875377655\n",
      "          total_loss: 694.2406616210938\n",
      "          vf_explained_var: 0.2767491638660431\n",
      "          vf_loss: 694.2542724609375\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.87\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07900307051123598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08201445653199607\n",
      "    mean_inference_ms: 1.1313530922681776\n",
      "    mean_raw_obs_processing_ms: 0.1199765393807445\n",
      "  time_since_restore: 46.35895276069641\n",
      "  time_this_iter_s: 6.657696008682251\n",
      "  time_total_s: 46.35895276069641\n",
      "  timers:\n",
      "    learn_throughput: 1140.089\n",
      "    learn_time_ms: 3508.498\n",
      "    load_throughput: 7838355.448\n",
      "    load_time_ms: 0.51\n",
      "    sample_throughput: 457.703\n",
      "    sample_time_ms: 8739.294\n",
      "    update_time_ms: 2.707\n",
      "  timestamp: 1641213849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:11 (running for 00:00:54.26)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          46.359</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  131.82</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            131.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:16 (running for 00:00:59.27)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          46.359</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  131.82</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            131.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:21 (running for 00:01:04.28)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          46.359</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  131.82</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            131.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:26 (running for 00:01:09.29)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          46.359</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  131.82</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            131.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-44-28\n",
      "  done: false\n",
      "  episode_len_mean: 163.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 163.4\n",
      "  episode_reward_min: 14.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 350\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 365.55\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 365.55\n",
      "    episode_reward_min: 133.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 406\n",
      "      - 500\n",
      "      - 281\n",
      "      - 404\n",
      "      - 400\n",
      "      - 279\n",
      "      - 271\n",
      "      - 458\n",
      "      - 441\n",
      "      - 304\n",
      "      - 440\n",
      "      - 397\n",
      "      - 296\n",
      "      - 500\n",
      "      - 133\n",
      "      - 262\n",
      "      - 414\n",
      "      - 473\n",
      "      - 270\n",
      "      - 382\n",
      "      episode_reward:\n",
      "      - 406.0\n",
      "      - 500.0\n",
      "      - 281.0\n",
      "      - 404.0\n",
      "      - 400.0\n",
      "      - 279.0\n",
      "      - 271.0\n",
      "      - 458.0\n",
      "      - 441.0\n",
      "      - 304.0\n",
      "      - 440.0\n",
      "      - 397.0\n",
      "      - 296.0\n",
      "      - 500.0\n",
      "      - 133.0\n",
      "      - 262.0\n",
      "      - 414.0\n",
      "      - 473.0\n",
      "      - 270.0\n",
      "      - 382.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10014727425918575\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10307793697627146\n",
      "      mean_inference_ms: 1.4106295812189358\n",
      "      mean_raw_obs_processing_ms: 0.12409709008050959\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5589821338653564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0039726984687149525\n",
      "          model: {}\n",
      "          policy_loss: -0.010850244201719761\n",
      "          total_loss: 441.4124450683594\n",
      "          vf_explained_var: 0.3547799587249756\n",
      "          vf_loss: 441.4220886230469\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.774074074074074\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07927501205942958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08230437368200383\n",
      "    mean_inference_ms: 1.1353438701977427\n",
      "    mean_raw_obs_processing_ms: 0.11964107912874493\n",
      "  time_since_restore: 65.38984847068787\n",
      "  time_this_iter_s: 19.030895709991455\n",
      "  time_total_s: 65.38984847068787\n",
      "  timers:\n",
      "    learn_throughput: 1143.818\n",
      "    learn_time_ms: 3497.06\n",
      "    load_throughput: 8657718.758\n",
      "    load_time_ms: 0.462\n",
      "    sample_throughput: 475.646\n",
      "    sample_time_ms: 8409.612\n",
      "    update_time_ms: 2.581\n",
      "  timestamp: 1641213868\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:31 (running for 00:01:14.34)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         65.3898</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">   163.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             163.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-44-34\n",
      "  done: false\n",
      "  episode_len_mean: 193.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 193.4\n",
      "  episode_reward_min: 14.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 360\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5351145267486572\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00829379539936781\n",
      "          model: {}\n",
      "          policy_loss: -0.011822070926427841\n",
      "          total_loss: 362.6712341308594\n",
      "          vf_explained_var: 0.2681409418582916\n",
      "          vf_loss: 362.6817932128906\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.166666666666666\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07949524436539876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08253404854247841\n",
      "    mean_inference_ms: 1.1386905747585974\n",
      "    mean_raw_obs_processing_ms: 0.1191516615134056\n",
      "  time_since_restore: 71.94238519668579\n",
      "  time_this_iter_s: 6.552536725997925\n",
      "  time_total_s: 71.94238519668579\n",
      "  timers:\n",
      "    learn_throughput: 1145.349\n",
      "    learn_time_ms: 3492.385\n",
      "    load_throughput: 8423505.379\n",
      "    load_time_ms: 0.475\n",
      "    sample_throughput: 403.575\n",
      "    sample_time_ms: 9911.416\n",
      "    update_time_ms: 2.52\n",
      "  timestamp: 1641213874\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:36 (running for 00:01:19.94)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         71.9424</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   193.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             193.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:41 (running for 00:01:24.95)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         71.9424</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   193.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             193.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:46 (running for 00:01:29.96)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         71.9424</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   193.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             193.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:51 (running for 00:01:34.97)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         71.9424</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   193.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             193.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:44:56 (running for 00:01:39.97)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         71.9424</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   193.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             193.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-44-58\n",
      "  done: false\n",
      "  episode_len_mean: 225.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 225.28\n",
      "  episode_reward_min: 14.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 369\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 468.1\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 468.1\n",
      "    episode_reward_min: 316.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 431\n",
      "      - 443\n",
      "      - 316\n",
      "      - 343\n",
      "      - 474\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 499\n",
      "      - 500\n",
      "      - 500\n",
      "      - 356\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 431.0\n",
      "      - 443.0\n",
      "      - 316.0\n",
      "      - 343.0\n",
      "      - 474.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 499.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 356.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10106344657864592\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10413185348367779\n",
      "      mean_inference_ms: 1.4217966227836218\n",
      "      mean_raw_obs_processing_ms: 0.12453373288105556\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5113347172737122\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004379384219646454\n",
      "          model: {}\n",
      "          policy_loss: -0.007106391713023186\n",
      "          total_loss: 328.7005310058594\n",
      "          vf_explained_var: 0.28214773535728455\n",
      "          vf_loss: 328.7070007324219\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.499999999999998\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07931333257735361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08237211894150098\n",
      "    mean_inference_ms: 1.1366604480079978\n",
      "    mean_raw_obs_processing_ms: 0.11816209307487437\n",
      "  time_since_restore: 95.2527289390564\n",
      "  time_this_iter_s: 23.310343742370605\n",
      "  time_total_s: 95.2527289390564\n",
      "  timers:\n",
      "    learn_throughput: 1146.529\n",
      "    learn_time_ms: 3488.791\n",
      "    load_throughput: 8094182.125\n",
      "    load_time_ms: 0.494\n",
      "    sample_throughput: 420.724\n",
      "    sample_time_ms: 9507.418\n",
      "    update_time_ms: 2.46\n",
      "  timestamp: 1641213898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:02 (running for 00:01:45.30)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         95.2527</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  225.28</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            225.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-45-04\n",
      "  done: false\n",
      "  episode_len_mean: 260.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 260.1\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 378\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.48875856399536133\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005784942302852869\n",
      "          model: {}\n",
      "          policy_loss: -0.004727379884570837\n",
      "          total_loss: 388.62091064453125\n",
      "          vf_explained_var: 0.15254899859428406\n",
      "          vf_loss: 388.6252136230469\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.766666666666666\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07913206874047868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08220464437410616\n",
      "    mean_inference_ms: 1.134642407090073\n",
      "    mean_raw_obs_processing_ms: 0.1171311694544639\n",
      "  time_since_restore: 101.90212655067444\n",
      "  time_this_iter_s: 6.649397611618042\n",
      "  time_total_s: 101.90212655067444\n",
      "  timers:\n",
      "    learn_throughput: 1144.962\n",
      "    learn_time_ms: 3493.567\n",
      "    load_throughput: 7803356.279\n",
      "    load_time_ms: 0.513\n",
      "    sample_throughput: 362.298\n",
      "    sample_time_ms: 11040.629\n",
      "    update_time_ms: 2.381\n",
      "  timestamp: 1641213904\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:07 (running for 00:01:50.99)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         101.902</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   260.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">             260.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:12 (running for 00:01:56.00)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         101.902</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   260.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">             260.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:17 (running for 00:02:01.01)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         101.902</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   260.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">             260.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:22 (running for 00:02:06.02)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         101.902</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">   260.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">             260.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-45-27\n",
      "  done: false\n",
      "  episode_len_mean: 294.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 294.48\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 389\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 459.65\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 459.65\n",
      "    episode_reward_min: 381.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 421\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 471\n",
      "      - 500\n",
      "      - 500\n",
      "      - 456\n",
      "      - 469\n",
      "      - 458\n",
      "      - 406\n",
      "      - 396\n",
      "      - 394\n",
      "      - 449\n",
      "      - 500\n",
      "      - 381\n",
      "      - 392\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 421.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 471.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 456.0\n",
      "      - 469.0\n",
      "      - 458.0\n",
      "      - 406.0\n",
      "      - 396.0\n",
      "      - 394.0\n",
      "      - 449.0\n",
      "      - 500.0\n",
      "      - 381.0\n",
      "      - 392.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10099773003778753\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1041771087035361\n",
      "      mean_inference_ms: 1.422287756722969\n",
      "      mean_raw_obs_processing_ms: 0.12433040690577138\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5206699967384338\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006092665251344442\n",
      "          model: {}\n",
      "          policy_loss: -0.006521477364003658\n",
      "          total_loss: 176.64041137695312\n",
      "          vf_explained_var: 0.539181649684906\n",
      "          vf_loss: 176.64646911621094\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.1375\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07918270639411773\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08226233495294545\n",
      "    mean_inference_ms: 1.1356321120490847\n",
      "    mean_raw_obs_processing_ms: 0.11634345193573531\n",
      "  time_since_restore: 124.45768737792969\n",
      "  time_this_iter_s: 22.55556082725525\n",
      "  time_total_s: 124.45768737792969\n",
      "  timers:\n",
      "    learn_throughput: 1145.753\n",
      "    learn_time_ms: 3491.154\n",
      "    load_throughput: 7670285.74\n",
      "    load_time_ms: 0.521\n",
      "    sample_throughput: 377.97\n",
      "    sample_time_ms: 10582.837\n",
      "    update_time_ms: 2.422\n",
      "  timestamp: 1641213927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:28 (running for 00:02:11.60)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         124.458</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  294.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            294.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:33 (running for 00:02:16.61)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         124.458</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  294.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            294.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-45-34\n",
      "  done: false\n",
      "  episode_len_mean: 324.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 324.49\n",
      "  episode_reward_min: 27.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 397\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5341718196868896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008021753281354904\n",
      "          model: {}\n",
      "          policy_loss: -0.004115492105484009\n",
      "          total_loss: 140.74203491210938\n",
      "          vf_explained_var: 0.48715221881866455\n",
      "          vf_loss: 140.74554443359375\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.470000000000002\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0794586475353722\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08255020108052168\n",
      "    mean_inference_ms: 1.139531941899478\n",
      "    mean_raw_obs_processing_ms: 0.11614808624770369\n",
      "  time_since_restore: 131.24266409873962\n",
      "  time_this_iter_s: 6.7849767208099365\n",
      "  time_total_s: 131.24266409873962\n",
      "  timers:\n",
      "    learn_throughput: 1151.312\n",
      "    learn_time_ms: 3474.298\n",
      "    load_throughput: 8219693.303\n",
      "    load_time_ms: 0.487\n",
      "    sample_throughput: 319.776\n",
      "    sample_time_ms: 12508.759\n",
      "    update_time_ms: 2.315\n",
      "  timestamp: 1641213934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:39 (running for 00:02:22.42)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         131.243</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  324.49</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">            324.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:44 (running for 00:02:27.43)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         131.243</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  324.49</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">            324.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:49 (running for 00:02:32.44)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         131.243</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  324.49</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">            324.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:54 (running for 00:02:37.45)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         131.243</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  324.49</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">            324.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-45-57\n",
      "  done: false\n",
      "  episode_len_mean: 357.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 357.11\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 406\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 492.4\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 492.4\n",
      "    episode_reward_min: 380.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 380\n",
      "      - 500\n",
      "      - 500\n",
      "      - 468\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 380.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 468.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10025475779150288\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10338942276328436\n",
      "      mean_inference_ms: 1.4138633834522745\n",
      "      mean_raw_obs_processing_ms: 0.12335116448609725\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07500000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5026039481163025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003474506549537182\n",
      "          model: {}\n",
      "          policy_loss: -0.0017497400986030698\n",
      "          total_loss: 309.4649963378906\n",
      "          vf_explained_var: 0.3694356679916382\n",
      "          vf_loss: 309.4664611816406\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.087878787878788\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07973361019686534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08283335170955519\n",
      "    mean_inference_ms: 1.1434501056490693\n",
      "    mean_raw_obs_processing_ms: 0.11585680569932451\n",
      "  time_since_restore: 154.50243520736694\n",
      "  time_this_iter_s: 23.25977110862732\n",
      "  time_total_s: 154.50243520736694\n",
      "  timers:\n",
      "    learn_throughput: 1152.721\n",
      "    learn_time_ms: 3470.051\n",
      "    load_throughput: 8441366.541\n",
      "    load_time_ms: 0.474\n",
      "    sample_throughput: 320.537\n",
      "    sample_time_ms: 12479.05\n",
      "    update_time_ms: 2.238\n",
      "  timestamp: 1641213957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:45:59 (running for 00:02:42.72)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         154.502</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  357.11</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            357.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 381.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 381.1\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 414\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5165870785713196\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005959213245660067\n",
      "          model: {}\n",
      "          policy_loss: -0.0020698525477200747\n",
      "          total_loss: 180.06182861328125\n",
      "          vf_explained_var: 0.5193233489990234\n",
      "          vf_loss: 180.0636749267578\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.1\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07967921933349016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08279823550903932\n",
      "    mean_inference_ms: 1.1432283706147028\n",
      "    mean_raw_obs_processing_ms: 0.11522529024906841\n",
      "  time_since_restore: 161.2825345993042\n",
      "  time_this_iter_s: 6.780099391937256\n",
      "  time_total_s: 161.2825345993042\n",
      "  timers:\n",
      "    learn_throughput: 1151.081\n",
      "    learn_time_ms: 3474.994\n",
      "    load_throughput: 8237855.249\n",
      "    load_time_ms: 0.486\n",
      "    sample_throughput: 287.649\n",
      "    sample_time_ms: 13905.856\n",
      "    update_time_ms: 2.297\n",
      "  timestamp: 1641213964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:05 (running for 00:02:48.55)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         161.283</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   381.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             381.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:10 (running for 00:02:53.55)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         161.283</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   381.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             381.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:15 (running for 00:02:58.56)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         161.283</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   381.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             381.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:20 (running for 00:03:03.57)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         161.283</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   381.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             381.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:25 (running for 00:03:08.58)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         161.283</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   381.1</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             381.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 406.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 406.44\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 422\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 495.85\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 495.85\n",
      "    episode_reward_min: 417.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 417\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 417.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.100465290110699\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1036170394321652\n",
      "      mean_inference_ms: 1.4179030730874522\n",
      "      mean_raw_obs_processing_ms: 0.1238538678226698\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5151876211166382\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0054946839809417725\n",
      "          model: {}\n",
      "          policy_loss: -0.0007648436003364623\n",
      "          total_loss: 276.9347839355469\n",
      "          vf_explained_var: 0.4184960424900055\n",
      "          vf_loss: 276.9353332519531\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.285714285714285\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07992168915611161\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08305190723397197\n",
      "    mean_inference_ms: 1.146826994029727\n",
      "    mean_raw_obs_processing_ms: 0.11502705733966839\n",
      "  time_since_restore: 185.71392345428467\n",
      "  time_this_iter_s: 24.43138885498047\n",
      "  time_total_s: 185.71392345428467\n",
      "  timers:\n",
      "    learn_throughput: 1151.121\n",
      "    learn_time_ms: 3474.873\n",
      "    load_throughput: 7839087.936\n",
      "    load_time_ms: 0.51\n",
      "    sample_throughput: 286.755\n",
      "    sample_time_ms: 13949.167\n",
      "    update_time_ms: 2.213\n",
      "  timestamp: 1641213988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:30 (running for 00:03:14.03)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         185.714</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  406.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            406.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-46-35\n",
      "  done: false\n",
      "  episode_len_mean: 432.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 432.59\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 430\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5273802876472473\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00733661325648427\n",
      "          model: {}\n",
      "          policy_loss: 0.0002818219072651118\n",
      "          total_loss: 304.7541809082031\n",
      "          vf_explained_var: 0.2888195216655731\n",
      "          vf_loss: 304.7536315917969\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.4\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08017815411389745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08331997253915105\n",
      "    mean_inference_ms: 1.150923691646074\n",
      "    mean_raw_obs_processing_ms: 0.11487508726545222\n",
      "  time_since_restore: 192.54755973815918\n",
      "  time_this_iter_s: 6.833636283874512\n",
      "  time_total_s: 192.54755973815918\n",
      "  timers:\n",
      "    learn_throughput: 1151.35\n",
      "    learn_time_ms: 3474.182\n",
      "    load_throughput: 7810258.368\n",
      "    load_time_ms: 0.512\n",
      "    sample_throughput: 272.753\n",
      "    sample_time_ms: 14665.272\n",
      "    update_time_ms: 2.147\n",
      "  timestamp: 1641213995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:36 (running for 00:03:19.91)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         192.548</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  432.59</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            432.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:41 (running for 00:03:24.91)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         192.548</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  432.59</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            432.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:46 (running for 00:03:29.92)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         192.548</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  432.59</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            432.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:51 (running for 00:03:34.93)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         192.548</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  432.59</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            432.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-46-56\n",
      "  done: false\n",
      "  episode_len_mean: 444.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 444.6\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 438\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 414.8\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 414.8\n",
      "    episode_reward_min: 144.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 216\n",
      "      - 417\n",
      "      - 500\n",
      "      - 500\n",
      "      - 278\n",
      "      - 500\n",
      "      - 500\n",
      "      - 386\n",
      "      - 388\n",
      "      - 379\n",
      "      - 144\n",
      "      - 367\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 221\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 216.0\n",
      "      - 417.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 278.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 386.0\n",
      "      - 388.0\n",
      "      - 379.0\n",
      "      - 144.0\n",
      "      - 367.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 221.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10018910292198142\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10337515402961692\n",
      "      mean_inference_ms: 1.414373075126012\n",
      "      mean_raw_obs_processing_ms: 0.12350604226305173\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5323923826217651\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004643614869564772\n",
      "          model: {}\n",
      "          policy_loss: -0.0038051274605095387\n",
      "          total_loss: 152.39059448242188\n",
      "          vf_explained_var: 0.44824594259262085\n",
      "          vf_loss: 152.39422607421875\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.958620689655175\n",
      "    ram_util_percent: 11.837931034482763\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.080265079143969\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08341594832395156\n",
      "    mean_inference_ms: 1.1527488471778489\n",
      "    mean_raw_obs_processing_ms: 0.11456734938602098\n",
      "  time_since_restore: 213.35079216957092\n",
      "  time_this_iter_s: 20.803232431411743\n",
      "  time_total_s: 213.35079216957092\n",
      "  timers:\n",
      "    learn_throughput: 1151.035\n",
      "    learn_time_ms: 3475.134\n",
      "    load_throughput: 7780557.436\n",
      "    load_time_ms: 0.514\n",
      "    sample_throughput: 273.103\n",
      "    sample_time_ms: 14646.479\n",
      "    update_time_ms: 2.217\n",
      "  timestamp: 1641214016\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:46:57 (running for 00:03:40.76)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         213.351</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">   444.6</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             444.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:02 (running for 00:03:45.76)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         213.351</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">   444.6</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             444.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-47-03\n",
      "  done: false\n",
      "  episode_len_mean: 453.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 453.71\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 447\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4969511032104492\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005400669761002064\n",
      "          model: {}\n",
      "          policy_loss: -0.011936291120946407\n",
      "          total_loss: 270.889404296875\n",
      "          vf_explained_var: 0.40124747157096863\n",
      "          vf_loss: 270.9012451171875\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.89\n",
      "    ram_util_percent: 11.76\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08025624234848648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08340322857139394\n",
      "    mean_inference_ms: 1.1532691326338969\n",
      "    mean_raw_obs_processing_ms: 0.11415906490683135\n",
      "  time_since_restore: 219.7460675239563\n",
      "  time_this_iter_s: 6.395275354385376\n",
      "  time_total_s: 219.7460675239563\n",
      "  timers:\n",
      "    learn_throughput: 1151.531\n",
      "    learn_time_ms: 3473.637\n",
      "    load_throughput: 7698089.382\n",
      "    load_time_ms: 0.52\n",
      "    sample_throughput: 269.779\n",
      "    sample_time_ms: 14826.967\n",
      "    update_time_ms: 2.294\n",
      "  timestamp: 1641214023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:08 (running for 00:03:51.19)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         219.746</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  453.71</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            453.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:13 (running for 00:03:56.20)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         219.746</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  453.71</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            453.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:18 (running for 00:04:01.21)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         219.746</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  453.71</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            453.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:23 (running for 00:04:06.22)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         219.746</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  453.71</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            453.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-47-26\n",
      "  done: false\n",
      "  episode_len_mean: 462.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 462.61\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 455\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 493.9\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 493.9\n",
      "    episode_reward_min: 378.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 378\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 378.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09978511039342158\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10303779970507634\n",
      "      mean_inference_ms: 1.4098502244946258\n",
      "      mean_raw_obs_processing_ms: 0.12326797487389016\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.47515296936035156\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007485141512006521\n",
      "          model: {}\n",
      "          policy_loss: -0.004375540651381016\n",
      "          total_loss: 225.72230529785156\n",
      "          vf_explained_var: 0.5140494108200073\n",
      "          vf_loss: 225.72653198242188\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.212121212121213\n",
      "    ram_util_percent: 11.884848484848485\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08029113218771436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0834396962903968\n",
      "    mean_inference_ms: 1.154212622440793\n",
      "    mean_raw_obs_processing_ms: 0.11389584928969652\n",
      "  time_since_restore: 243.26226925849915\n",
      "  time_this_iter_s: 23.516201734542847\n",
      "  time_total_s: 243.26226925849915\n",
      "  timers:\n",
      "    learn_throughput: 1152.033\n",
      "    learn_time_ms: 3472.122\n",
      "    load_throughput: 8258129.553\n",
      "    load_time_ms: 0.484\n",
      "    sample_throughput: 269.805\n",
      "    sample_time_ms: 14825.515\n",
      "    update_time_ms: 2.275\n",
      "  timestamp: 1641214046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:28 (running for 00:04:11.74)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         243.262</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  462.61</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            462.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-47-32\n",
      "  done: false\n",
      "  episode_len_mean: 465.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 465.42\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 463\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.46219807863235474\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004400151781737804\n",
      "          model: {}\n",
      "          policy_loss: -0.002735947957262397\n",
      "          total_loss: 140.81980895996094\n",
      "          vf_explained_var: 0.56380695104599\n",
      "          vf_loss: 140.82244873046875\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.944444444444445\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08029197087482284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08342756636082208\n",
      "    mean_inference_ms: 1.154621186293605\n",
      "    mean_raw_obs_processing_ms: 0.11362897572342391\n",
      "  time_since_restore: 249.51125812530518\n",
      "  time_this_iter_s: 6.24898886680603\n",
      "  time_total_s: 249.51125812530518\n",
      "  timers:\n",
      "    learn_throughput: 1154.233\n",
      "    learn_time_ms: 3465.505\n",
      "    load_throughput: 8391964.786\n",
      "    load_time_ms: 0.477\n",
      "    sample_throughput: 270.068\n",
      "    sample_time_ms: 14811.097\n",
      "    update_time_ms: 2.316\n",
      "  timestamp: 1641214052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:33 (running for 00:04:17.03)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         249.511</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  465.42</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            465.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:38 (running for 00:04:22.04)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         249.511</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  465.42</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            465.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:43 (running for 00:04:27.05)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         249.511</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  465.42</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            465.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:48 (running for 00:04:32.06)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         249.511</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  465.42</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            465.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:54 (running for 00:04:37.06)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         249.511</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  465.42</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            465.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-47-57\n",
      "  done: false\n",
      "  episode_len_mean: 468.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 468.27\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 471\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.100105893292095\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10337834525863725\n",
      "      mean_inference_ms: 1.4144293368483374\n",
      "      mean_raw_obs_processing_ms: 0.12340765953569317\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00937500037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.45820188522338867\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0031977591570466757\n",
      "          model: {}\n",
      "          policy_loss: 0.0005085796001367271\n",
      "          total_loss: 181.96998596191406\n",
      "          vf_explained_var: 0.5409795641899109\n",
      "          vf_loss: 181.96945190429688\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.577142857142858\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08028119486507443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08340499456734475\n",
      "    mean_inference_ms: 1.1548213229188307\n",
      "    mean_raw_obs_processing_ms: 0.11338280886639858\n",
      "  time_since_restore: 273.92436146736145\n",
      "  time_this_iter_s: 24.413103342056274\n",
      "  time_total_s: 273.92436146736145\n",
      "  timers:\n",
      "    learn_throughput: 1155.931\n",
      "    learn_time_ms: 3460.415\n",
      "    load_throughput: 8470774.513\n",
      "    load_time_ms: 0.472\n",
      "    sample_throughput: 269.785\n",
      "    sample_time_ms: 14826.643\n",
      "    update_time_ms: 2.268\n",
      "  timestamp: 1641214077\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:47:59 (running for 00:04:42.50)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         273.924</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  468.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            468.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-48-03\n",
      "  done: false\n",
      "  episode_len_mean: 473.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 473.35\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 479\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004687500186264515\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4432362914085388\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00505047058686614\n",
      "          model: {}\n",
      "          policy_loss: 0.0021547339856624603\n",
      "          total_loss: 284.6266784667969\n",
      "          vf_explained_var: 0.5598533153533936\n",
      "          vf_loss: 284.6245422363281\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.066666666666666\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08024151803154615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08335548716827214\n",
      "    mean_inference_ms: 1.1546486884257041\n",
      "    mean_raw_obs_processing_ms: 0.1131199240899339\n",
      "  time_since_restore: 280.30135583877563\n",
      "  time_this_iter_s: 6.376994371414185\n",
      "  time_total_s: 280.30135583877563\n",
      "  timers:\n",
      "    learn_throughput: 1156.786\n",
      "    learn_time_ms: 3457.858\n",
      "    load_throughput: 7911542.016\n",
      "    load_time_ms: 0.506\n",
      "    sample_throughput: 267.526\n",
      "    sample_time_ms: 14951.83\n",
      "    update_time_ms: 2.258\n",
      "  timestamp: 1641214083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:04 (running for 00:04:47.91)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         280.301</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  473.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            473.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:09 (running for 00:04:52.92)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         280.301</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  473.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            473.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:14 (running for 00:04:57.92)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         280.301</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  473.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            473.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:19 (running for 00:05:02.93)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         280.301</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  473.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            473.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:24 (running for 00:05:07.94)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         280.301</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  473.35</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            473.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-48-27\n",
      "  done: false\n",
      "  episode_len_mean: 481.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 481.06\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 487\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09983109608077598\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10304116590148046\n",
      "      mean_inference_ms: 1.4101553587376912\n",
      "      mean_raw_obs_processing_ms: 0.12321155504192804\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.004687500186264515\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.41949203610420227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0030744285322725773\n",
      "          model: {}\n",
      "          policy_loss: 0.0008549469639547169\n",
      "          total_loss: 126.43228912353516\n",
      "          vf_explained_var: 0.48506686091423035\n",
      "          vf_loss: 126.4314193725586\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.458823529411763\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08024448382338494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08336119542740951\n",
      "    mean_inference_ms: 1.1550906718792857\n",
      "    mean_raw_obs_processing_ms: 0.11292802768969422\n",
      "  time_since_restore: 303.8563857078552\n",
      "  time_this_iter_s: 23.55502986907959\n",
      "  time_total_s: 303.8563857078552\n",
      "  timers:\n",
      "    learn_throughput: 1157.294\n",
      "    learn_time_ms: 3456.338\n",
      "    load_throughput: 7744283.604\n",
      "    load_time_ms: 0.517\n",
      "    sample_throughput: 267.318\n",
      "    sample_time_ms: 14963.456\n",
      "    update_time_ms: 2.237\n",
      "  timestamp: 1641214107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:30 (running for 00:05:13.51)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         303.856</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  481.06</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            481.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-48-33\n",
      "  done: false\n",
      "  episode_len_mean: 484.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 484.83\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 495\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0023437500931322575\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4058391749858856\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0034382410813122988\n",
      "          model: {}\n",
      "          policy_loss: -0.0004056684556417167\n",
      "          total_loss: 167.3172149658203\n",
      "          vf_explained_var: 0.7687280774116516\n",
      "          vf_loss: 167.31761169433594\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.200000000000001\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08022232712990335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08333284623699147\n",
      "    mean_inference_ms: 1.1550813329526606\n",
      "    mean_raw_obs_processing_ms: 0.11272178383415525\n",
      "  time_since_restore: 310.25225472450256\n",
      "  time_this_iter_s: 6.395869016647339\n",
      "  time_total_s: 310.25225472450256\n",
      "  timers:\n",
      "    learn_throughput: 1160.233\n",
      "    learn_time_ms: 3447.583\n",
      "    load_throughput: 7775148.763\n",
      "    load_time_ms: 0.514\n",
      "    sample_throughput: 267.578\n",
      "    sample_time_ms: 14948.893\n",
      "    update_time_ms: 2.251\n",
      "  timestamp: 1641214113\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:35 (running for 00:05:18.95)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         310.252</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  484.83</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            484.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:40 (running for 00:05:23.96)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         310.252</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  484.83</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            484.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:45 (running for 00:05:28.96)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         310.252</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  484.83</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            484.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:50 (running for 00:05:33.97)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         310.252</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  484.83</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            484.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:48:55 (running for 00:05:38.98)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         310.252</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  484.83</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            484.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-48-58\n",
      "  done: false\n",
      "  episode_len_mean: 492.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.4\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 503\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09998848846356197\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10319030098548938\n",
      "      mean_inference_ms: 1.411754838645783\n",
      "      mean_raw_obs_processing_ms: 0.12325853495637581\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0011718750465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3959515392780304\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0035098567605018616\n",
      "          model: {}\n",
      "          policy_loss: -0.004721165634691715\n",
      "          total_loss: 111.00982666015625\n",
      "          vf_explained_var: 0.6156907677650452\n",
      "          vf_loss: 111.0145492553711\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.414285714285715\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08022994097113151\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08334166029313898\n",
      "    mean_inference_ms: 1.1555068953752567\n",
      "    mean_raw_obs_processing_ms: 0.11257086602261233\n",
      "  time_since_restore: 334.6890869140625\n",
      "  time_this_iter_s: 24.436832189559937\n",
      "  time_total_s: 334.6890869140625\n",
      "  timers:\n",
      "    learn_throughput: 1160.553\n",
      "    learn_time_ms: 3446.632\n",
      "    load_throughput: 7717210.672\n",
      "    load_time_ms: 0.518\n",
      "    sample_throughput: 267.784\n",
      "    sample_time_ms: 14937.428\n",
      "    update_time_ms: 2.25\n",
      "  timestamp: 1641214138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:01 (running for 00:05:44.44)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         334.689</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-49-04\n",
      "  done: false\n",
      "  episode_len_mean: 492.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.4\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 511\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0005859375232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3711017966270447\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00191035820171237\n",
      "          model: {}\n",
      "          policy_loss: 0.0005888226442039013\n",
      "          total_loss: 270.68115234375\n",
      "          vf_explained_var: 0.4633130133152008\n",
      "          vf_loss: 270.6805419921875\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.122222222222222\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0802107632924208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08331584076486034\n",
      "    mean_inference_ms: 1.1554800397796539\n",
      "    mean_raw_obs_processing_ms: 0.11240112800129712\n",
      "  time_since_restore: 341.08573842048645\n",
      "  time_this_iter_s: 6.39665150642395\n",
      "  time_total_s: 341.08573842048645\n",
      "  timers:\n",
      "    learn_throughput: 1161.701\n",
      "    learn_time_ms: 3443.227\n",
      "    load_throughput: 8316668.815\n",
      "    load_time_ms: 0.481\n",
      "    sample_throughput: 268.448\n",
      "    sample_time_ms: 14900.464\n",
      "    update_time_ms: 2.227\n",
      "  timestamp: 1641214144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:06 (running for 00:05:49.87)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         341.086</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:11 (running for 00:05:54.88)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         341.086</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:16 (running for 00:05:59.89)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         341.086</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:21 (running for 00:06:04.90)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         341.086</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:26 (running for 00:06:09.90)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         341.086</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-49-27\n",
      "  done: false\n",
      "  episode_len_mean: 492.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.4\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 519\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09954568683295094\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.102662717970965\n",
      "      mean_inference_ms: 1.4051876212950658\n",
      "      mean_raw_obs_processing_ms: 0.12253929868607154\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0002929687616415322\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3576214015483856\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004535712767392397\n",
      "          model: {}\n",
      "          policy_loss: -0.0013734217500314116\n",
      "          total_loss: 222.8177490234375\n",
      "          vf_explained_var: 0.4276777505874634\n",
      "          vf_loss: 222.8191375732422\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.427272727272728\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08013078155598903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08322334788237092\n",
      "    mean_inference_ms: 1.1545560525014955\n",
      "    mean_raw_obs_processing_ms: 0.11216874833685868\n",
      "  time_since_restore: 364.052127122879\n",
      "  time_this_iter_s: 22.966388702392578\n",
      "  time_total_s: 364.052127122879\n",
      "  timers:\n",
      "    learn_throughput: 1161.712\n",
      "    learn_time_ms: 3443.194\n",
      "    load_throughput: 8078008.57\n",
      "    load_time_ms: 0.495\n",
      "    sample_throughput: 268.677\n",
      "    sample_time_ms: 14887.748\n",
      "    update_time_ms: 2.151\n",
      "  timestamp: 1641214167\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:32 (running for 00:06:15.90)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         364.052</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-49-34\n",
      "  done: false\n",
      "  episode_len_mean: 492.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.4\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 527\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0001464843808207661\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.36017468571662903\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0034243902191519737\n",
      "          model: {}\n",
      "          policy_loss: 0.0010374116245657206\n",
      "          total_loss: 281.56884765625\n",
      "          vf_explained_var: 0.37897738814353943\n",
      "          vf_loss: 281.5677795410156\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.033333333333335\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08001243148732361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08309049227250064\n",
      "    mean_inference_ms: 1.1529038434635428\n",
      "    mean_raw_obs_processing_ms: 0.11190238371479752\n",
      "  time_since_restore: 370.43571066856384\n",
      "  time_this_iter_s: 6.3835835456848145\n",
      "  time_total_s: 370.43571066856384\n",
      "  timers:\n",
      "    learn_throughput: 1162.242\n",
      "    learn_time_ms: 3441.623\n",
      "    load_throughput: 8171252.679\n",
      "    load_time_ms: 0.49\n",
      "    sample_throughput: 264.653\n",
      "    sample_time_ms: 15114.132\n",
      "    update_time_ms: 2.053\n",
      "  timestamp: 1641214174\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:38 (running for 00:06:21.31)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         370.436</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:43 (running for 00:06:26.32)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         370.436</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:48 (running for 00:06:31.33)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         370.436</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:53 (running for 00:06:36.34)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         370.436</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-49-57\n",
      "  done: false\n",
      "  episode_len_mean: 492.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.4\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 535\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09942641215691356\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10265161153002715\n",
      "      mean_inference_ms: 1.4040633232008823\n",
      "      mean_raw_obs_processing_ms: 0.12241957140361297\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.324219041038305e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.36790016293525696\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0020432034507393837\n",
      "          model: {}\n",
      "          policy_loss: 0.0010514894966036081\n",
      "          total_loss: 349.08477783203125\n",
      "          vf_explained_var: 0.06388817727565765\n",
      "          vf_loss: 349.083740234375\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.458823529411763\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07990143629753838\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08296411776238405\n",
      "    mean_inference_ms: 1.151195582617503\n",
      "    mean_raw_obs_processing_ms: 0.11165261981322569\n",
      "  time_since_restore: 394.09208583831787\n",
      "  time_this_iter_s: 23.65637516975403\n",
      "  time_total_s: 394.09208583831787\n",
      "  timers:\n",
      "    learn_throughput: 1162.646\n",
      "    learn_time_ms: 3440.427\n",
      "    load_throughput: 7685042.371\n",
      "    load_time_ms: 0.52\n",
      "    sample_throughput: 264.987\n",
      "    sample_time_ms: 15095.101\n",
      "    update_time_ms: 2.115\n",
      "  timestamp: 1641214197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:49:58 (running for 00:06:42.02)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         394.092</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:03 (running for 00:06:47.03)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         394.092</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">   492.4</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">             492.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-50-04\n",
      "  done: false\n",
      "  episode_len_mean: 492.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.87\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 543\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.662109520519152e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3628605008125305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037267031148076057\n",
      "          model: {}\n",
      "          policy_loss: -0.0016175990458577871\n",
      "          total_loss: 299.8031005859375\n",
      "          vf_explained_var: 0.33579498529434204\n",
      "          vf_loss: 299.80462646484375\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.233333333333333\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07979603981221543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08284418398483377\n",
      "    mean_inference_ms: 1.1495919600449391\n",
      "    mean_raw_obs_processing_ms: 0.11141823261985014\n",
      "  time_since_restore: 400.56104707717896\n",
      "  time_this_iter_s: 6.468961238861084\n",
      "  time_total_s: 400.56104707717896\n",
      "  timers:\n",
      "    learn_throughput: 1163.158\n",
      "    learn_time_ms: 3438.914\n",
      "    load_throughput: 7657332.725\n",
      "    load_time_ms: 0.522\n",
      "    sample_throughput: 264.004\n",
      "    sample_time_ms: 15151.276\n",
      "    update_time_ms: 2.119\n",
      "  timestamp: 1641214204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:09 (running for 00:06:52.53)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         400.561</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  492.87</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            492.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:14 (running for 00:06:57.54)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         400.561</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  492.87</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            492.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:19 (running for 00:07:02.55)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         400.561</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  492.87</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            492.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:24 (running for 00:07:07.56)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         400.561</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">  492.87</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            492.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-50-28\n",
      "  done: false\n",
      "  episode_len_mean: 494.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 494.29\n",
      "  episode_reward_min: 136.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 551\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09949913197389361\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10266152362793961\n",
      "      mean_inference_ms: 1.4039306472210185\n",
      "      mean_raw_obs_processing_ms: 0.12231681913493524\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.831054760259576e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.36582711338996887\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003521433100104332\n",
      "          model: {}\n",
      "          policy_loss: 0.0003125047078356147\n",
      "          total_loss: 399.474365234375\n",
      "          vf_explained_var: 0.05885005742311478\n",
      "          vf_loss: 399.4740295410156\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.314705882352941\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07971463370052113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08275123618675483\n",
      "    mean_inference_ms: 1.1483230802275544\n",
      "    mean_raw_obs_processing_ms: 0.11122250748701966\n",
      "  time_since_restore: 424.4192636013031\n",
      "  time_this_iter_s: 23.858216524124146\n",
      "  time_total_s: 424.4192636013031\n",
      "  timers:\n",
      "    learn_throughput: 1163.37\n",
      "    learn_time_ms: 3438.287\n",
      "    load_throughput: 7660479.43\n",
      "    load_time_ms: 0.522\n",
      "    sample_throughput: 264.064\n",
      "    sample_time_ms: 15147.847\n",
      "    update_time_ms: 2.1\n",
      "  timestamp: 1641214228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:30 (running for 00:07:13.43)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         424.419</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">  494.29</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 136</td><td style=\"text-align: right;\">            494.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-50-34\n",
      "  done: false\n",
      "  episode_len_mean: 499.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.09\n",
      "  episode_reward_min: 409.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 559\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.15527380129788e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3726036250591278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0030164276249706745\n",
      "          model: {}\n",
      "          policy_loss: 0.0016188398003578186\n",
      "          total_loss: 363.93804931640625\n",
      "          vf_explained_var: 0.2855881154537201\n",
      "          vf_loss: 363.93646240234375\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.955555555555556\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0796333275130944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08265892413336313\n",
      "    mean_inference_ms: 1.14707785259026\n",
      "    mean_raw_obs_processing_ms: 0.11103458462736569\n",
      "  time_since_restore: 430.7374367713928\n",
      "  time_this_iter_s: 6.318173170089722\n",
      "  time_total_s: 430.7374367713928\n",
      "  timers:\n",
      "    learn_throughput: 1162.619\n",
      "    learn_time_ms: 3440.508\n",
      "    load_throughput: 7618388.884\n",
      "    load_time_ms: 0.525\n",
      "    sample_throughput: 265.154\n",
      "    sample_time_ms: 15085.549\n",
      "    update_time_ms: 2.116\n",
      "  timestamp: 1641214234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:35 (running for 00:07:18.79)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         430.737</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  499.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 409</td><td style=\"text-align: right;\">            499.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:40 (running for 00:07:23.79)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         430.737</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  499.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 409</td><td style=\"text-align: right;\">            499.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:45 (running for 00:07:28.80)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         430.737</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  499.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 409</td><td style=\"text-align: right;\">            499.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:50 (running for 00:07:33.81)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         430.737</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  499.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 409</td><td style=\"text-align: right;\">            499.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:50:55 (running for 00:07:38.82)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         430.737</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">  499.09</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 409</td><td style=\"text-align: right;\">            499.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-50-58\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 567\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 497.6\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 497.6\n",
      "    episode_reward_min: 452.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 452\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 452.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09941306876026418\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10259009065571885\n",
      "      mean_inference_ms: 1.4029271923729152\n",
      "      mean_raw_obs_processing_ms: 0.12218759803706276\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.57763690064894e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.38642317056655884\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0028554166201502085\n",
      "          model: {}\n",
      "          policy_loss: 0.002737188246101141\n",
      "          total_loss: 469.2366638183594\n",
      "          vf_explained_var: -0.008716159500181675\n",
      "          vf_loss: 469.23394775390625\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.661764705882355\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07956828918094931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08258411462560852\n",
      "    mean_inference_ms: 1.1460408600055312\n",
      "    mean_raw_obs_processing_ms: 0.11087609551514761\n",
      "  time_since_restore: 454.4033613204956\n",
      "  time_this_iter_s: 23.665924549102783\n",
      "  time_total_s: 454.4033613204956\n",
      "  timers:\n",
      "    learn_throughput: 1163.44\n",
      "    learn_time_ms: 3438.08\n",
      "    load_throughput: 7601819.665\n",
      "    load_time_ms: 0.526\n",
      "    sample_throughput: 265.019\n",
      "    sample_time_ms: 15093.272\n",
      "    update_time_ms: 2.111\n",
      "  timestamp: 1641214258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:01 (running for 00:07:44.50)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         454.403</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-51-04\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 575\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.28881845032447e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3741539716720581\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0032545658759772778\n",
      "          model: {}\n",
      "          policy_loss: -0.0036664640065282583\n",
      "          total_loss: 258.59686279296875\n",
      "          vf_explained_var: 0.35744550824165344\n",
      "          vf_loss: 258.6005554199219\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.266666666666664\n",
      "    ram_util_percent: 11.9\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07953178572130858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0825386767939392\n",
      "    mean_inference_ms: 1.1453670318288705\n",
      "    mean_raw_obs_processing_ms: 0.11076034848128394\n",
      "  time_since_restore: 460.95565938949585\n",
      "  time_this_iter_s: 6.552298069000244\n",
      "  time_total_s: 460.95565938949585\n",
      "  timers:\n",
      "    learn_throughput: 1164.393\n",
      "    learn_time_ms: 3435.266\n",
      "    load_throughput: 8106110.064\n",
      "    load_time_ms: 0.493\n",
      "    sample_throughput: 264.587\n",
      "    sample_time_ms: 15117.895\n",
      "    update_time_ms: 2.114\n",
      "  timestamp: 1641214264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:07 (running for 00:07:50.10)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         460.956</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:12 (running for 00:07:55.10)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         460.956</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:17 (running for 00:08:00.11)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         460.956</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:22 (running for 00:08:05.12)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         460.956</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:27 (running for 00:08:10.13)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         460.956</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-51-28\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 583\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09939444434400664\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10261330896684975\n",
      "      mean_inference_ms: 1.4029501707320542\n",
      "      mean_raw_obs_processing_ms: 0.12211571331520385\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.144409225162235e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.34169572591781616\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0034873634576797485\n",
      "          model: {}\n",
      "          policy_loss: -0.0019346944754943252\n",
      "          total_loss: 361.1817321777344\n",
      "          vf_explained_var: 0.34877005219459534\n",
      "          vf_loss: 361.18365478515625\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.429411764705883\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0795083969860858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08250855917557369\n",
      "    mean_inference_ms: 1.1448883509574266\n",
      "    mean_raw_obs_processing_ms: 0.11066939700750612\n",
      "  time_since_restore: 484.88394808769226\n",
      "  time_this_iter_s: 23.92828869819641\n",
      "  time_total_s: 484.88394808769226\n",
      "  timers:\n",
      "    learn_throughput: 1164.621\n",
      "    learn_time_ms: 3434.594\n",
      "    load_throughput: 8269934.441\n",
      "    load_time_ms: 0.484\n",
      "    sample_throughput: 264.998\n",
      "    sample_time_ms: 15094.433\n",
      "    update_time_ms: 2.147\n",
      "  timestamp: 1641214288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:33 (running for 00:08:16.08)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         484.884</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-51-35\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 591\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.722046125811175e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3170822858810425\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0040565794333815575\n",
      "          model: {}\n",
      "          policy_loss: -0.0009620698401704431\n",
      "          total_loss: 325.4109802246094\n",
      "          vf_explained_var: 0.35483792424201965\n",
      "          vf_loss: 325.4119567871094\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.66\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07949190811164271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08248702672896545\n",
      "    mean_inference_ms: 1.1445445225731237\n",
      "    mean_raw_obs_processing_ms: 0.11059374707721133\n",
      "  time_since_restore: 491.37184953689575\n",
      "  time_this_iter_s: 6.487901449203491\n",
      "  time_total_s: 491.37184953689575\n",
      "  timers:\n",
      "    learn_throughput: 1164.387\n",
      "    learn_time_ms: 3435.284\n",
      "    load_throughput: 8152194.363\n",
      "    load_time_ms: 0.491\n",
      "    sample_throughput: 265.383\n",
      "    sample_time_ms: 15072.545\n",
      "    update_time_ms: 2.225\n",
      "  timestamp: 1641214295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:38 (running for 00:08:21.61)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         491.372</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:43 (running for 00:08:26.62)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         491.372</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:48 (running for 00:08:31.63)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         491.372</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:53 (running for 00:08:36.63)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         491.372</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:51:58 (running for 00:08:41.64)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         491.372</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-51-59\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 599\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09946977952560254\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1026684843078071\n",
      "      mean_inference_ms: 1.4036112275235868\n",
      "      mean_raw_obs_processing_ms: 0.12226953902445337\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.8610230629055877e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.34436705708503723\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0025885074865072966\n",
      "          model: {}\n",
      "          policy_loss: -0.003418699838221073\n",
      "          total_loss: 229.7168731689453\n",
      "          vf_explained_var: 0.23247107863426208\n",
      "          vf_loss: 229.7202606201172\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.452941176470588\n",
      "    ram_util_percent: 11.870588235294118\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07947543049787381\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08246425203070337\n",
      "    mean_inference_ms: 1.1441923956151887\n",
      "    mean_raw_obs_processing_ms: 0.11052214775048806\n",
      "  time_since_restore: 515.6011052131653\n",
      "  time_this_iter_s: 24.22925567626953\n",
      "  time_total_s: 515.6011052131653\n",
      "  timers:\n",
      "    learn_throughput: 1163.944\n",
      "    learn_time_ms: 3436.592\n",
      "    load_throughput: 7854869.61\n",
      "    load_time_ms: 0.509\n",
      "    sample_throughput: 264.876\n",
      "    sample_time_ms: 15101.413\n",
      "    update_time_ms: 2.297\n",
      "  timestamp: 1641214319\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:03 (running for 00:08:46.89)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         515.601</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-52-06\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 607\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4305115314527939e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3192770779132843\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0038329511880874634\n",
      "          model: {}\n",
      "          policy_loss: -0.0026558642275631428\n",
      "          total_loss: 372.1199951171875\n",
      "          vf_explained_var: 0.2135278582572937\n",
      "          vf_loss: 372.1226501464844\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.929999999999998\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0794510540005532\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08243323290971885\n",
      "    mean_inference_ms: 1.1437380434740003\n",
      "    mean_raw_obs_processing_ms: 0.11044263750401666\n",
      "  time_since_restore: 522.1010003089905\n",
      "  time_this_iter_s: 6.499895095825195\n",
      "  time_total_s: 522.1010003089905\n",
      "  timers:\n",
      "    learn_throughput: 1164.569\n",
      "    learn_time_ms: 3434.747\n",
      "    load_throughput: 8368940.989\n",
      "    load_time_ms: 0.478\n",
      "    sample_throughput: 262.928\n",
      "    sample_time_ms: 15213.262\n",
      "    update_time_ms: 2.288\n",
      "  timestamp: 1641214326\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:09 (running for 00:08:52.42)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         522.101</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:14 (running for 00:08:57.43)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         522.101</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:19 (running for 00:09:02.44)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         522.101</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:24 (running for 00:09:07.44)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         522.101</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:29 (running for 00:09:12.45)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         522.101</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-52-30\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 615\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09948361939194289\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10270222213565239\n",
      "      mean_inference_ms: 1.4040903515341654\n",
      "      mean_raw_obs_processing_ms: 0.12227653393190181\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.152557657263969e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.31491655111312866\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0033327066339552402\n",
      "          model: {}\n",
      "          policy_loss: -0.0007229884504340589\n",
      "          total_loss: 494.2162780761719\n",
      "          vf_explained_var: 0.005715495441108942\n",
      "          vf_loss: 494.2170104980469\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.305882352941177\n",
      "    ram_util_percent: 11.820588235294117\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07945064011080472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08242891952645134\n",
      "    mean_inference_ms: 1.1436184922174488\n",
      "    mean_raw_obs_processing_ms: 0.11039615979228397\n",
      "  time_since_restore: 546.2564222812653\n",
      "  time_this_iter_s: 24.15542197227478\n",
      "  time_total_s: 546.2564222812653\n",
      "  timers:\n",
      "    learn_throughput: 1164.203\n",
      "    learn_time_ms: 3435.827\n",
      "    load_throughput: 9085463.013\n",
      "    load_time_ms: 0.44\n",
      "    sample_throughput: 262.547\n",
      "    sample_time_ms: 15235.358\n",
      "    update_time_ms: 2.258\n",
      "  timestamp: 1641214350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:34 (running for 00:09:17.63)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         546.256</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-52-37\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 623\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.5762788286319847e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.33596229553222656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003063222859054804\n",
      "          model: {}\n",
      "          policy_loss: -0.0007319195428863168\n",
      "          total_loss: 502.2861328125\n",
      "          vf_explained_var: -0.01001955009996891\n",
      "          vf_loss: 502.2868347167969\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.933333333333334\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07946011831913216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0824354710751524\n",
      "    mean_inference_ms: 1.1436666500625277\n",
      "    mean_raw_obs_processing_ms: 0.11036115238870597\n",
      "  time_since_restore: 552.7793071269989\n",
      "  time_this_iter_s: 6.522884845733643\n",
      "  time_total_s: 552.7793071269989\n",
      "  timers:\n",
      "    learn_throughput: 1164.516\n",
      "    learn_time_ms: 3434.904\n",
      "    load_throughput: 9100247.342\n",
      "    load_time_ms: 0.44\n",
      "    sample_throughput: 262.011\n",
      "    sample_time_ms: 15266.558\n",
      "    update_time_ms: 2.291\n",
      "  timestamp: 1641214357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:40 (running for 00:09:23.19)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         552.779</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:45 (running for 00:09:28.20)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         552.779</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:50 (running for 00:09:33.21)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         552.779</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:52:55 (running for 00:09:38.21)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         552.779</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:00 (running for 00:09:43.23)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         552.779</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-53-00\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 631\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 490.95\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 490.95\n",
      "    episode_reward_min: 319.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 319\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 319.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09946971032991077\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10267935775545815\n",
      "      mean_inference_ms: 1.4039944099567374\n",
      "      mean_raw_obs_processing_ms: 0.12224188563527427\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7881394143159923e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3237580358982086\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0028110574930906296\n",
      "          model: {}\n",
      "          policy_loss: -0.00015313291805796325\n",
      "          total_loss: 514.155029296875\n",
      "          vf_explained_var: -0.09653975814580917\n",
      "          vf_loss: 514.1551513671875\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.614705882352938\n",
      "    ram_util_percent: 11.814705882352941\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07948881806111809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08246290306069122\n",
      "    mean_inference_ms: 1.1439807044381347\n",
      "    mean_raw_obs_processing_ms: 0.110351855341977\n",
      "  time_since_restore: 576.4864287376404\n",
      "  time_this_iter_s: 23.70712161064148\n",
      "  time_total_s: 576.4864287376404\n",
      "  timers:\n",
      "    learn_throughput: 1163.565\n",
      "    learn_time_ms: 3437.711\n",
      "    load_throughput: 8904631.389\n",
      "    load_time_ms: 0.449\n",
      "    sample_throughput: 261.773\n",
      "    sample_time_ms: 15280.425\n",
      "    update_time_ms: 2.35\n",
      "  timestamp: 1641214380\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:05 (running for 00:09:48.95)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         576.486</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-53-07\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 639\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.940697071579962e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.35207512974739075\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0028695587534457445\n",
      "          model: {}\n",
      "          policy_loss: -0.005121050402522087\n",
      "          total_loss: 273.1157531738281\n",
      "          vf_explained_var: 0.08673982322216034\n",
      "          vf_loss: 273.1208801269531\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.059999999999997\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07951985567033365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08249402599508113\n",
      "    mean_inference_ms: 1.1443259011003093\n",
      "    mean_raw_obs_processing_ms: 0.1103508887481165\n",
      "  time_since_restore: 582.979861497879\n",
      "  time_this_iter_s: 6.4934327602386475\n",
      "  time_total_s: 582.979861497879\n",
      "  timers:\n",
      "    learn_throughput: 1164.283\n",
      "    learn_time_ms: 3435.592\n",
      "    load_throughput: 9757599.162\n",
      "    load_time_ms: 0.41\n",
      "    sample_throughput: 261.938\n",
      "    sample_time_ms: 15270.811\n",
      "    update_time_ms: 2.348\n",
      "  timestamp: 1641214387\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:11 (running for 00:09:54.48)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">          582.98</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:16 (running for 00:09:59.49)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">          582.98</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:21 (running for 00:10:04.50)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">          582.98</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:26 (running for 00:10:09.51)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">          582.98</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-53-30\n",
      "  done: false\n",
      "  episode_len_mean: 500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 500.0\n",
      "  episode_reward_min: 500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 647\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 471.9\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 471.9\n",
      "    episode_reward_min: 242.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 254\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 242\n",
      "      - 473\n",
      "      - 500\n",
      "      - 500\n",
      "      - 469\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 254.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 242.0\n",
      "      - 473.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 469.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09965025751636167\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10281629507669261\n",
      "      mean_inference_ms: 1.4054618489181412\n",
      "      mean_raw_obs_processing_ms: 0.1223282085178143\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.470348535789981e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3788440525531769\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0017615531105548143\n",
      "          model: {}\n",
      "          policy_loss: -0.001946029718965292\n",
      "          total_loss: 427.7779846191406\n",
      "          vf_explained_var: -0.09892924875020981\n",
      "          vf_loss: 427.7799377441406\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.457575757575759\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07955387161304833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08252783961940695\n",
      "    mean_inference_ms: 1.1447204782187546\n",
      "    mean_raw_obs_processing_ms: 0.11035193494428737\n",
      "  time_since_restore: 606.4087183475494\n",
      "  time_this_iter_s: 23.42885684967041\n",
      "  time_total_s: 606.4087183475494\n",
      "  timers:\n",
      "    learn_throughput: 1163.901\n",
      "    learn_time_ms: 3436.719\n",
      "    load_throughput: 10648144.199\n",
      "    load_time_ms: 0.376\n",
      "    sample_throughput: 261.79\n",
      "    sample_time_ms: 15279.408\n",
      "    update_time_ms: 2.354\n",
      "  timestamp: 1641214410\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:31 (running for 00:10:14.94)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         606.409</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:36 (running for 00:10:19.95)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         606.409</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">     500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">               500</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-53-37\n",
      "  done: false\n",
      "  episode_len_mean: 498.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 498.92\n",
      "  episode_reward_min: 392.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 656\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2351742678949904e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3735828697681427\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004065442830324173\n",
      "          model: {}\n",
      "          policy_loss: -0.0010430102702230215\n",
      "          total_loss: 464.3570556640625\n",
      "          vf_explained_var: 0.017288917675614357\n",
      "          vf_loss: 464.3581237792969\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.839999999999998\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07961279127955698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08259065100470758\n",
      "    mean_inference_ms: 1.145475742766385\n",
      "    mean_raw_obs_processing_ms: 0.11038388157426014\n",
      "  time_since_restore: 613.0383384227753\n",
      "  time_this_iter_s: 6.62962007522583\n",
      "  time_total_s: 613.0383384227753\n",
      "  timers:\n",
      "    learn_throughput: 1163.255\n",
      "    learn_time_ms: 3438.628\n",
      "    load_throughput: 10523909.171\n",
      "    load_time_ms: 0.38\n",
      "    sample_throughput: 262.294\n",
      "    sample_time_ms: 15250.054\n",
      "    update_time_ms: 2.286\n",
      "  timestamp: 1641214417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:42 (running for 00:10:25.61)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         613.038</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  498.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 392</td><td style=\"text-align: right;\">            498.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:47 (running for 00:10:30.62)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         613.038</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  498.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 392</td><td style=\"text-align: right;\">            498.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:52 (running for 00:10:35.63)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         613.038</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  498.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 392</td><td style=\"text-align: right;\">            498.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:53:57 (running for 00:10:40.64)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         613.038</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  498.92</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 392</td><td style=\"text-align: right;\">            498.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-54-01\n",
      "  done: false\n",
      "  episode_len_mean: 496.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.63\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 664\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 494.9\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 494.9\n",
      "    episode_reward_min: 399.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 399\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 499\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 399.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 499.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09971305891514728\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10286078802189898\n",
      "      mean_inference_ms: 1.4063051596587663\n",
      "      mean_raw_obs_processing_ms: 0.12235596499347381\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1175871339474952e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.37061235308647156\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0032300094608217478\n",
      "          model: {}\n",
      "          policy_loss: -0.006143657024949789\n",
      "          total_loss: 378.09521484375\n",
      "          vf_explained_var: 0.14620283246040344\n",
      "          vf_loss: 378.10137939453125\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.344117647058823\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0796581589068329\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08263862276650341\n",
      "    mean_inference_ms: 1.1460402830098035\n",
      "    mean_raw_obs_processing_ms: 0.11040482993997289\n",
      "  time_since_restore: 636.8817765712738\n",
      "  time_this_iter_s: 23.843438148498535\n",
      "  time_total_s: 636.8817765712738\n",
      "  timers:\n",
      "    learn_throughput: 1164.196\n",
      "    learn_time_ms: 3435.848\n",
      "    load_throughput: 10515993.481\n",
      "    load_time_ms: 0.38\n",
      "    sample_throughput: 262.459\n",
      "    sample_time_ms: 15240.49\n",
      "    update_time_ms: 2.251\n",
      "  timestamp: 1641214441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:03 (running for 00:10:46.50)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         636.882</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-54-07\n",
      "  done: false\n",
      "  episode_len_mean: 496.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.63\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 672\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.587935669737476e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3455185294151306\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0016453824937343597\n",
      "          model: {}\n",
      "          policy_loss: -0.001856086659245193\n",
      "          total_loss: 263.1059265136719\n",
      "          vf_explained_var: 0.15679922699928284\n",
      "          vf_loss: 263.1077880859375\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.322222222222223\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07969534660773528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08267752127570777\n",
      "    mean_inference_ms: 1.1465005841811662\n",
      "    mean_raw_obs_processing_ms: 0.1104197732897764\n",
      "  time_since_restore: 643.3969051837921\n",
      "  time_this_iter_s: 6.5151286125183105\n",
      "  time_total_s: 643.3969051837921\n",
      "  timers:\n",
      "    learn_throughput: 1164.311\n",
      "    learn_time_ms: 3435.509\n",
      "    load_throughput: 9742866.434\n",
      "    load_time_ms: 0.411\n",
      "    sample_throughput: 262.375\n",
      "    sample_time_ms: 15245.338\n",
      "    update_time_ms: 2.247\n",
      "  timestamp: 1641214447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:09 (running for 00:10:52.06)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         643.397</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:14 (running for 00:10:57.07)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         643.397</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:19 (running for 00:11:02.08)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         643.397</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:24 (running for 00:11:07.09)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         643.397</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:29 (running for 00:11:12.10)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         643.397</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-54-32\n",
      "  done: false\n",
      "  episode_len_mean: 496.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.63\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 680\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09976714310559924\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10284341906678422\n",
      "      mean_inference_ms: 1.4063103343901902\n",
      "      mean_raw_obs_processing_ms: 0.12231234045793518\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.793967834868738e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.33968988060951233\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0019635462667793036\n",
      "          model: {}\n",
      "          policy_loss: -0.0013499544002115726\n",
      "          total_loss: 367.69903564453125\n",
      "          vf_explained_var: 0.03755499795079231\n",
      "          vf_loss: 367.70037841796875\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.461764705882352\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07973023632468008\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08271456330389965\n",
      "    mean_inference_ms: 1.1469383131402473\n",
      "    mean_raw_obs_processing_ms: 0.1104306162093986\n",
      "  time_since_restore: 667.5081536769867\n",
      "  time_this_iter_s: 24.11124849319458\n",
      "  time_total_s: 667.5081536769867\n",
      "  timers:\n",
      "    learn_throughput: 1164.357\n",
      "    learn_time_ms: 3435.372\n",
      "    load_throughput: 9548241.99\n",
      "    load_time_ms: 0.419\n",
      "    sample_throughput: 262.385\n",
      "    sample_time_ms: 15244.755\n",
      "    update_time_ms: 2.249\n",
      "  timestamp: 1641214472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:34 (running for 00:11:17.22)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         667.508</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-54-38\n",
      "  done: false\n",
      "  episode_len_mean: 496.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.63\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 688\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.396983917434369e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.34701216220855713\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0024114095140248537\n",
      "          model: {}\n",
      "          policy_loss: 0.00015137766604311764\n",
      "          total_loss: 370.8667297363281\n",
      "          vf_explained_var: -0.011806507594883442\n",
      "          vf_loss: 370.8665771484375\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.669999999999998\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07977179852190504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0827573745106352\n",
      "    mean_inference_ms: 1.1474494306306835\n",
      "    mean_raw_obs_processing_ms: 0.11044989737693695\n",
      "  time_since_restore: 674.0614902973175\n",
      "  time_this_iter_s: 6.5533366203308105\n",
      "  time_total_s: 674.0614902973175\n",
      "  timers:\n",
      "    learn_throughput: 1163.344\n",
      "    learn_time_ms: 3438.363\n",
      "    load_throughput: 8897075.887\n",
      "    load_time_ms: 0.45\n",
      "    sample_throughput: 262.549\n",
      "    sample_time_ms: 15235.259\n",
      "    update_time_ms: 2.324\n",
      "  timestamp: 1641214478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:39 (running for 00:11:22.81)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         674.061</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:44 (running for 00:11:27.82)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         674.061</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:49 (running for 00:11:32.83)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         674.061</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:54 (running for 00:11:37.83)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         674.061</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:54:59 (running for 00:11:42.84)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         674.061</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">  496.63</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-55-02\n",
      "  done: false\n",
      "  episode_len_mean: 496.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.48\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 696\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 496.55\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 496.55\n",
      "    episode_reward_min: 431.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 431\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 431.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09983354106727835\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10290856197559965\n",
      "      mean_inference_ms: 1.4073860516169066\n",
      "      mean_raw_obs_processing_ms: 0.12239292282595482\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.984919587171845e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.33118343353271484\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0025007727090269327\n",
      "          model: {}\n",
      "          policy_loss: -0.0012579438043758273\n",
      "          total_loss: 270.6835021972656\n",
      "          vf_explained_var: 0.38449805974960327\n",
      "          vf_loss: 270.68475341796875\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.43529411764706\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07980871882638868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08279560733589987\n",
      "    mean_inference_ms: 1.1478979040697703\n",
      "    mean_raw_obs_processing_ms: 0.11046145646450796\n",
      "  time_since_restore: 698.238331079483\n",
      "  time_this_iter_s: 24.176840782165527\n",
      "  time_total_s: 698.238331079483\n",
      "  timers:\n",
      "    learn_throughput: 1163.772\n",
      "    learn_time_ms: 3437.1\n",
      "    load_throughput: 8284226.743\n",
      "    load_time_ms: 0.483\n",
      "    sample_throughput: 262.59\n",
      "    sample_time_ms: 15232.863\n",
      "    update_time_ms: 2.307\n",
      "  timestamp: 1641214502\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:04 (running for 00:11:48.04)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         698.238</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-55-09\n",
      "  done: false\n",
      "  episode_len_mean: 496.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.48\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 704\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4924597935859225e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.28589412569999695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0026714280247688293\n",
      "          model: {}\n",
      "          policy_loss: -0.001123060705140233\n",
      "          total_loss: 340.09228515625\n",
      "          vf_explained_var: 0.23291447758674622\n",
      "          vf_loss: 340.0933532714844\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.979999999999999\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07984557957922908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08283436602376197\n",
      "    mean_inference_ms: 1.148349744115344\n",
      "    mean_raw_obs_processing_ms: 0.11047810317648876\n",
      "  time_since_restore: 704.7191970348358\n",
      "  time_this_iter_s: 6.480865955352783\n",
      "  time_total_s: 704.7191970348358\n",
      "  timers:\n",
      "    learn_throughput: 1164.736\n",
      "    learn_time_ms: 3434.256\n",
      "    load_throughput: 8351859.817\n",
      "    load_time_ms: 0.479\n",
      "    sample_throughput: 262.472\n",
      "    sample_time_ms: 15239.705\n",
      "    update_time_ms: 2.312\n",
      "  timestamp: 1641214509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:10 (running for 00:11:53.56)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         704.719</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:15 (running for 00:11:58.56)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         704.719</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:20 (running for 00:12:03.57)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         704.719</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:25 (running for 00:12:08.58)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         704.719</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:30 (running for 00:12:13.59)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         704.719</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-55-33\n",
      "  done: false\n",
      "  episode_len_mean: 496.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.48\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 712\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09973444154478656\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10281132505136674\n",
      "      mean_inference_ms: 1.4064705656434335\n",
      "      mean_raw_obs_processing_ms: 0.12226821385175272\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7462298967929613e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.28462663292884827\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0030475114472210407\n",
      "          model: {}\n",
      "          policy_loss: -0.0005468263407237828\n",
      "          total_loss: 327.8911437988281\n",
      "          vf_explained_var: 0.3008342683315277\n",
      "          vf_loss: 327.8916931152344\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.266666666666666\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07987973208792945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08287053758138593\n",
      "    mean_inference_ms: 1.1487766856984327\n",
      "    mean_raw_obs_processing_ms: 0.11049268285016811\n",
      "  time_since_restore: 728.3583090305328\n",
      "  time_this_iter_s: 23.63911199569702\n",
      "  time_total_s: 728.3583090305328\n",
      "  timers:\n",
      "    learn_throughput: 1164.687\n",
      "    learn_time_ms: 3434.4\n",
      "    load_throughput: 8688356.292\n",
      "    load_time_ms: 0.46\n",
      "    sample_throughput: 262.812\n",
      "    sample_time_ms: 15220.034\n",
      "    update_time_ms: 2.305\n",
      "  timestamp: 1641214533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:36 (running for 00:12:19.25)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         728.358</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-55-39\n",
      "  done: false\n",
      "  episode_len_mean: 496.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.48\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 720\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.731149483964806e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2651657164096832\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003235872834920883\n",
      "          model: {}\n",
      "          policy_loss: -0.00046385437599383295\n",
      "          total_loss: 410.3289794921875\n",
      "          vf_explained_var: 0.14666806161403656\n",
      "          vf_loss: 410.32940673828125\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.0\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.079912490231387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08290536445603257\n",
      "    mean_inference_ms: 1.1491775600808136\n",
      "    mean_raw_obs_processing_ms: 0.11050723811283886\n",
      "  time_since_restore: 734.9852323532104\n",
      "  time_this_iter_s: 6.626923322677612\n",
      "  time_total_s: 734.9852323532104\n",
      "  timers:\n",
      "    learn_throughput: 1163.672\n",
      "    learn_time_ms: 3437.396\n",
      "    load_throughput: 8540196.488\n",
      "    load_time_ms: 0.468\n",
      "    sample_throughput: 262.455\n",
      "    sample_time_ms: 15240.733\n",
      "    update_time_ms: 2.374\n",
      "  timestamp: 1641214539\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:41 (running for 00:12:24.92)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         734.985</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:46 (running for 00:12:29.93)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         734.985</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:51 (running for 00:12:34.94)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         734.985</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:55:56 (running for 00:12:39.94)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         734.985</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:01 (running for 00:12:44.95)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         734.985</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">  496.48</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-56-04\n",
      "  done: false\n",
      "  episode_len_mean: 496.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.27\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 728\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09976090878042859\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10285603627104975\n",
      "      mean_inference_ms: 1.4069825283660182\n",
      "      mean_raw_obs_processing_ms: 0.12229781694092796\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.365574741982403e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.32048898935317993\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004733977373689413\n",
      "          model: {}\n",
      "          policy_loss: -0.008360615000128746\n",
      "          total_loss: 257.9974670410156\n",
      "          vf_explained_var: 0.47615835070610046\n",
      "          vf_loss: 258.005859375\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.488235294117647\n",
      "    ram_util_percent: 11.805882352941175\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07994238130834193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08293769930998028\n",
      "    mean_inference_ms: 1.1495395952376788\n",
      "    mean_raw_obs_processing_ms: 0.11052202672905281\n",
      "  time_since_restore: 759.1922824382782\n",
      "  time_this_iter_s: 24.20705008506775\n",
      "  time_total_s: 759.1922824382782\n",
      "  timers:\n",
      "    learn_throughput: 1162.878\n",
      "    learn_time_ms: 3439.741\n",
      "    load_throughput: 7971688.682\n",
      "    load_time_ms: 0.502\n",
      "    sample_throughput: 262.464\n",
      "    sample_time_ms: 15240.186\n",
      "    update_time_ms: 2.448\n",
      "  timestamp: 1641214564\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:07 (running for 00:12:50.18)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         759.192</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-56-10\n",
      "  done: false\n",
      "  episode_len_mean: 496.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.27\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 736\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1827873709912016e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.28173762559890747\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0032018960919231176\n",
      "          model: {}\n",
      "          policy_loss: -0.0011629423825070262\n",
      "          total_loss: 304.3319091796875\n",
      "          vf_explained_var: 0.37079381942749023\n",
      "          vf_loss: 304.33306884765625\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.2\n",
      "    ram_util_percent: 11.85\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0799674059460663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08296538792707649\n",
      "    mean_inference_ms: 1.1498512246663766\n",
      "    mean_raw_obs_processing_ms: 0.11052903807749895\n",
      "  time_since_restore: 765.6568610668182\n",
      "  time_this_iter_s: 6.464578628540039\n",
      "  time_total_s: 765.6568610668182\n",
      "  timers:\n",
      "    learn_throughput: 1162.294\n",
      "    learn_time_ms: 3441.469\n",
      "    load_throughput: 7554582.133\n",
      "    load_time_ms: 0.529\n",
      "    sample_throughput: 261.357\n",
      "    sample_time_ms: 15304.765\n",
      "    update_time_ms: 2.504\n",
      "  timestamp: 1641214570\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:12 (running for 00:12:55.68)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         765.657</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:17 (running for 00:13:00.69)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         765.657</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:22 (running for 00:13:05.70)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         765.657</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:27 (running for 00:13:10.71)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         765.657</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:32 (running for 00:13:15.72)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         765.657</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-56-34\n",
      "  done: false\n",
      "  episode_len_mean: 496.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.27\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 744\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09978637612975969\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10289952802479876\n",
      "      mean_inference_ms: 1.4080818673795648\n",
      "      mean_raw_obs_processing_ms: 0.12232249671802926\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0913936854956008e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3075120747089386\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003919175360351801\n",
      "          model: {}\n",
      "          policy_loss: -0.002855784958228469\n",
      "          total_loss: 225.8036346435547\n",
      "          vf_explained_var: 0.4734848141670227\n",
      "          vf_loss: 225.80648803710938\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.482352941176472\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0799951621512265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08299653071822777\n",
      "    mean_inference_ms: 1.1502019761011724\n",
      "    mean_raw_obs_processing_ms: 0.11054397667879921\n",
      "  time_since_restore: 789.9095482826233\n",
      "  time_this_iter_s: 24.252687215805054\n",
      "  time_total_s: 789.9095482826233\n",
      "  timers:\n",
      "    learn_throughput: 1162.254\n",
      "    learn_time_ms: 3441.589\n",
      "    load_throughput: 7461182.958\n",
      "    load_time_ms: 0.536\n",
      "    sample_throughput: 261.134\n",
      "    sample_time_ms: 15317.803\n",
      "    update_time_ms: 2.52\n",
      "  timestamp: 1641214594\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:37 (running for 00:13:20.97)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">          789.91</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-56-41\n",
      "  done: false\n",
      "  episode_len_mean: 496.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 496.27\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 752\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.456968427478004e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3077785074710846\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.001683449256233871\n",
      "          model: {}\n",
      "          policy_loss: -0.00023939917446114123\n",
      "          total_loss: 297.7902526855469\n",
      "          vf_explained_var: 0.4101290702819824\n",
      "          vf_loss: 297.79052734375\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.511111111111111\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08000520479022466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08300801537829423\n",
      "    mean_inference_ms: 1.1503124822190636\n",
      "    mean_raw_obs_processing_ms: 0.1105389686971247\n",
      "  time_since_restore: 796.3458256721497\n",
      "  time_this_iter_s: 6.436277389526367\n",
      "  time_total_s: 796.3458256721497\n",
      "  timers:\n",
      "    learn_throughput: 1162.023\n",
      "    learn_time_ms: 3442.273\n",
      "    load_throughput: 7511289.398\n",
      "    load_time_ms: 0.533\n",
      "    sample_throughput: 260.784\n",
      "    sample_time_ms: 15338.367\n",
      "    update_time_ms: 2.548\n",
      "  timestamp: 1641214601\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:43 (running for 00:13:26.45)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         796.346</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:48 (running for 00:13:31.46)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         796.346</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:53 (running for 00:13:36.47)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         796.346</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:56:58 (running for 00:13:41.47)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         796.346</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:03 (running for 00:13:46.48)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         796.346</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">  496.27</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            496.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-57-04\n",
      "  done: false\n",
      "  episode_len_mean: 497.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.99\n",
      "  episode_reward_min: 376.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 760\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 476.3\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 476.3\n",
      "    episode_reward_min: 352.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 352\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 359\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 353\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 462\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 352.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 359.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 353.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 462.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09984009959303886\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10294986878947832\n",
      "      mean_inference_ms: 1.4088995640454605\n",
      "      mean_raw_obs_processing_ms: 0.12238530637763405\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.728484213739002e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2721678912639618\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006919963285326958\n",
      "          model: {}\n",
      "          policy_loss: -0.004764837678521872\n",
      "          total_loss: 341.0838623046875\n",
      "          vf_explained_var: 0.24915827810764313\n",
      "          vf_loss: 341.0886535644531\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.638235294117646\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08002320923740176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08302857086602768\n",
      "    mean_inference_ms: 1.150534127225943\n",
      "    mean_raw_obs_processing_ms: 0.11054501727885163\n",
      "  time_since_restore: 819.8854575157166\n",
      "  time_this_iter_s: 23.539631843566895\n",
      "  time_total_s: 819.8854575157166\n",
      "  timers:\n",
      "    learn_throughput: 1162.541\n",
      "    learn_time_ms: 3440.739\n",
      "    load_throughput: 7659779.939\n",
      "    load_time_ms: 0.522\n",
      "    sample_throughput: 260.771\n",
      "    sample_time_ms: 15339.1\n",
      "    update_time_ms: 2.483\n",
      "  timestamp: 1641214624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:08 (running for 00:13:52.05)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         819.885</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">  497.99</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 376</td><td style=\"text-align: right;\">            497.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-57-11\n",
      "  done: false\n",
      "  episode_len_mean: 498.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 498.75\n",
      "  episode_reward_min: 459.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 768\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.728484213739002e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.27726444602012634\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002490389160811901\n",
      "          model: {}\n",
      "          policy_loss: -0.0012705748667940497\n",
      "          total_loss: 212.70944213867188\n",
      "          vf_explained_var: 0.4870787262916565\n",
      "          vf_loss: 212.71072387695312\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.066666666666666\n",
      "    ram_util_percent: 11.877777777777778\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08003794685929408\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0830455903386086\n",
      "    mean_inference_ms: 1.1507252604753953\n",
      "    mean_raw_obs_processing_ms: 0.11054702976800378\n",
      "  time_since_restore: 826.3715515136719\n",
      "  time_this_iter_s: 6.486093997955322\n",
      "  time_total_s: 826.3715515136719\n",
      "  timers:\n",
      "    learn_throughput: 1162.56\n",
      "    learn_time_ms: 3440.684\n",
      "    load_throughput: 7645468.465\n",
      "    load_time_ms: 0.523\n",
      "    sample_throughput: 261.845\n",
      "    sample_time_ms: 15276.206\n",
      "    update_time_ms: 2.422\n",
      "  timestamp: 1641214631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:14 (running for 00:13:57.58)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         826.372</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  498.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 459</td><td style=\"text-align: right;\">            498.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:19 (running for 00:14:02.58)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         826.372</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  498.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 459</td><td style=\"text-align: right;\">            498.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:24 (running for 00:14:07.59)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         826.372</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  498.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 459</td><td style=\"text-align: right;\">            498.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:29 (running for 00:14:12.60)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         826.372</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  498.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 459</td><td style=\"text-align: right;\">            498.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:34 (running for 00:14:17.61)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         826.372</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">  498.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 459</td><td style=\"text-align: right;\">            498.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-57-35\n",
      "  done: false\n",
      "  episode_len_mean: 497.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.89\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 776\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 496.05\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 496.05\n",
      "    episode_reward_min: 447.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 447\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 484\n",
      "      - 490\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 447.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 484.0\n",
      "      - 490.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.0997692268760024\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10283119178938802\n",
      "      mean_inference_ms: 1.4076848126861221\n",
      "      mean_raw_obs_processing_ms: 0.12223860666725266\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.364242106869501e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2788257300853729\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002548348158597946\n",
      "          model: {}\n",
      "          policy_loss: -0.001480271341279149\n",
      "          total_loss: 185.6182098388672\n",
      "          vf_explained_var: 0.5972358584403992\n",
      "          vf_loss: 185.61968994140625\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.591176470588236\n",
      "    ram_util_percent: 11.80294117647059\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08004803677071749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08305722379350482\n",
      "    mean_inference_ms: 1.150849956281451\n",
      "    mean_raw_obs_processing_ms: 0.11054440264789316\n",
      "  time_since_restore: 849.8774726390839\n",
      "  time_this_iter_s: 23.505921125411987\n",
      "  time_total_s: 849.8774726390839\n",
      "  timers:\n",
      "    learn_throughput: 1162.076\n",
      "    learn_time_ms: 3442.114\n",
      "    load_throughput: 7623235.187\n",
      "    load_time_ms: 0.525\n",
      "    sample_throughput: 261.842\n",
      "    sample_time_ms: 15276.417\n",
      "    update_time_ms: 2.447\n",
      "  timestamp: 1641214655\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:40 (running for 00:14:23.12)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         849.877</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">  497.89</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-57-41\n",
      "  done: false\n",
      "  episode_len_mean: 497.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.62\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 784\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.821210534347505e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2768877446651459\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002392576541751623\n",
      "          model: {}\n",
      "          policy_loss: -0.0008955386583693326\n",
      "          total_loss: 288.79681396484375\n",
      "          vf_explained_var: 0.2627524137496948\n",
      "          vf_loss: 288.7977294921875\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.56666666666667\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.080058435280725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08306925872963983\n",
      "    mean_inference_ms: 1.1509798152026431\n",
      "    mean_raw_obs_processing_ms: 0.11054508493370786\n",
      "  time_since_restore: 856.478374004364\n",
      "  time_this_iter_s: 6.600901365280151\n",
      "  time_total_s: 856.478374004364\n",
      "  timers:\n",
      "    learn_throughput: 1160.273\n",
      "    learn_time_ms: 3447.465\n",
      "    load_throughput: 7579496.725\n",
      "    load_time_ms: 0.528\n",
      "    sample_throughput: 262.905\n",
      "    sample_time_ms: 15214.624\n",
      "    update_time_ms: 2.455\n",
      "  timestamp: 1641214661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:45 (running for 00:14:28.76)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         856.478</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  497.62</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:50 (running for 00:14:33.77)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         856.478</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  497.62</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:57:55 (running for 00:14:38.78)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         856.478</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  497.62</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:00 (running for 00:14:43.78)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         856.478</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  497.62</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-58-05\n",
      "  done: false\n",
      "  episode_len_mean: 497.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.77\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 792\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09976934566782643\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10276283948817229\n",
      "      mean_inference_ms: 1.407192663059987\n",
      "      mean_raw_obs_processing_ms: 0.12216108791504979\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4106052671737525e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.29833635687828064\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005802231375128031\n",
      "          model: {}\n",
      "          policy_loss: -0.0041998811066150665\n",
      "          total_loss: 221.92808532714844\n",
      "          vf_explained_var: 0.399289608001709\n",
      "          vf_loss: 221.93231201171875\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.38823529411765\n",
      "    ram_util_percent: 11.8\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08006413229682677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08307639196592119\n",
      "    mean_inference_ms: 1.151053500467093\n",
      "    mean_raw_obs_processing_ms: 0.11053999618092711\n",
      "  time_since_restore: 880.211318731308\n",
      "  time_this_iter_s: 23.73294472694397\n",
      "  time_total_s: 880.211318731308\n",
      "  timers:\n",
      "    learn_throughput: 1160.431\n",
      "    learn_time_ms: 3446.997\n",
      "    load_throughput: 7464834.705\n",
      "    load_time_ms: 0.536\n",
      "    sample_throughput: 262.815\n",
      "    sample_time_ms: 15219.837\n",
      "    update_time_ms: 2.434\n",
      "  timestamp: 1641214685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:06 (running for 00:14:49.53)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         880.211</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:11 (running for 00:14:54.54)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         880.211</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-58-11\n",
      "  done: false\n",
      "  episode_len_mean: 497.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.77\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 800\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4106052671737525e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.27059662342071533\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037121737841516733\n",
      "          model: {}\n",
      "          policy_loss: -0.0013116763439029455\n",
      "          total_loss: 359.6751708984375\n",
      "          vf_explained_var: 0.24808254837989807\n",
      "          vf_loss: 359.6764831542969\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.01\n",
      "    ram_util_percent: 11.639999999999997\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08006899929365378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08308271113012815\n",
      "    mean_inference_ms: 1.1511144327498926\n",
      "    mean_raw_obs_processing_ms: 0.11053576293399457\n",
      "  time_since_restore: 886.7374684810638\n",
      "  time_this_iter_s: 6.526149749755859\n",
      "  time_total_s: 886.7374684810638\n",
      "  timers:\n",
      "    learn_throughput: 1161.623\n",
      "    learn_time_ms: 3443.458\n",
      "    load_throughput: 7295393.312\n",
      "    load_time_ms: 0.548\n",
      "    sample_throughput: 262.784\n",
      "    sample_time_ms: 15221.654\n",
      "    update_time_ms: 2.441\n",
      "  timestamp: 1641214691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:17 (running for 00:15:00.11)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         886.737</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:22 (running for 00:15:05.11)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         886.737</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:27 (running for 00:15:10.12)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         886.737</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:32 (running for 00:15:15.13)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         886.737</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-58-36\n",
      "  done: false\n",
      "  episode_len_mean: 497.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.77\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 808\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 498.6\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 498.6\n",
      "    episode_reward_min: 472.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 472\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 472.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09979632241253603\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10279804331082583\n",
      "      mean_inference_ms: 1.4078386921465798\n",
      "      mean_raw_obs_processing_ms: 0.12217542268668727\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7053026335868762e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.29947254061698914\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0016222784761339426\n",
      "          model: {}\n",
      "          policy_loss: -3.099422247032635e-05\n",
      "          total_loss: 379.5009460449219\n",
      "          vf_explained_var: 0.20277585089206696\n",
      "          vf_loss: 379.5009460449219\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.311764705882354\n",
      "    ram_util_percent: 11.600000000000001\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08007952608832712\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08309573476078519\n",
      "    mean_inference_ms: 1.1512581942874394\n",
      "    mean_raw_obs_processing_ms: 0.11053790665641436\n",
      "  time_since_restore: 911.0552768707275\n",
      "  time_this_iter_s: 24.317808389663696\n",
      "  time_total_s: 911.0552768707275\n",
      "  timers:\n",
      "    learn_throughput: 1160.381\n",
      "    learn_time_ms: 3447.144\n",
      "    load_throughput: 7535241.859\n",
      "    load_time_ms: 0.531\n",
      "    sample_throughput: 262.739\n",
      "    sample_time_ms: 15224.239\n",
      "    update_time_ms: 2.369\n",
      "  timestamp: 1641214716\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:37 (running for 00:15:20.46)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         911.055</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:42 (running for 00:15:25.47)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         911.055</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-58-42\n",
      "  done: false\n",
      "  episode_len_mean: 497.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.77\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 816\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.526513167934381e-15\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.29696959257125854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003980488050729036\n",
      "          model: {}\n",
      "          policy_loss: -0.002883958863094449\n",
      "          total_loss: 318.4349060058594\n",
      "          vf_explained_var: 0.21819797158241272\n",
      "          vf_loss: 318.43780517578125\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.059999999999999\n",
      "    ram_util_percent: 11.599999999999998\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08008427241833951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08310188992836416\n",
      "    mean_inference_ms: 1.1513195327906394\n",
      "    mean_raw_obs_processing_ms: 0.11053224961197167\n",
      "  time_since_restore: 917.5175948143005\n",
      "  time_this_iter_s: 6.462317943572998\n",
      "  time_total_s: 917.5175948143005\n",
      "  timers:\n",
      "    learn_throughput: 1161.434\n",
      "    learn_time_ms: 3444.019\n",
      "    load_throughput: 8057833.918\n",
      "    load_time_ms: 0.496\n",
      "    sample_throughput: 262.606\n",
      "    sample_time_ms: 15231.927\n",
      "    update_time_ms: 2.317\n",
      "  timestamp: 1641214722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:47 (running for 00:15:30.97)<br>Memory usage on this node: 7.2/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         917.518</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:52 (running for 00:15:35.98)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         917.518</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:58:57 (running for 00:15:40.98)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         917.518</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:02 (running for 00:15:45.99)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         917.518</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-59-06\n",
      "  done: false\n",
      "  episode_len_mean: 497.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.77\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 824\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09981694013766426\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10280622675018387\n",
      "      mean_inference_ms: 1.4079228930650882\n",
      "      mean_raw_obs_processing_ms: 0.12216947691168205\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.2632565839671906e-15\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2887202203273773\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00418429309502244\n",
      "          model: {}\n",
      "          policy_loss: -0.004043938592076302\n",
      "          total_loss: 210.58938598632812\n",
      "          vf_explained_var: 0.32004860043525696\n",
      "          vf_loss: 210.59341430664062\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.941176470588236\n",
      "    ram_util_percent: 11.600000000000001\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08009064129429111\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08311008750641999\n",
      "    mean_inference_ms: 1.1514064395225148\n",
      "    mean_raw_obs_processing_ms: 0.11052659452047892\n",
      "  time_since_restore: 941.5922567844391\n",
      "  time_this_iter_s: 24.07466197013855\n",
      "  time_total_s: 941.5922567844391\n",
      "  timers:\n",
      "    learn_throughput: 1160.555\n",
      "    learn_time_ms: 3446.626\n",
      "    load_throughput: 8614302.732\n",
      "    load_time_ms: 0.464\n",
      "    sample_throughput: 262.561\n",
      "    sample_time_ms: 15234.572\n",
      "    update_time_ms: 2.339\n",
      "  timestamp: 1641214746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:08 (running for 00:15:51.09)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         941.592</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:13 (running for 00:15:56.10)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         941.592</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">  497.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-59-13\n",
      "  done: false\n",
      "  episode_len_mean: 497.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.98\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 832\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1316282919835953e-15\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3001687526702881\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005532457958906889\n",
      "          model: {}\n",
      "          policy_loss: -0.0025179916992783546\n",
      "          total_loss: 415.4772644042969\n",
      "          vf_explained_var: 0.09152212738990784\n",
      "          vf_loss: 415.4797668457031\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.144444444444444\n",
      "    ram_util_percent: 11.6\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08009958874335484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08312176771071565\n",
      "    mean_inference_ms: 1.1515184400249967\n",
      "    mean_raw_obs_processing_ms: 0.11052816011746053\n",
      "  time_since_restore: 948.113970041275\n",
      "  time_this_iter_s: 6.5217132568359375\n",
      "  time_total_s: 948.113970041275\n",
      "  timers:\n",
      "    learn_throughput: 1160.149\n",
      "    learn_time_ms: 3447.833\n",
      "    load_throughput: 8535417.175\n",
      "    load_time_ms: 0.469\n",
      "    sample_throughput: 262.826\n",
      "    sample_time_ms: 15219.175\n",
      "    update_time_ms: 2.259\n",
      "  timestamp: 1641214753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:18 (running for 00:16:01.65)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         948.114</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:23 (running for 00:16:06.66)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         948.114</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:28 (running for 00:16:11.67)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         948.114</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:33 (running for 00:16:16.68)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         948.114</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-59-37\n",
      "  done: false\n",
      "  episode_len_mean: 497.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.98\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 840\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 494.65\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 494.65\n",
      "    episode_reward_min: 399.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 494\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 399\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 494.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 399.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.09989183278780296\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10291821737926685\n",
      "      mean_inference_ms: 1.4092371092524651\n",
      "      mean_raw_obs_processing_ms: 0.12222442483607858\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1316282919835953e-15\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3224461078643799\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003274397226050496\n",
      "          model: {}\n",
      "          policy_loss: -0.002108244923874736\n",
      "          total_loss: 322.9053039550781\n",
      "          vf_explained_var: 0.12203709781169891\n",
      "          vf_loss: 322.9073791503906\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.665714285714287\n",
      "    ram_util_percent: 11.600000000000001\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08010812866505933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08313288745019998\n",
      "    mean_inference_ms: 1.15160513062406\n",
      "    mean_raw_obs_processing_ms: 0.11052948807726296\n",
      "  time_since_restore: 972.208221912384\n",
      "  time_this_iter_s: 24.09425187110901\n",
      "  time_total_s: 972.208221912384\n",
      "  timers:\n",
      "    learn_throughput: 1161.333\n",
      "    learn_time_ms: 3444.316\n",
      "    load_throughput: 9147383.458\n",
      "    load_time_ms: 0.437\n",
      "    sample_throughput: 263.279\n",
      "    sample_time_ms: 15193.01\n",
      "    update_time_ms: 2.265\n",
      "  timestamp: 1641214777\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:38 (running for 00:16:21.80)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         972.208</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:43 (running for 00:16:26.81)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         972.208</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">  497.98</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_13-59-44\n",
      "  done: false\n",
      "  episode_len_mean: 497.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.75\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 848\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0658141459917976e-15\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3097001016139984\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002771975938230753\n",
      "          model: {}\n",
      "          policy_loss: -0.003007206367328763\n",
      "          total_loss: 239.74151611328125\n",
      "          vf_explained_var: 0.2675352692604065\n",
      "          vf_loss: 239.74452209472656\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.309999999999999\n",
      "    ram_util_percent: 11.599999999999998\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0801234646827389\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08315262206165483\n",
      "    mean_inference_ms: 1.1517882543068423\n",
      "    mean_raw_obs_processing_ms: 0.11053882892200324\n",
      "  time_since_restore: 979.170768737793\n",
      "  time_this_iter_s: 6.9625468254089355\n",
      "  time_total_s: 979.170768737793\n",
      "  timers:\n",
      "    learn_throughput: 1153.135\n",
      "    learn_time_ms: 3468.806\n",
      "    load_throughput: 9043346.27\n",
      "    load_time_ms: 0.442\n",
      "    sample_throughput: 261.46\n",
      "    sample_time_ms: 15298.698\n",
      "    update_time_ms: 2.332\n",
      "  timestamp: 1641214784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:49 (running for 00:16:32.81)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         979.171</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  497.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:54 (running for 00:16:37.82)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         979.171</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  497.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 13:59:59 (running for 00:16:42.83)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         979.171</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  497.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:04 (running for 00:16:47.84)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         979.171</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  497.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:09 (running for 00:16:52.85)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         979.171</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">  497.75</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            497.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-00-10\n",
      "  done: false\n",
      "  episode_len_mean: 498.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 498.16\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 856\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 499.1\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 499.1\n",
      "    episode_reward_min: 484.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 484\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 498\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 484.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 498.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10020298272581836\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1032840614585954\n",
      "      mean_inference_ms: 1.413428283674976\n",
      "      mean_raw_obs_processing_ms: 0.12253733467458183\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.329070729958988e-16\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2968416213989258\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004095285199582577\n",
      "          model: {}\n",
      "          policy_loss: -0.0013736553955823183\n",
      "          total_loss: 223.34593200683594\n",
      "          vf_explained_var: 0.23436591029167175\n",
      "          vf_loss: 223.34730529785156\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.113513513513512\n",
      "    ram_util_percent: 11.600000000000003\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0801438035736974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08317796131058935\n",
      "    mean_inference_ms: 1.1520383664547724\n",
      "    mean_raw_obs_processing_ms: 0.11055292898037589\n",
      "  time_since_restore: 1005.1589820384979\n",
      "  time_this_iter_s: 25.988213300704956\n",
      "  time_total_s: 1005.1589820384979\n",
      "  timers:\n",
      "    learn_throughput: 1149.17\n",
      "    learn_time_ms: 3480.773\n",
      "    load_throughput: 9050175.855\n",
      "    load_time_ms: 0.442\n",
      "    sample_throughput: 260.747\n",
      "    sample_time_ms: 15340.545\n",
      "    update_time_ms: 2.418\n",
      "  timestamp: 1641214810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:15 (running for 00:16:58.83)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1005.16</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">  498.16</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            498.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-00-17\n",
      "  done: false\n",
      "  episode_len_mean: 498.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 498.45\n",
      "  episode_reward_min: 446.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 864\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.664535364979494e-16\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.30517786741256714\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003519083373248577\n",
      "          model: {}\n",
      "          policy_loss: -0.003675314364954829\n",
      "          total_loss: 180.7675018310547\n",
      "          vf_explained_var: 0.4545365571975708\n",
      "          vf_loss: 180.77117919921875\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.45\n",
      "    ram_util_percent: 11.599999999999998\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08016040714287424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08320022588534645\n",
      "    mean_inference_ms: 1.152233572022073\n",
      "    mean_raw_obs_processing_ms: 0.11056228542274027\n",
      "  time_since_restore: 1012.0019028186798\n",
      "  time_this_iter_s: 6.842920780181885\n",
      "  time_total_s: 1012.0019028186798\n",
      "  timers:\n",
      "    learn_throughput: 1137.101\n",
      "    learn_time_ms: 3517.718\n",
      "    load_throughput: 9057504.724\n",
      "    load_time_ms: 0.442\n",
      "    sample_throughput: 257.103\n",
      "    sample_time_ms: 15557.993\n",
      "    update_time_ms: 2.387\n",
      "  timestamp: 1641214817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:21 (running for 00:17:04.71)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">            1012</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">  498.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            498.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:26 (running for 00:17:09.72)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">            1012</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">  498.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            498.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:31 (running for 00:17:14.73)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">            1012</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">  498.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            498.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:36 (running for 00:17:19.74)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">            1012</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">  498.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            498.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:41 (running for 00:17:24.74)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">            1012</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">  498.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">            498.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-00-42\n",
      "  done: false\n",
      "  episode_len_mean: 499.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.45\n",
      "  episode_reward_min: 473.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 872\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10020614010090115\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10332594148566636\n",
      "      mean_inference_ms: 1.4135846093682924\n",
      "      mean_raw_obs_processing_ms: 0.12253645668128085\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.332267682489747e-16\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.30894914269447327\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0013987822458148003\n",
      "          model: {}\n",
      "          policy_loss: -0.00022974552121013403\n",
      "          total_loss: 284.0566101074219\n",
      "          vf_explained_var: 0.3465968370437622\n",
      "          vf_loss: 284.0568542480469\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.005882352941176\n",
      "    ram_util_percent: 11.600000000000001\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08017626258577297\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0832225227046527\n",
      "    mean_inference_ms: 1.152411373739642\n",
      "    mean_raw_obs_processing_ms: 0.11057217404297028\n",
      "  time_since_restore: 1036.4369163513184\n",
      "  time_this_iter_s: 24.43501353263855\n",
      "  time_total_s: 1036.4369163513184\n",
      "  timers:\n",
      "    learn_throughput: 1124.16\n",
      "    learn_time_ms: 3558.213\n",
      "    load_throughput: 9403741.943\n",
      "    load_time_ms: 0.425\n",
      "    sample_throughput: 256.527\n",
      "    sample_time_ms: 15592.919\n",
      "    update_time_ms: 2.486\n",
      "  timestamp: 1641214842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:47 (running for 00:17:30.19)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         1036.44</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">  499.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">            499.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-00-48\n",
      "  done: false\n",
      "  episode_len_mean: 499.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.5\n",
      "  episode_reward_min: 473.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 880\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.661338412448735e-17\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.33786264061927795\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0016626294236630201\n",
      "          model: {}\n",
      "          policy_loss: -0.0017250650562345982\n",
      "          total_loss: 257.88360595703125\n",
      "          vf_explained_var: 0.2730449438095093\n",
      "          vf_loss: 257.88531494140625\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.55\n",
      "    ram_util_percent: 11.599999999999998\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08019006074290559\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08324356824201036\n",
      "    mean_inference_ms: 1.1525640234997365\n",
      "    mean_raw_obs_processing_ms: 0.11057836872730675\n",
      "  time_since_restore: 1043.0214354991913\n",
      "  time_this_iter_s: 6.584519147872925\n",
      "  time_total_s: 1043.0214354991913\n",
      "  timers:\n",
      "    learn_throughput: 1124.365\n",
      "    learn_time_ms: 3557.562\n",
      "    load_throughput: 9055549.198\n",
      "    load_time_ms: 0.442\n",
      "    sample_throughput: 255.224\n",
      "    sample_time_ms: 15672.527\n",
      "    update_time_ms: 2.431\n",
      "  timestamp: 1641214848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:52 (running for 00:17:35.81)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1043.02</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">   499.5</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">             499.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:00:57 (running for 00:17:40.81)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1043.02</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">   499.5</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">             499.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:02 (running for 00:17:45.82)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1043.02</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">   499.5</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">             499.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:07 (running for 00:17:50.83)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1043.02</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">   499.5</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">             499.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:12 (running for 00:17:55.84)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1043.02</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">   499.5</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">             499.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-01-13\n",
      "  done: false\n",
      "  episode_len_mean: 499.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.77\n",
      "  episode_reward_min: 477.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 888\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 497.55\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 497.55\n",
      "    episode_reward_min: 451.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 451\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 451.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10027534232670468\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10340226980917995\n",
      "      mean_inference_ms: 1.4145307414740955\n",
      "      mean_raw_obs_processing_ms: 0.12264937259858495\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.3306692062243676e-17\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3179149329662323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0014973991783335805\n",
      "          model: {}\n",
      "          policy_loss: 0.0008643927285447717\n",
      "          total_loss: 356.0588684082031\n",
      "          vf_explained_var: 0.19004207849502563\n",
      "          vf_loss: 356.0579833984375\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.732352941176472\n",
      "    ram_util_percent: 11.705882352941176\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08019768375382848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0832582054780674\n",
      "    mean_inference_ms: 1.152641538502277\n",
      "    mean_raw_obs_processing_ms: 0.11057740979924692\n",
      "  time_since_restore: 1067.282835483551\n",
      "  time_this_iter_s: 24.26139998435974\n",
      "  time_total_s: 1067.282835483551\n",
      "  timers:\n",
      "    learn_throughput: 1125.662\n",
      "    learn_time_ms: 3553.464\n",
      "    load_throughput: 8689256.267\n",
      "    load_time_ms: 0.46\n",
      "    sample_throughput: 255.676\n",
      "    sample_time_ms: 15644.797\n",
      "    update_time_ms: 2.451\n",
      "  timestamp: 1641214873\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:18 (running for 00:18:01.12)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1067.28</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-01-20\n",
      "  done: false\n",
      "  episode_len_mean: 499.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.77\n",
      "  episode_reward_min: 477.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 896\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6653346031121838e-17\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3299762010574341\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003860413795337081\n",
      "          model: {}\n",
      "          policy_loss: -0.0030521831940859556\n",
      "          total_loss: 199.1850128173828\n",
      "          vf_explained_var: 0.3740776479244232\n",
      "          vf_loss: 199.1880645751953\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.090000000000003\n",
      "    ram_util_percent: 11.76\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08021284110887222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08328197262888487\n",
      "    mean_inference_ms: 1.1528163419678576\n",
      "    mean_raw_obs_processing_ms: 0.11058702929412653\n",
      "  time_since_restore: 1074.238920211792\n",
      "  time_this_iter_s: 6.956084728240967\n",
      "  time_total_s: 1074.238920211792\n",
      "  timers:\n",
      "    learn_throughput: 1117.416\n",
      "    learn_time_ms: 3579.689\n",
      "    load_throughput: 8080342.918\n",
      "    load_time_ms: 0.495\n",
      "    sample_throughput: 254.964\n",
      "    sample_time_ms: 15688.517\n",
      "    update_time_ms: 2.486\n",
      "  timestamp: 1641214880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:24 (running for 00:18:07.10)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1074.24</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:29 (running for 00:18:12.11)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1074.24</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:34 (running for 00:18:17.12)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1074.24</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:39 (running for 00:18:22.13)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1074.24</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:44 (running for 00:18:27.14)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1074.24</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-01-46\n",
      "  done: false\n",
      "  episode_len_mean: 499.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.77\n",
      "  episode_reward_min: 477.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 904\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 500.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 500.0\n",
      "    episode_reward_min: 500.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10062712371422684\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10376565666472744\n",
      "      mean_inference_ms: 1.4189139694667139\n",
      "      mean_raw_obs_processing_ms: 0.12303206260355178\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.326673015560919e-18\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.30264073610305786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006416999269276857\n",
      "          model: {}\n",
      "          policy_loss: -0.004021155647933483\n",
      "          total_loss: 235.52613830566406\n",
      "          vf_explained_var: 0.4266692101955414\n",
      "          vf_loss: 235.5301513671875\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.557894736842103\n",
      "    ram_util_percent: 11.76578947368421\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08023011628365052\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08330935179634663\n",
      "    mean_inference_ms: 1.1530076809114365\n",
      "    mean_raw_obs_processing_ms: 0.11059816253902949\n",
      "  time_since_restore: 1100.5570487976074\n",
      "  time_this_iter_s: 26.31812858581543\n",
      "  time_total_s: 1100.5570487976074\n",
      "  timers:\n",
      "    learn_throughput: 1110.958\n",
      "    learn_time_ms: 3600.495\n",
      "    load_throughput: 7632946.315\n",
      "    load_time_ms: 0.524\n",
      "    sample_throughput: 254.447\n",
      "    sample_time_ms: 15720.355\n",
      "    update_time_ms: 2.467\n",
      "  timestamp: 1641214906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:49 (running for 00:18:32.47)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         1100.56</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-01-53\n",
      "  done: false\n",
      "  episode_len_mean: 499.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.77\n",
      "  episode_reward_min: 477.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 912\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.326673015560919e-18\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.30977368354797363\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003388606710359454\n",
      "          model: {}\n",
      "          policy_loss: -0.001255176728591323\n",
      "          total_loss: 306.4930725097656\n",
      "          vf_explained_var: 0.17501209676265717\n",
      "          vf_loss: 306.4943542480469\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.1\n",
      "    ram_util_percent: 11.7\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08024956236733362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08334016864980617\n",
      "    mean_inference_ms: 1.1532353017663581\n",
      "    mean_raw_obs_processing_ms: 0.11061414418288137\n",
      "  time_since_restore: 1107.3738012313843\n",
      "  time_this_iter_s: 6.8167524337768555\n",
      "  time_total_s: 1107.3738012313843\n",
      "  timers:\n",
      "    learn_throughput: 1106.783\n",
      "    learn_time_ms: 3614.078\n",
      "    load_throughput: 7641637.896\n",
      "    load_time_ms: 0.523\n",
      "    sample_throughput: 250.737\n",
      "    sample_time_ms: 15953.001\n",
      "    update_time_ms: 2.563\n",
      "  timestamp: 1641214913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:01:55 (running for 00:18:38.33)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1107.37</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:00 (running for 00:18:43.33)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1107.37</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:05 (running for 00:18:48.34)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1107.37</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:10 (running for 00:18:53.35)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1107.37</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:15 (running for 00:18:58.36)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1107.37</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  499.77</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">            499.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-02-17\n",
      "  done: false\n",
      "  episode_len_mean: 499.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.08\n",
      "  episode_reward_min: 456.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 921\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 497.8\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 497.8\n",
      "    episode_reward_min: 479.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 497\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 480\n",
      "      - 479\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 497.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 480.0\n",
      "      - 479.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10069513217696986\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10384755988703583\n",
      "      mean_inference_ms: 1.4196638936439678\n",
      "      mean_raw_obs_processing_ms: 0.12307972347654825\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.1633365077804595e-18\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.28842437267303467\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0022505007218569517\n",
      "          model: {}\n",
      "          policy_loss: -0.0027017544489353895\n",
      "          total_loss: 155.21408081054688\n",
      "          vf_explained_var: 0.5348164439201355\n",
      "          vf_loss: 155.21676635742188\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.837142857142858\n",
      "    ram_util_percent: 11.748571428571429\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08026674625280418\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08336479144224679\n",
      "    mean_inference_ms: 1.1533740748844306\n",
      "    mean_raw_obs_processing_ms: 0.11062193273009764\n",
      "  time_since_restore: 1131.9896500110626\n",
      "  time_this_iter_s: 24.615848779678345\n",
      "  time_total_s: 1131.9896500110626\n",
      "  timers:\n",
      "    learn_throughput: 1096.307\n",
      "    learn_time_ms: 3648.613\n",
      "    load_throughput: 7651744.96\n",
      "    load_time_ms: 0.523\n",
      "    sample_throughput: 250.404\n",
      "    sample_time_ms: 15974.163\n",
      "    update_time_ms: 2.612\n",
      "  timestamp: 1641214937\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:20 (running for 00:19:03.98)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1131.99</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">  499.08</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">            499.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-02-24\n",
      "  done: false\n",
      "  episode_len_mean: 499.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 499.08\n",
      "  episode_reward_min: 456.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 929\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.0816682538902298e-18\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2569127678871155\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003834979608654976\n",
      "          model: {}\n",
      "          policy_loss: -0.0022537545301020145\n",
      "          total_loss: 184.89852905273438\n",
      "          vf_explained_var: 0.5317127108573914\n",
      "          vf_loss: 184.90077209472656\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.722222222222221\n",
      "    ram_util_percent: 11.755555555555555\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08026415978511454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08337157980322107\n",
      "    mean_inference_ms: 1.1533092912504341\n",
      "    mean_raw_obs_processing_ms: 0.11061006060274411\n",
      "  time_since_restore: 1138.2685542106628\n",
      "  time_this_iter_s: 6.27890419960022\n",
      "  time_total_s: 1138.2685542106628\n",
      "  timers:\n",
      "    learn_throughput: 1100.653\n",
      "    learn_time_ms: 3634.205\n",
      "    load_throughput: 8235429.02\n",
      "    load_time_ms: 0.486\n",
      "    sample_throughput: 250.57\n",
      "    sample_time_ms: 15963.617\n",
      "    update_time_ms: 2.614\n",
      "  timestamp: 1641214944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:26 (running for 00:19:09.30)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1138.27</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  499.08</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">            499.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:31 (running for 00:19:14.31)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1138.27</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  499.08</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">            499.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:36 (running for 00:19:19.31)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1138.27</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  499.08</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">            499.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:41 (running for 00:19:24.32)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1138.27</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  499.08</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">            499.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:46 (running for 00:19:29.33)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1138.27</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">  499.08</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">            499.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-02-48\n",
      "  done: false\n",
      "  episode_len_mean: 494.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 494.06\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 938\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 484.45\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 484.45\n",
      "    episode_reward_min: 407.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 477\n",
      "      - 491\n",
      "      - 500\n",
      "      - 500\n",
      "      - 468\n",
      "      - 440\n",
      "      - 500\n",
      "      - 500\n",
      "      - 421\n",
      "      - 500\n",
      "      - 500\n",
      "      - 407\n",
      "      - 485\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 477.0\n",
      "      - 491.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 468.0\n",
      "      - 440.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 421.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 407.0\n",
      "      - 485.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10077220689618134\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10394825696670647\n",
      "      mean_inference_ms: 1.4209386167556604\n",
      "      mean_raw_obs_processing_ms: 0.12317528789592354\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0408341269451149e-18\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.25869643688201904\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0027097088750451803\n",
      "          model: {}\n",
      "          policy_loss: -0.006009688600897789\n",
      "          total_loss: 96.60514068603516\n",
      "          vf_explained_var: 0.6104920506477356\n",
      "          vf_loss: 96.61115264892578\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.564705882352941\n",
      "    ram_util_percent: 11.7\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08026830720500518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08338698422603116\n",
      "    mean_inference_ms: 1.153344187201906\n",
      "    mean_raw_obs_processing_ms: 0.11060474001492959\n",
      "  time_since_restore: 1162.4961895942688\n",
      "  time_this_iter_s: 24.227635383605957\n",
      "  time_total_s: 1162.4961895942688\n",
      "  timers:\n",
      "    learn_throughput: 1101.606\n",
      "    learn_time_ms: 3631.063\n",
      "    load_throughput: 8183608.604\n",
      "    load_time_ms: 0.489\n",
      "    sample_throughput: 251.184\n",
      "    sample_time_ms: 15924.578\n",
      "    update_time_ms: 2.506\n",
      "  timestamp: 1641214968\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:51 (running for 00:19:34.58)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">          1162.5</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">  494.06</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            494.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-02-55\n",
      "  done: false\n",
      "  episode_len_mean: 492.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.45\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 946\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.204170634725574e-19\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2483358383178711\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003815557574853301\n",
      "          model: {}\n",
      "          policy_loss: -0.00015161468763835728\n",
      "          total_loss: 176.90817260742188\n",
      "          vf_explained_var: 0.5207358002662659\n",
      "          vf_loss: 176.90834045410156\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.01\n",
      "    ram_util_percent: 11.7\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08026873338852676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08339684334243128\n",
      "    mean_inference_ms: 1.1533358543620529\n",
      "    mean_raw_obs_processing_ms: 0.11059580191219995\n",
      "  time_since_restore: 1169.2003734111786\n",
      "  time_this_iter_s: 6.70418381690979\n",
      "  time_total_s: 1169.2003734111786\n",
      "  timers:\n",
      "    learn_throughput: 1109.076\n",
      "    learn_time_ms: 3606.606\n",
      "    load_throughput: 8209637.894\n",
      "    load_time_ms: 0.487\n",
      "    sample_throughput: 253.399\n",
      "    sample_time_ms: 15785.377\n",
      "    update_time_ms: 2.585\n",
      "  timestamp: 1641214975\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:02:57 (running for 00:19:40.32)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          1169.2</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">  492.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:02 (running for 00:19:45.33)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          1169.2</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">  492.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:07 (running for 00:19:50.34)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          1169.2</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">  492.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:12 (running for 00:19:55.34)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          1169.2</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">  492.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:17 (running for 00:20:00.35)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          1169.2</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">  492.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-03-20\n",
      "  done: false\n",
      "  episode_len_mean: 492.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.45\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 954\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 497.55\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 497.55\n",
      "    episode_reward_min: 451.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 451\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 451.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10091182007568471\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.1040961749408513\n",
      "      mean_inference_ms: 1.4227154702618872\n",
      "      mean_raw_obs_processing_ms: 0.12325830412241379\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.602085317362787e-19\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2623783051967621\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0008481194381602108\n",
      "          model: {}\n",
      "          policy_loss: 0.0006717838114127517\n",
      "          total_loss: 256.2461242675781\n",
      "          vf_explained_var: 0.3934962749481201\n",
      "          vf_loss: 256.2454528808594\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.527777777777777\n",
      "    ram_util_percent: 11.699999999999998\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08026127029794097\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08339792577048381\n",
      "    mean_inference_ms: 1.1532257950723988\n",
      "    mean_raw_obs_processing_ms: 0.1105779512975858\n",
      "  time_since_restore: 1194.269511938095\n",
      "  time_this_iter_s: 25.069138526916504\n",
      "  time_total_s: 1194.269511938095\n",
      "  timers:\n",
      "    learn_throughput: 1114.3\n",
      "    learn_time_ms: 3589.697\n",
      "    load_throughput: 8339820.053\n",
      "    load_time_ms: 0.48\n",
      "    sample_throughput: 253.713\n",
      "    sample_time_ms: 15765.855\n",
      "    update_time_ms: 2.507\n",
      "  timestamp: 1641215000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:22 (running for 00:20:05.44)<br>Memory usage on this node: 7.3/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         1194.27</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">  492.45</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-03-27\n",
      "  done: false\n",
      "  episode_len_mean: 492.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.44\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 962\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3010426586813936e-19\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.26068851351737976\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004601421300321817\n",
      "          model: {}\n",
      "          policy_loss: -0.00252857175655663\n",
      "          total_loss: 327.0229797363281\n",
      "          vf_explained_var: 0.399458646774292\n",
      "          vf_loss: 327.0255126953125\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.544444444444444\n",
      "    ram_util_percent: 11.822222222222223\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08025798667890836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08340253170878308\n",
      "    mean_inference_ms: 1.1531596377499853\n",
      "    mean_raw_obs_processing_ms: 0.11056490037888685\n",
      "  time_since_restore: 1200.9417476654053\n",
      "  time_this_iter_s: 6.672235727310181\n",
      "  time_total_s: 1200.9417476654053\n",
      "  timers:\n",
      "    learn_throughput: 1113.89\n",
      "    learn_time_ms: 3591.02\n",
      "    load_throughput: 8393644.187\n",
      "    load_time_ms: 0.477\n",
      "    sample_throughput: 252.657\n",
      "    sample_time_ms: 15831.709\n",
      "    update_time_ms: 2.554\n",
      "  timestamp: 1641215007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:28 (running for 00:20:11.15)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1200.94</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:33 (running for 00:20:16.16)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1200.94</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:38 (running for 00:20:21.17)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1200.94</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:43 (running for 00:20:26.17)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1200.94</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:48 (running for 00:20:31.18)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1200.94</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-03-51\n",
      "  done: false\n",
      "  episode_len_mean: 492.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.44\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 970\n",
      "  evaluation:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 497.75\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 500.0\n",
      "    episode_reward_mean: 497.75\n",
      "    episode_reward_min: 455.0\n",
      "    episodes_this_iter: 20\n",
      "    hist_stats:\n",
      "      episode_lengths:\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 455\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      - 500\n",
      "      episode_reward:\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 455.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "      - 500.0\n",
      "    off_policy_estimator: {}\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.10093963057855689\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 0.10412633695622056\n",
      "      mean_inference_ms: 1.423294915576578\n",
      "      mean_raw_obs_processing_ms: 0.12327615221576915\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.505213293406968e-20\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2688307464122772\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0021669436246156693\n",
      "          model: {}\n",
      "          policy_loss: 0.0009603818180039525\n",
      "          total_loss: 484.1625061035156\n",
      "          vf_explained_var: -0.077602818608284\n",
      "          vf_loss: 484.1615295410156\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.737142857142857\n",
      "    ram_util_percent: 11.899999999999999\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08025302047742677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08340485100631874\n",
      "    mean_inference_ms: 1.1530862613238626\n",
      "    mean_raw_obs_processing_ms: 0.11054963019123512\n",
      "  time_since_restore: 1225.1908042430878\n",
      "  time_this_iter_s: 24.249056577682495\n",
      "  time_total_s: 1225.1908042430878\n",
      "  timers:\n",
      "    learn_throughput: 1113.912\n",
      "    learn_time_ms: 3590.948\n",
      "    load_throughput: 8748613.443\n",
      "    load_time_ms: 0.457\n",
      "    sample_throughput: 252.583\n",
      "    sample_time_ms: 15836.407\n",
      "    update_time_ms: 2.579\n",
      "  timestamp: 1641215031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:53 (running for 00:20:36.45)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         1225.19</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v1_b92be_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-01-03_14-03-58\n",
      "  done: false\n",
      "  episode_len_mean: 492.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.44\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 978\n",
      "  experiment_id: b4dc5ffd0f634bb3b56cec8b19d70331\n",
      "  hostname: devbox-x299\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.252606646703484e-20\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2761077880859375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002532893093302846\n",
      "          model: {}\n",
      "          policy_loss: -0.0015050852671265602\n",
      "          total_loss: 263.3819885253906\n",
      "          vf_explained_var: 0.48334449529647827\n",
      "          vf_loss: 263.3835144042969\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.0.90\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.860000000000003\n",
      "    ram_util_percent: 11.900000000000002\n",
      "  pid: 11117\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08025120618213681\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08341052334973513\n",
      "    mean_inference_ms: 1.153057153688451\n",
      "    mean_raw_obs_processing_ms: 0.11053798827725313\n",
      "  time_since_restore: 1231.981814622879\n",
      "  time_this_iter_s: 6.79101037979126\n",
      "  time_total_s: 1231.981814622879\n",
      "  timers:\n",
      "    learn_throughput: 1119.859\n",
      "    learn_time_ms: 3571.88\n",
      "    load_throughput: 8687456.504\n",
      "    load_time_ms: 0.46\n",
      "    sample_throughput: 252.595\n",
      "    sample_time_ms: 15835.609\n",
      "    update_time_ms: 2.53\n",
      "  timestamp: 1641215038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: b92be_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:03:59 (running for 00:20:42.28)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1231.98</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:04:04 (running for 00:20:47.29)<br>Memory usage on this node: 7.4/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1231.98</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:04:09 (running for 00:20:52.30)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1231.98</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-03 14:04:14 (running for 00:20:57.31)<br>Memory usage on this node: 7.5/62.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/28 CPUs, 0/1 GPUs, 0.0/34.89 GiB heap, 0.0/17.44 GiB objects<br>Result logdir: /home/dibya/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_b92be_00000</td><td>RUNNING </td><td>192.168.0.90:11117</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1231.98</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">  492.44</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">            492.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"CartPole-v1\",\n",
    "                 \"evaluation_interval\": 2,    # num of training iter between evaluations\n",
    "                 \"evaluation_num_episodes\": 20,\n",
    "                 \"num_gpus\": 0\n",
    "                 }\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d547c2d-aa55-48f4-b586-f183b763a301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
