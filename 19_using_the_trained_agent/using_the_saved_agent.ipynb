{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8532d434-a069-4abd-b6fb-f08296a34678",
   "metadata": {},
   "source": [
    "# Using the saved agent\n",
    "\n",
    "<img src=\"images/restore/restore.png\" width=\"500\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032e646-dc33-443d-a83e-a0a6d69c5220",
   "metadata": {},
   "source": [
    "## Step 1: Restoring the agent from the checkpoint\n",
    "\n",
    "1. **Algorithm trainer class**: Find it in the algorithm implementation (linked in the [`rllib` algorithms page](https://docs.ray.io/en/master/rllib-algorithms.html))\n",
    "2. Import the trainer class.\n",
    "3. Create an empty agent by initializing the trainer class. **Use the same configuration as the experiment**.\n",
    "4. Restore the agent from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c675ea4-37b2-47be-857c-f4e476782761",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.90',\n",
       " 'raylet_ip_address': '192.168.0.90',\n",
       " 'redis_address': '192.168.0.90:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2022-01-04_13-12-43_027975_3647/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-01-04_13-12-43_027975_3647/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-01-04_13-12-43_027975_3647',\n",
       " 'metrics_export_port': 59211,\n",
       " 'node_id': '2f158cedc42e83d5aeb34f494af1533cf982fde785a7349fc0a2b2d0'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc4e887-4063-4061-abbe-b7f802810faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 13:12:48,373\tINFO trainer.py:722 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also want to then set `eager_tracing=True` in order to reach similar execution speed as with static-graph mode.\n",
      "2022-01-04 13:12:48,374\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-01-04 13:12:48,375\tINFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=6606)\u001b[0m 2022-01-04 13:12:50,254\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "2022-01-04 13:12:51,029\tWARNING deprecation.py:45 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "2022-01-04 13:12:51,575\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-01-04 13:12:52,178\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
      "2022-01-04 13:12:52,231\tINFO trainable.py:467 -- Restored on 192.168.0.90 from checkpoint: ../18_saving_the_trained_agent/cartpole_v1/PPO/PPO_CartPole-v1_788b8_00000_0_2022-01-03_16-26-08/checkpoint_000016/checkpoint-16\n",
      "2022-01-04 13:12:52,233\tINFO trainable.py:475 -- Current state after restoring: {'_iteration': 16, '_timesteps_total': 0, '_time_total': 221.60105895996094, '_episodes_total': 452}\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "\n",
    "agent = PPOTrainer(config={\"env\": \"CartPole-v1\",\n",
    "                           \"evaluation_interval\": 2,\n",
    "                           \"evaluation_num_episodes\": 20\n",
    "                           }\n",
    "                   )\n",
    "agent.restore(\"../18_saving_the_trained_agent/cartpole_v1/PPO/PPO_CartPole-v1_788b8_00000_0_2022-01-03_16-26-08/checkpoint_000016/checkpoint-16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af115db8-ef89-444a-af2d-c7d643527db1",
   "metadata": {},
   "source": [
    "## Step 2: Use the agent\n",
    "\n",
    "- Compute the action (according to the **trained policy**) using the `agent.compute_action()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54bcc939-eda1-4d99-97b1-5dc1b4e4a58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 13:14:01,465\tWARNING deprecation.py:45 -- DeprecationWarning: `compute_action` has been deprecated. Use `compute_single_action` instead. This will raise an error in the future!\n",
      "Failed to establish dbus connection"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f1d9e-dfd3-49cb-92c0-20d379286dc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Making videos of the agent in action\n",
    "\n",
    "- Wrap the `env` in the `gym.wrappers.RecordVideo` class.\n",
    "    - Supply the directory to write the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b97fba-2db0-482b-b213-3505ce98760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "env = RecordVideo(gym.make(\"CartPole-v1\"), \"ppo_video\")\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b0206-1676-4a7a-9a15-9b82f4825aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
